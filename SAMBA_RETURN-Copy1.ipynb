{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5a6d84-1b9b-43c9-aa1c-d3e73207e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import argparse\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SAMBA(nn.Module):\n",
    "    def __init__(self, ModelArgs,hidden,inp,out,embed,cheb_k):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "\n",
    "        self.mam1 = Mamba(ModelArgs,hidden)\n",
    "\n",
    "        \n",
    "\n",
    "        self.cheb_k=cheb_k\n",
    "\n",
    "        self.gamma=nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "       \n",
    "\n",
    "  \n",
    "\n",
    "        self.adj=nn.Parameter(torch.randn(ModelArgs.vocab_size,embed), requires_grad=True)\n",
    "\n",
    "        self.embed_w=nn.Parameter(torch.randn(embed,embed), requires_grad=True)\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, out))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed, out))\n",
    "\n",
    "        self.proj=nn.Linear(ModelArgs.vocab_size,1)\n",
    "        self.proj_seq=nn.Linear(ModelArgs.seq_in,1)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def gaussian_kernel_graph(self,E_A,x ,gamma=1.0):\n",
    "    \n",
    "    # Compute pairwise squared Euclidean distance\n",
    "\n",
    "        x_mean=torch.mean(x,dim=0)\n",
    "\n",
    "        x_time=torch.mm(x_mean.permute(1,0),x_mean)\n",
    "\n",
    "        \n",
    "        N = E_A.size(0)\n",
    "    # Expanding the dimensions to compute pairwise differences\n",
    "        E_A_expanded = E_A.unsqueeze(0).expand(N, N, -1)\n",
    "        E_A_T_expanded = E_A.unsqueeze(1).expand(N, N, -1)\n",
    "    # Pairwise squared Euclidean distances\n",
    "        distance_matrix = torch.sum((E_A_expanded - E_A_T_expanded)**2, dim=2)\n",
    "    \n",
    "    # Apply Gaussian kernel\n",
    "        A = torch.exp(-gamma * distance_matrix)\n",
    "\n",
    "        dr=nn.Dropout(0.35)\n",
    "\n",
    "        #A=torch.tanh(torch.mm(self.adj, self.adj.transpose(0, 1))+1)\n",
    "    \n",
    "    # Optional: Normalize the adjacency matrix with softmax (row-wise)\n",
    "        A = F.softmax(A, dim=1)\n",
    "    \n",
    "        return dr(A)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        #x_mean=torch.mean(input_ids,dim=0)\n",
    "\n",
    "        #ADJ=F.softmax(torch.mm(x_mean.permute(1,0),x_mean),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        xx=self.mam1(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "        #m = nn.LeakyReLU(0.1)\n",
    "        #ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "        #dr=nn.Dropout(0.35)\n",
    "        #ADJ=dr(F.softmax(F.relu(torch.mm(torch.mm(self.adj, self.embed_w),self.adj.transpose(0, 1))),dim=1))\n",
    "        \n",
    "        ADJ=self.gaussian_kernel_graph(self.adj,xx,gamma=self.gamma)\n",
    "\n",
    "        #degree = torch.sum(ADJ, dim=1)\n",
    "        # laplacian is sym or not\n",
    "        #attention = 0.5 * (attention + attention.T)\n",
    "        #degree_l = torch.diag(degree)+1e-5\n",
    "        \n",
    "        #deg=torch.diag(1 / (degree + 1e-5))\n",
    "        \n",
    "        #diagonal_degree_hat = torch.diag(1 / (torch.sqrt(degree) + 1e-5))\n",
    "        #attention = torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))#milan\n",
    "        #A=torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))\n",
    "        #r=torch.rand(1).cuda()\n",
    "        \n",
    "        #d1=torch.diag(1 / (torch.pow(degree,1-r) + 1e-5))\n",
    "        #d2=torch.diag(1 / (torch.pow(degree,r) + 1e-5))\n",
    "        \n",
    "        \n",
    "        #L = torch.eye(input_ids.size(2)).cuda()-torch.matmul(d1,torch.matmul(ADJ,d2))\n",
    "\n",
    "        I=torch.eye(input_ids.size(2)).cuda()\n",
    "\n",
    "        #L=I-ADJ\n",
    "\n",
    "        #out=self.mlp(xx)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        support_set = [I,ADJ]#(math.sqrt(2)/2)*L,(-math.sqrt(2)/2)*L]\n",
    "    \n",
    "        \n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "        weights = torch.einsum('nd,dkio->nkio', self.adj, self.weights_pool)  #N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)                       #N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, xx.permute(0,2,1))      #B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out = torch.einsum('bnki,nkio->bno', x_g, weights) + bias#B,N,D_OUT\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return self.proj(out.permute(0,2,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs,hid):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "        self.nl=args.n_layer\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        self.layers2 = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        #self.layers3 = nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "\n",
    "        #self.layers3=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "        \n",
    "        #self.layers4=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),gconv(args.seq_in,hid,2,10,args.d_model),nn.ReLU(),gconv(hid,args.seq_in,2,10,args.d_model)) for _ in range(args.n_layer)])\n",
    "      \n",
    "       \n",
    "\n",
    "        self.lin=nn.ModuleList([nn.Sequential(nn.LayerNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "        \n",
    "        #self.lin2=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "        \n",
    "        \n",
    "        self.norm_f = nn.LayerNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size)\n",
    "\n",
    "\n",
    "        self.proj=nn.Sequential(nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))\n",
    "\n",
    "        self.nnl=nn.LayerNorm(args.vocab_size)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "      \n",
    "        #self.proj=nn.Linear(2*ModelArgs.vocab_size, ModelArgs.vocab_size)\n",
    "        #self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "                                                     # See \"Weight Tying\" paper\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        x1=x\n",
    "        x2=x\n",
    "    \n",
    "\n",
    "        for i in range(self.nl):\n",
    "            \n",
    "            x1 = self.layers[i](x1)\n",
    "            x2=self.layers2[i](x2.flip([1]))\n",
    "            \n",
    "            x=x1+x2.flip([1])+x\n",
    "\n",
    "            x=self.lin[i](x.permute(0,2,1)).permute(0,2,1)+x\n",
    "\n",
    "            x1=x\n",
    "            x2=x\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        x = self.norm_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        \n",
    "\n",
    "#        a=logits.shape\n",
    "\n",
    " #       #sq=torch.reshape(logits,(a[0],a[2],a[1]))\n",
    "\n",
    "  #      out=self.out(sq)\n",
    "\n",
    "   #     b=out.shape\n",
    "\n",
    "    #    out=torch.reshape(out,(b[0],b[2],b[1]))\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name: str):\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name: One of\n",
    "                * 'state-spaces/mamba-2.8b-slimpj'\n",
    "                * 'state-spaces/mamba-2.8b'\n",
    "                * 'state-spaces/mamba-1.4b'\n",
    "                * 'state-spaces/mamba-790m'\n",
    "                * 'state-spaces/mamba-370m'\n",
    "                * 'state-spaces/mamba-130m'\n",
    "\n",
    "        Returns:\n",
    "            model: Mamba model with weights loaded\n",
    "\n",
    "        \"\"\"\n",
    "        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "        from transformers.utils.hub import cached_file\n",
    "\n",
    "        def load_config_hf(model_name):\n",
    "            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return json.load(open(resolved_archive_file))\n",
    "\n",
    "\n",
    "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
    "            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n",
    "\n",
    "        config_data = load_config_hf(pretrained_model_name)\n",
    "        args = ModelArgs(\n",
    "            d_model=config_data['d_model'],\n",
    "            n_layer=config_data['n_layer'],\n",
    "            vocab_size=config_data['vocab_size']\n",
    "        )\n",
    "        model = Mamba(args)\n",
    "\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
    "        new_state_dict = {}\n",
    "        for key in state_dict:\n",
    "            new_key = key.replace('backbone.', '')\n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = nn.LayerNorm(args.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "\n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.mixer(self.norm(x)) \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class gconv(nn.Module):\n",
    "    def __init__(self, inp, hid,embed,cheb_k,n):\n",
    "        super(gconv, self).__init__()\n",
    "\n",
    "        self.node_num=n\n",
    "\n",
    "        self.inp=inp\n",
    "\n",
    "        self.cheb_k=cheb_k\n",
    "\n",
    "        self.adj=nn.Parameter(torch.randn(n,embed), requires_grad=True)\n",
    "       \n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, hid))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed,hid))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        #output shape [B, N, C]\n",
    "        \n",
    "        ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        support_set = [torch.eye(self.node_num).cuda(),ADJ]\n",
    "    \n",
    "        \n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "        weights = torch.einsum('nd,dkio->nkio', self.adj, self.weights_pool)  #N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)                       #N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, x)      #B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out_6 = torch.einsum('bnki,nkio->bno', x_g, weights) + bias   #B,N,D_OUT\n",
    "\n",
    "        return out_6\n",
    "\n",
    "class AVWGCN(nn.Module):\n",
    "    def __init__(self, dim_in, hid, cheb_k,n):\n",
    "        super(AVWGCN, self).__init__()\n",
    "\n",
    "        self.node_num=n\n",
    "\n",
    "        self.inp=dim_in\n",
    "\n",
    "        self.cheb_k = cheb_k\n",
    "        self.node_embeddings = nn.Parameter(torch.randn(n,dim_in,dim_in), requires_grad=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(cheb_k,n,dim_in, hid))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(n, hid))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        #output shape [B, N, C]\n",
    "        \n",
    "        supports = F.softmax(F.relu(self.node_embeddings), dim=2)\n",
    "\n",
    "        I=torch.eye(self.inp).cuda()\n",
    "\n",
    "        I2=I[None,:,:].repeat(x.size(1),1,1)\n",
    "        \n",
    "\n",
    "        support_set = [I2, supports]\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "                              #N, dim_out\n",
    "        x_g = torch.einsum(\"bnc,kncm->bknm\", x, supports)      #B, cheb_k, N, dim_in\n",
    "        #x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        x_gconv = torch.einsum('bknm,knmo->bno', x_g, self.weights_pool) + self.bias_pool     #b, N, dim_out\n",
    "        return x_gconv\n",
    "\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.in_proj_r = nn.Linear(args.d_model, args.d_inner, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size,bias=False)\n",
    "\n",
    "        #self.x_proj_r = nn.Linear(args.d_inner, args.dt_rank + args.d_state, bias=True)\n",
    "\n",
    "        #self.x_proj = FourierKANLayer(args.d_inner, args.dt_rank + args.d_state * 2, 100)\n",
    "\n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "        #self.dt_proj=FourierKANLayer(args.dt_rank, args.d_inner, 100)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "\n",
    "        #x=self.embedding(x)\n",
    "\n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
    "\n",
    "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
    "    \n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
    "\n",
    "\n",
    "\n",
    "        x = F.silu(x)\n",
    "\n",
    "        gate=x*(1-F.sigmoid(res))\n",
    "\n",
    "        \n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * F.silu(res)\n",
    "\n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        #o1=self.norm_f(output)\n",
    "\n",
    "        #o2=self.lm_head(o1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "\n",
    "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "\n",
    "        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def selective_scan(self, u, delta, A, B, C, D):\n",
    "        \"\"\"Does selective scan algorithm. See:\n",
    "            - Section 2 State Space Models in the Mamba paper [1]\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        This is the classic discrete state space formula:\n",
    "            x(t + 1) = Ax(t) + Bu(t)\n",
    "            y(t)     = Cx(t) + Du(t)\n",
    "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
    "\n",
    "        Args:\n",
    "            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "            delta: shape (b, l, d_in)\n",
    "            A: shape (d_in, n)\n",
    "            B: shape (b, l, n)\n",
    "            C: shape (b, l, n)\n",
    "            D: shape (d_in,)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
    "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d_in) = u.shape\n",
    "        n = A.shape[1]\n",
    "\n",
    "        # Discretize continuous parameters (A, B)\n",
    "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
    "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
    "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
    "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n",
    "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
    "\n",
    "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
    "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n",
    "        # is additionally hardware-aware (like FlashAttention).\n",
    "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
    "        ys = []\n",
    "        for i in range(l):\n",
    "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
    "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
    "\n",
    "        y = y + u * D\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293f11fd-3038-417c-bef4-4368a25e7d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          price      d12       e12     AAA     BAA     lty     tbl     Rfree  \\\n",
      "yyyymm                                                                         \n",
      "187101     4.44   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.000000   \n",
      "187102     4.50   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004967   \n",
      "187103     4.61   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004525   \n",
      "187104     4.74   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004252   \n",
      "187105     4.86   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004643   \n",
      "...         ...      ...       ...     ...     ...     ...     ...       ...   \n",
      "202308  4507.66  69.1137  183.1700  0.0495  0.0602  0.0417  0.0530  0.004500   \n",
      "202309  4288.05  69.3131  184.2500  0.0513  0.0616  0.0438  0.0532  0.004300   \n",
      "202310  4193.80  69.6433  186.9767  0.0561  0.0663  0.0480  0.0534  0.004700   \n",
      "202311  4567.80  69.9735  189.7033  0.0528  0.0629  0.0450  0.0527  0.004400   \n",
      "202312  4769.83  70.3037  192.4300  0.0474  0.0564  0.0402  0.0524  0.004300   \n",
      "\n",
      "             d/p       d/y       e/p       d/e       b/m     tms     dfy  \\\n",
      "yyyymm                                                                     \n",
      "187101  0.058559  0.000000  0.090090  0.650000  0.000000  0.0000  0.0000   \n",
      "187102  0.057778  0.058559  0.088889  0.650000  0.000000  0.0000  0.0000   \n",
      "187103  0.056399  0.057778  0.086768  0.650000  0.000000  0.0000  0.0000   \n",
      "187104  0.054852  0.056399  0.084388  0.650000  0.000000  0.0000  0.0000   \n",
      "187105  0.053498  0.054852  0.082305  0.650000  0.000000  0.0000  0.0000   \n",
      "...          ...       ...       ...       ...       ...     ...     ...   \n",
      "202308  0.015333  0.015061  0.040635  0.377320  0.210885 -0.0113  0.0107   \n",
      "202309  0.016164  0.015377  0.042968  0.376191  0.218528 -0.0094  0.0103   \n",
      "202310  0.016606  0.016241  0.044584  0.372471  0.221534 -0.0054  0.0102   \n",
      "202311  0.015319  0.016685  0.041531  0.368858  0.203676 -0.0077  0.0101   \n",
      "202312  0.014739  0.015391  0.040343  0.365347  0.194280 -0.0122  0.0090   \n",
      "\n",
      "            infl      svar  trend  \n",
      "yyyymm                             \n",
      "187101  0.000000  0.000000      1  \n",
      "187102  0.000000  0.000000      1  \n",
      "187103  0.000000  0.000000      1  \n",
      "187104  0.000000  0.000000      1  \n",
      "187105  0.000000  0.000000      0  \n",
      "...          ...       ...    ...  \n",
      "202308  0.004367  0.001337      0  \n",
      "202309  0.002485  0.001023      0  \n",
      "202310 -0.000383  0.001678      1  \n",
      "202311 -0.002015  0.001341      1  \n",
      "202312 -0.000993  0.000797      0  \n",
      "\n",
      "[1836 rows x 18 columns]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     72\u001b[0m val_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mXX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     73\u001b[0m train_len \u001b[38;5;241m=\u001b[39m  XX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mtest_len\u001b[38;5;241m-\u001b[39mval_len\n\u001b[0;32m---> 77\u001b[0m X_test\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtest_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m Y_test\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mfloat(YY[\u001b[38;5;241m-\u001b[39mtest_len:,:,:])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     82\u001b[0m X_train\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mfloat(XX[test_len:test_len\u001b[38;5;241m+\u001b[39mtrain_len,:,:])\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"Data2023.csv\", index_col=\"yyyymm\")\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "\n",
    "missing_ratio = X.isnull().mean()  # 计算每列的缺失值比例\n",
    "columns_to_drop = missing_ratio[missing_ratio > 0.35].index  # 找到缺失值比例超过 35% 的列\n",
    "X = X.drop(columns=columns_to_drop)  # 删除这些列\n",
    "\n",
    "X = X.fillna(value=0)\n",
    "\n",
    "\n",
    "X[\"trend\"] = (X[\"price\"].pct_change().shift(-1) > 0).astype(int)\n",
    "\n",
    "\n",
    "print(X)\n",
    "\n",
    "\n",
    "class MinMaxNorm01(object):\n",
    "    \"\"\"scale data to range [0, 1]\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.min = x.min()\n",
    "        self.max = x.max()\n",
    "        #print('Min:{}, Max:{}'.format(self.min, self.max))\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = 1.0 * (x - self.min) / (self.max - self.min)\n",
    "        return x\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        x = x * (self.max - self.min) + self.min\n",
    "        return x\n",
    "\n",
    "a=X.to_numpy()\n",
    "\n",
    "#data1=train_data.to_numpy()\n",
    "\n",
    "mmn = MinMaxNorm01()\n",
    "\n",
    "data=a\n",
    "\n",
    "\n",
    "dataset = data\n",
    "\n",
    "window=5\n",
    "predict=1\n",
    "\n",
    "ran=data.shape[0]\n",
    "i=0\n",
    "X=[]\n",
    "Y=[]\n",
    "while i+window<ran:\n",
    "\n",
    "    X.append(torch.Tensor(dataset[i:i+window,1:]))\n",
    "    Y.append(torch.Tensor(dataset[i+window:i+window+predict,0]))\n",
    "    i+=1\n",
    "\n",
    "XX=torch.stack(X,dim=0)\n",
    "YY=torch.stack(Y,dim=0)\n",
    "YY=YY[:,:,None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_len = int(0.15*XX.shape[0])\n",
    "val_len = int(0.05*XX.shape[0])\n",
    "train_len =  XX.shape[0]-test_len-val_len\n",
    "\n",
    "\n",
    "\n",
    "X_test=torch.Tensor.float(XX[-test_len:,:,:]).cuda()\n",
    "\n",
    "\n",
    "Y_test=torch.Tensor.float(YY[-test_len:,:,:]).cuda()\n",
    "\n",
    "X_train=torch.Tensor.float(XX[test_len:test_len+train_len,:,:]).cuda()\n",
    "Y_train=torch.Tensor.float(YY[test_len:test_len+train_len,:,:]).cuda()\n",
    "\n",
    "X_val=torch.Tensor.float(XX[-val_len:,:,:]).cuda()\n",
    "Y_val=torch.Tensor.float(YY[-val_len:,:,:]).cuda()\n",
    "\n",
    "print(X_train, X_test)\n",
    "print(Y_train, Y_test)\n",
    "\n",
    "\n",
    "def data_loader(X, Y, batch_size, shuffle=True, drop_last=True):\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    TensorFloat = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    X, Y = TensorFloat(X), TensorFloat(Y)\n",
    "    data = torch.utils.data.TensorDataset(X, Y)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                             shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader\n",
    "\n",
    "train_loader = data_loader(X_train, Y_train, 64, shuffle=False, drop_last=False)\n",
    "val_loader = data_loader(X_val, Y_val, 64, shuffle=False, drop_last=False)\n",
    "test_loader = data_loader(X_test, Y_test, 64, shuffle=False, drop_last=False)\n",
    "\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c59ab7c2-1c15-4762-b601-33f5ae688764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(root, name=None, debug=True):\n",
    "    #when debug is true, show DEBUG and INFO in screen\n",
    "    #when debug is false, show DEBUG in file and info in both screen&file\n",
    "    #INFO will always be in screen\n",
    "    # create a logger\n",
    "    logger = logging.getLogger(name)\n",
    "    #critical > error > warning > info > debug > notset\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # define the formate\n",
    "    formatter = logging.Formatter('%(asctime)s: %(message)s', \"%Y-%m-%d %H:%M\")\n",
    "    # create another handler for output log to console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    if debug:\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        # create a handler for write log to file\n",
    "        logfile = os.path.join(root, 'run.log')\n",
    "        print('Creat Log File in: ', logfile)\n",
    "        file_handler = logging.FileHandler(logfile, mode='w')\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    # add Handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    if not debug:\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def init_seed(seed):\n",
    "    '''\n",
    "    Disable cudnn to maximize reproducibility\n",
    "    '''\n",
    "    torch.cuda.cudnn_enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def init_device(opt):\n",
    "    if torch.cuda.is_available():\n",
    "        opt.cuda = True\n",
    "        torch.cuda.set_device(int(opt.device[5]))\n",
    "    else:\n",
    "        opt.cuda = False\n",
    "        opt.device = 'cpu'\n",
    "    return opt\n",
    "\n",
    "def init_optim(model, opt):\n",
    "    '''\n",
    "    Initialize optimizer\n",
    "    '''\n",
    "    return torch.optim.Adam(params=model.parameters(),lr=opt.lr_init)\n",
    "\n",
    "def init_lr_scheduler(optim, opt):\n",
    "    '''\n",
    "    Initialize the learning rate scheduler\n",
    "    '''\n",
    "    #return torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=opt.lr_scheduler_rate,step_size=opt.lr_scheduler_step)\n",
    "    return torch.optim.lr_scheduler.MultiStepLR(optimizer=optim, milestones=opt.lr_decay_steps,\n",
    "                                                gamma = opt.lr_scheduler_rate)\n",
    "\n",
    "def print_model_parameters(model, only_num = True):\n",
    "    \n",
    "\n",
    "    \n",
    "    print('*****************Model Parameter*****************')\n",
    "    if not only_num:\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape, param.requires_grad)\n",
    "    total_num = sum([param.nelement() for param in model.parameters()])\n",
    "    print('Total params num: {}'.format(total_num))\n",
    "    print('*****************Finish Parameter****************')\n",
    "\n",
    "def get_memory_usage(device):\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024*1024.)\n",
    "    cached_memory = torch.cuda.memory_cached(device) / (1024*1024.)\n",
    "    return allocated_memory, cached_memory\n",
    "    #print('Allocated Memory: {:.2f} MB, Cached Memory: {:.2f} MB'.format(allocated_memory, cached_memory))\n",
    "\n",
    "\n",
    "def MAE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true-pred))\n",
    "\n",
    "def MSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean((pred - true) ** 2)\n",
    "\n",
    "def RMSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.mean((pred - true) ** 2))\n",
    "\n",
    "def RRSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.sum((pred - true) ** 2)) / torch.sqrt(torch.sum((pred - true.mean()) ** 2))\n",
    "\n",
    "\n",
    "\n",
    "def MAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(torch.div((true - pred), true)))\n",
    "\n",
    "def PNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    indicator = torch.gt(pred - true, 0).float()\n",
    "    return indicator.mean()\n",
    "\n",
    "def oPNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    bias = (true+pred) / (2*true)\n",
    "    return bias.mean()\n",
    "\n",
    "def MARE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.div(torch.sum(torch.abs((true - pred))), torch.sum(true))\n",
    "\n",
    "def SMAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true-pred)/(torch.abs(true)+torch.abs(pred)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def All_Metrics(pred, true, mask1, mask2):\n",
    "    #mask1 filter the very small value, mask2 filter the value lower than a defined threshold\n",
    "    assert type(pred) == type(true)\n",
    "    #if type(pred) == np.ndarray:\n",
    "    #    mae  = MAE_np(pred, true, mask1)\n",
    "    #    rmse = RMSE_np(pred, true, mask1)\n",
    "    #    mape = MAPE_np(pred, true, mask2)\n",
    "    #    rrse = RRSE_np(pred, true, mask1)\n",
    "\n",
    "        #corr = CORR_np(pred, true, mask1)\n",
    "        #pnbi = PNBI_np(pred, true, mask1)\n",
    "        #opnbi = oPNBI_np(pred, true, mask2)\n",
    "    if type(pred) == torch.Tensor:\n",
    "        mae  = MAE_torch(pred, true, mask1)\n",
    "        rmse = RMSE_torch(pred, true, mask1)\n",
    "        rrse = RRSE_torch(pred, true, mask1)\n",
    "\n",
    "        #pnbi = PNBI_torch(pred, true, mask1)\n",
    "        #opnbi = oPNBI_torch(pred, true, mask2)\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return mae, rmse, rrse\n",
    "\n",
    "def SIGIR_Metrics(pred, true, mask1, mask2):\n",
    "    rrse = RRSE_torch(pred, true, mask1)\n",
    "    corr = CORR_torch(pred, true, 0)\n",
    "    return rrse, corr\n",
    "\n",
    "def save_model(model, model_dir, epoch=None):\n",
    "    if model_dir is None:\n",
    "        return\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    epoch = str(epoch) if epoch else \"\"\n",
    "    file_name = os.path.join(model_dir, epoch + \"_stemgnn.pt\")\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        torch.save(model, f)\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, loss, optimizer, train_loader, val_loader, test_loader,\n",
    "                 args, lr_scheduler=None):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        # self.scaler = scaler\n",
    "        self.args = args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_per_epoch = len(train_loader)\n",
    "        if val_loader != None:\n",
    "            self.val_per_epoch = len(val_loader)\n",
    "        self.best_path = os.path.join(self.args.get('log_dir'), 'best_model.pth')\n",
    "        self.loss_figure_path = os.path.join(self.args.get('log_dir'), 'loss.png')\n",
    "        # log\n",
    "        if os.path.isdir(args.get('log_dir')) == False and not args.get('debug'):\n",
    "            os.makedirs(args.get('log_dir'), exist_ok=True)\n",
    "        self.logger = get_logger(args.get('log_dir'), name=args.get('model'), debug=args.get('debug'))\n",
    "        self.logger.info('Experiment log path in: {}'.format(args.get('log_dir')))\n",
    "        # if not args.debug:\n",
    "        # self.logger.info(\"Argument: %r\", args)\n",
    "        # for arg, value in sorted(vars(args).items()):\n",
    "        #     self.logger.info(\"Argument %s: %r\", arg, value)\n",
    "\n",
    "    def val_epoch(self, epoch, val_dataloader):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = self.model(data)\n",
    "                # if self.args.get('real_value'):\n",
    "                # label = self.scaler.inverse_transform(label)\n",
    "                loss = self.loss(output, label)\n",
    "                # a whole batch of Metr_LA is filtered\n",
    "                if not torch.isnan(loss):\n",
    "                    total_val_loss += loss.item()\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        self.logger.info('**********Val Epoch {}: average Loss: {:.6f}'.format(epoch, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        loss_values=[]\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data = data\n",
    "            label = target  # (..., 1)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # data and target shape: B, T, N, F; output shape: B, T, N, F\n",
    "            output = self.model(data)\n",
    "            # if self.args.get('real_value'):\n",
    "            #   label = self.scaler.inverse_transform(label)\n",
    "\n",
    "\n",
    "            loss = self.loss(output, label)\n",
    "            loss = self.loss(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # add max grad clipping\n",
    "            if self.args.get('grad_norm'):\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.get('max_grad_norm'))\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # log information\n",
    "            if batch_idx % self.args.get('log_step') == 0:\n",
    "                self.logger.info('Train Epoch {}: {}/{} Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx, self.train_per_epoch, loss.item()))\n",
    "        train_epoch_loss = total_loss / self.train_per_epoch\n",
    "        self.logger.info(\n",
    "            '**********Train Epoch {}: averaged Loss: {:.6f}'.format(epoch, train_epoch_loss))\n",
    "\n",
    "        # learning rate decay\n",
    "        if self.args.get('lr_decay'):\n",
    "            self.lr_scheduler.step()\n",
    "        return train_epoch_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        not_improved_count = 0\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, self.args.get('epochs') + 1):\n",
    "            # epoch_time = time.time()\n",
    "            train_epoch_loss = self.train_epoch(epoch)\n",
    "            # print(time.time()-epoch_time)\n",
    "            # exit()\n",
    "            if self.val_loader == None:\n",
    "                val_dataloader = self.test_loader\n",
    "            else:\n",
    "                val_dataloader = self.val_loader\n",
    "            val_epoch_loss = self.val_epoch(epoch, val_dataloader)\n",
    "\n",
    "            # print('LR:', self.optimizer.param_groups[0]['lr'])\n",
    "            train_loss_list.append(train_epoch_loss)\n",
    "            val_loss_list.append(val_epoch_loss)\n",
    "            if train_epoch_loss > 1e6:\n",
    "                self.logger.warning('Gradient explosion detected. Ending...')\n",
    "                break\n",
    "            # if self.val_loader == None:\n",
    "            # val_epoch_loss = train_epoch_loss\n",
    "            if val_epoch_loss < best_loss:\n",
    "                best_loss = val_epoch_loss\n",
    "                not_improved_count = 0\n",
    "                best_state = True\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "                best_state = False\n",
    "            # early stop\n",
    "            if self.args.get('early_stop'):\n",
    "                if not_improved_count == self.args.get('early_stop_patience'):\n",
    "                    self.logger.info(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                                     \"Training stops.\".format(self.args.get('early_stop_patience')))\n",
    "                    break\n",
    "            # save the best state\n",
    "            if best_state == True:\n",
    "                self.logger.info('*********************************Current best model saved!')\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        self.logger.info(\"Total training time: {:.4f}min, best loss: {:.6f}\".format((training_time / 60), best_loss))\n",
    "\n",
    "        with open('milan_sms_mamaba.txt', 'a') as f:\n",
    "            f.write(str(epoch))\n",
    "            f.write('\\n')\n",
    "            f.write(str(training_time / 60))\n",
    "            f.write('\\n')\n",
    "\n",
    "        # save the best model to file\n",
    "        if not self.args.get('debug'):\n",
    "            torch.save(best_model, self.best_path)\n",
    "            self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "        # test\n",
    "        self.model.load_state_dict(best_model)\n",
    "        # self.val_epoch(self.args.epochs, self.test_loader)\n",
    "        y1, y2 = self.test(self.model, self.args, self.test_loader, self.logger)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        state = {\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'config': self.args\n",
    "        }\n",
    "        torch.save(state, self.best_path)\n",
    "        self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def test(model, args, data_loader, logger, path=None):\n",
    "        if path != None:\n",
    "            check_point = torch.load(path)\n",
    "            state_dict = check_point['state_dict']\n",
    "            args = check_point['config']\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(args.get('device'))\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = model(data)\n",
    "\n",
    "                y_true.append(label)\n",
    "                y_pred.append(output)\n",
    "\n",
    "                #print(model.forward(data, [], teacher_forcing_ratio=0))\n",
    "        # y_true = scaler.inverse_transform(torch.cat(y_true, dim=0))\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        # if not args.get('real_value'):\n",
    "        #    y_pred = torch.cat(y_pred, dim=0)\n",
    "        # else:\n",
    "        # y_pred = scaler.inverse_transform(torch.cat(y_pred, dim=0))\n",
    "        # np.save('./{}_true.npy'.format(args.get('dataset')), y_true.cpu().numpy())\n",
    "        # np.save('./{}_pred.npy'.format(args.get('dataset')), y_pred.cpu().numpy())\n",
    "        # for t in range(y_true.shape[1]):\n",
    "        #    mae, rmse, mape, _ = All_Metrics(y_pred[:, t, ...], y_true[:, t, ...],\n",
    "        #                                        args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        #    logger.info(\"Horizon {:02d}, MAE: {:.2f}, RMSE: {:.2f}, MAPE: {:.4f}%\".format(\n",
    "        #        t + 1, mae, rmse, mape*100))\n",
    "        mae, rmse, _ = All_Metrics(y_pred, y_true, args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        logger.info(\"Average Horizon, MAE: {:.4f}, MSE: {:.4f}\".format(\n",
    "            mae, rmse))\n",
    "        return y_pred, y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_sampling_threshold(global_step, k):\n",
    "        \"\"\"\n",
    "        Computes the sampling probability for scheduled sampling using inverse sigmoid.\n",
    "        :param global_step:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return k / (k + math.exp(global_step / k))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542fef61-7668-4bf3-ae0f-abe8a0ab7f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m#init model\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     model \u001b[38;5;241m=\u001b[39m SAMBA(ModelArgs(args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_in\u001b[39m\u001b[38;5;124m\"\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlag\u001b[39m\u001b[38;5;124m'\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m'\u001b[39m)),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhid\u001b[39m\u001b[38;5;124m'\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlag\u001b[39m\u001b[38;5;124m'\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m'\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m'\u001b[39m),args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheb_k\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 149\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1053\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1053\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/network/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    seq_in: int\n",
    "    seq_out: int\n",
    "    d_state: int =128\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 3\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Ensure the tensors are of type float32\n",
    "    x = x.float()\n",
    "    y = y.float()\n",
    "\n",
    "    # Compute the mean of each tensor\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "\n",
    "    # Compute the deviations from the mean\n",
    "    dev_x = x - mean_x\n",
    "    dev_y = y - mean_y\n",
    "\n",
    "    # Compute the covariance between x and y\n",
    "    covariance = torch.sum(dev_x * dev_y)\n",
    "\n",
    "    # Compute the standard deviations of x and y\n",
    "    std_x = torch.sqrt(torch.sum(dev_x ** 2))\n",
    "    std_y = torch.sqrt(torch.sum(dev_y ** 2))\n",
    "\n",
    "    # Compute the Pearson correlation coefficient\n",
    "    pearson_corr = covariance / (std_x * std_y)\n",
    "\n",
    "    return pearson_corr\n",
    "\n",
    "def rank_tensor(x):\n",
    "    \"\"\"\n",
    "    Return the ranks of elements in a tensor.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): Input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Ranks of the input tensor elements.\n",
    "    \"\"\"\n",
    "    # Get the sorted indices\n",
    "    sorted_indices = torch.argsort(x)\n",
    "    \n",
    "    # Create an empty tensor to hold the ranks\n",
    "    ranks = torch.zeros_like(sorted_indices, dtype=torch.float)\n",
    "    \n",
    "    # Assign ranks based on sorted indices\n",
    "    ranks[sorted_indices] = torch.arange(1, len(x) + 1).float()\n",
    "    \n",
    "    return ranks\n",
    "\n",
    "def rank_information_coefficient(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Rank Information Coefficient (RIC) or Spearman's Rank Correlation Coefficient.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Rank Information Coefficient (RIC).\n",
    "    \"\"\"\n",
    "    # Get the ranks of the elements in x and y\n",
    "    rank_x = rank_tensor(x)\n",
    "    rank_y = rank_tensor(y)\n",
    "\n",
    "    # Calculate the mean rank for both tensors\n",
    "    mean_rank_x = torch.mean(rank_x)\n",
    "    mean_rank_y = torch.mean(rank_y)\n",
    "\n",
    "    # Calculate the covariance of the rank variables\n",
    "    covariance = torch.sum((rank_x - mean_rank_x) * (rank_y - mean_rank_y))\n",
    "\n",
    "    # Calculate the standard deviations of the ranks\n",
    "    std_rank_x = torch.sqrt(torch.sum((rank_x - mean_rank_x) ** 2))\n",
    "    std_rank_y = torch.sqrt(torch.sum((rank_y - mean_rank_y) ** 2))\n",
    "\n",
    "    # Calculate the Spearman rank correlation (RIC)\n",
    "    ric = covariance / (std_rank_x * std_rank_y)\n",
    "    \n",
    "    return ric\n",
    "\n",
    "def cal_r2(x, y):\n",
    "    true = pd.DataFrame(x)\n",
    "    pred = pd.DataFrame(y)\n",
    "    ss_total = np.sum((true[0] - np.mean(true[0]))**2)\n",
    "    ss_residual = np.sum((true[0] - pred[0])**2)\n",
    "    r2_out_of_sample = 1 - (ss_residual / ss_total)\n",
    "    return r2_out_of_sample\n",
    "    \n",
    "for i in range(5):\n",
    "\n",
    "    Mode = 'train'\n",
    "    DEBUG = 'True'\n",
    "    DATASET = 'PEMSD8'      #PEMSD4 or PEMSD8\n",
    "    DEVICE = 'cuda:0'\n",
    "    MODEL = 'AGCRN'\n",
    "\n",
    "#get configuration\n",
    "    config_file = './{}_{}.conf'.format(DATASET, MODEL)\n",
    "#print('Read configuration file: %s' % (config_file))\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parser\n",
    "\n",
    "    args={\"dataset\":DATASET,\"mode\":Mode,\"device\":DEVICE,\"debug\":DEBUG,\"model\":MODEL,\"cuda\":True,\"val_ratio\":0.15,\"test_ratio\":0.15,\n",
    "      \"lag\":window,\"horizon\":predict,\"num_nodes\":XX.shape[2],\"tod\":False,\"normalizer\":'std',\"column_wise\":False,\"default_graph\":True,\n",
    "     \"input_dim\":1,\"output_dim\":1,\"embed_dim\":10,\"rnn_units\":128,\"num_layers\":5,\"cheb_k\":3,\"loss_func\":'mae',\"seed\":1,\n",
    "     \"batch_size\":256,\"epochs\":1500,\"lr_init\":0.0001,\"lr_decay\":True,\"lr_decay_rate\":0.5,\"lr_decay_step\":[40,70,100],\n",
    "      \"early_stop\":True,\"early_stop_patience\":200,\"grad_norm\":False,\"max_grad_norm\":5,\"real_value\":False,\"mae_thresh\":None,\n",
    "      \"mape_thresh\":0,\"log_dir\":'./',\"log_step\":20,\"plot\":False,\"teacher_forcing\":False,\"d_in\":32,\"hid\":32}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#init model\n",
    "    model = SAMBA(ModelArgs(args.get(\"d_in\"),args.get(\"num_layers\"),args.get(\"num_nodes\"),args.get('lag'),args.get('horizon')),args.get('hid'),args.get('lag'),args.get('horizon'),args.get('embed_dim'),args.get(\"cheb_k\"))\n",
    "    model = model.cuda()\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p)\n",
    "    print_model_parameters(model, only_num=False)\n",
    "\n",
    "    if args.get('loss_func') == 'mask_mae':\n",
    "        loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "    elif args.get('loss_func') == 'mae':\n",
    "        loss = torch.nn.L1Loss().to(args.get('device'))\n",
    "    elif args.get('loss_func') == 'mse':\n",
    "        loss = torch.nn.MSELoss().to(args.get('device'))\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=args.get('lr_init'), eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "#learning rate decay\n",
    "    lr_scheduler = None\n",
    "    if args.get('lr_decay'):\n",
    "        print('Applying learning rate decay.')\n",
    "        lr_decay_steps = [int(i) for i in args.get('lr_decay_step')]\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "        milestones=[0.5 * args.get('epochs'),0.7 * args.get('epochs'), 0.9 * args.get('epochs')],gamma=0.1)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "    trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, args=args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    y1,y2=trainer.test(trainer.model, trainer.args, test_loader, trainer.logger)\n",
    "    \n",
    "    y_p=np.array(y1[:,0,:].cpu())\n",
    "\n",
    "    y_t=np.array(y2[:,0,:].cpu())\n",
    "\n",
    "\n",
    "#y_p=(y_p-mean)/std\n",
    "#y_t=(y_t-mean)/std\n",
    "\n",
    "    y_p=torch.tensor(y_p)\n",
    "    y_t=torch.tensor(y_t)\n",
    "\n",
    "    diff = y_p[1:] - y_p[:-1]\n",
    "    return_p = diff / y_p[:-1]\n",
    "    #return_p = y_p\n",
    "    \n",
    "    diff = y_t[1:] - y_t[:-1]\n",
    "    return_t = diff / y_t[:-1]\n",
    "    #return_t = y_t\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    mae, rmse, _=All_Metrics(return_p,return_t, None,None )\n",
    "\n",
    "    IC=pearson_correlation(return_t,return_p)\n",
    "    \n",
    "    RIC=rank_information_coefficient(return_t[:,0],return_p[:,0])\n",
    "\n",
    "    R2 = cal_r2(return_t,return_p)\n",
    "    \n",
    "    result_train_file = os.path.join(\"SAMBA_Data2023_return_250306\")\n",
    "\n",
    "    \n",
    "\n",
    "    save_model(trainer,result_train_file,i+1)\n",
    "\n",
    "    with open('SAMBA_Data2023return_250306', 'a') as f:\n",
    "        f.write(str(np.array(IC)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(RIC)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(mae)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(rmse)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(R2)))\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "    saved_return_p = return_p  # Store return_p in a variable\n",
    "    np.save(f'return_p1_{i+1}.npy', saved_return_p.cpu().numpy())  # Save to .npy file\n",
    "    \n",
    "\n",
    "    saved_return_t = return_t  # Store return_p in a variable\n",
    "    np.save(f'return_t1_{i+1}.npy', saved_return_t.cpu().numpy())  # Save to .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd9b8bc-ac0f-4c5f-add0-28a01029eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Metrics: Data2023 Monthly\n",
      "----------------------------------------------------------------------\n",
      "Index              RMSE           IC          RIC           R2\n",
      "----------------------------------------------------------------------\n",
      "Epoch_5           0.0399↓     0.4401↑      0.3137↑      0.1749↑\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_metrics(file_path):\n",
    "    metrics = {}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        num_epochs = len(lines) // 5\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            IC = float(lines[i * 5].strip())\n",
    "            RIC = float(lines[i * 5 + 1].strip())\n",
    "            MAE = float(lines[i * 5 + 2].strip())\n",
    "            RMSE = float(lines[i * 5 + 3].strip())\n",
    "            R2 = float(lines[i * 5 + 4].strip())\n",
    "            \n",
    "            metrics[f\"Epoch_{i+1}\"] = {\n",
    "                \"IC\": IC,\n",
    "                \"RIC\": RIC,\n",
    "                \"MAE\": MAE,\n",
    "                \"RMSE\": RMSE,\n",
    "                'R2': R2,\n",
    "            }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "file_path = 'SAMBA_Data2023return_change'\n",
    "metrics = read_metrics(file_path)\n",
    "\n",
    "\n",
    "print(\"Prediction Metrics: Data2023 Monthly\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Index              RMSE           IC          RIC           R2\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "# 最后一个结果代表训练到最后的情况\n",
    "for epoch, metrics_values in list(metrics.items())[-1:]:\n",
    "    rmse_str = f\"{metrics_values['RMSE']:.4f}\"\n",
    "    ic_str = f\"{metrics_values['IC']:.4f}\"\n",
    "    ric_str = f\"{metrics_values['RIC']:.4f}\"\n",
    "    r2_str = f\"{metrics_values['R2']:.4f}\"\n",
    "    \n",
    "    rmse_change = \"↓\" \n",
    "    ic_change = \"↑\" \n",
    "    ric_change = \"↑\" \n",
    "    R2_change = \"↑\"\n",
    "    \n",
    "    print(f\"{epoch:<18}{rmse_str}{rmse_change}     {ic_str}{ic_change}      {ric_str}{ric_change}      {r2_str}{R2_change}\")\n",
    "print(\"----------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadbf74-171e-41d2-b22c-c38385c8346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "11122222"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
