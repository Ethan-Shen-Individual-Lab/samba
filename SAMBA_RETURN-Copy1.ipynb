{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5a6d84-1b9b-43c9-aa1c-d3e73207e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import argparse\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SAMBA(nn.Module):\n",
    "    def __init__(self, ModelArgs,hidden,inp,out,embed,cheb_k):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "\n",
    "        self.mam1 = Mamba(ModelArgs,hidden)\n",
    "\n",
    "        \n",
    "\n",
    "        self.cheb_k=cheb_k\n",
    "\n",
    "        self.gamma=nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "       \n",
    "\n",
    "  \n",
    "\n",
    "        self.adj=nn.Parameter(torch.randn(ModelArgs.vocab_size,embed), requires_grad=True)\n",
    "\n",
    "        self.embed_w=nn.Parameter(torch.randn(embed,embed), requires_grad=True)\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, out))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed, out))\n",
    "\n",
    "        self.proj=nn.Linear(ModelArgs.vocab_size,1)\n",
    "        self.proj_seq=nn.Linear(ModelArgs.seq_in,1)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def gaussian_kernel_graph(self,E_A,x ,gamma=1.0):\n",
    "    \n",
    "    # Compute pairwise squared Euclidean distance\n",
    "\n",
    "        x_mean=torch.mean(x,dim=0)\n",
    "\n",
    "        x_time=torch.mm(x_mean.permute(1,0),x_mean)\n",
    "\n",
    "        \n",
    "        N = E_A.size(0)\n",
    "    # Expanding the dimensions to compute pairwise differences\n",
    "        E_A_expanded = E_A.unsqueeze(0).expand(N, N, -1)\n",
    "        E_A_T_expanded = E_A.unsqueeze(1).expand(N, N, -1)\n",
    "    # Pairwise squared Euclidean distances\n",
    "        distance_matrix = torch.sum((E_A_expanded - E_A_T_expanded)**2, dim=2)\n",
    "    \n",
    "    # Apply Gaussian kernel\n",
    "        A = torch.exp(-gamma * distance_matrix)\n",
    "\n",
    "        dr=nn.Dropout(0.35)\n",
    "\n",
    "        #A=torch.tanh(torch.mm(self.adj, self.adj.transpose(0, 1))+1)\n",
    "    \n",
    "    # Optional: Normalize the adjacency matrix with softmax (row-wise)\n",
    "        A = F.softmax(A, dim=1)\n",
    "    \n",
    "        return dr(A)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        #x_mean=torch.mean(input_ids,dim=0)\n",
    "\n",
    "        #ADJ=F.softmax(torch.mm(x_mean.permute(1,0),x_mean),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        xx=self.mam1(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "        #m = nn.LeakyReLU(0.1)\n",
    "        #ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "        #dr=nn.Dropout(0.35)\n",
    "        #ADJ=dr(F.softmax(F.relu(torch.mm(torch.mm(self.adj, self.embed_w),self.adj.transpose(0, 1))),dim=1))\n",
    "        \n",
    "        ADJ=self.gaussian_kernel_graph(self.adj,xx,gamma=self.gamma)\n",
    "\n",
    "        #degree = torch.sum(ADJ, dim=1)\n",
    "        # laplacian is sym or not\n",
    "        #attention = 0.5 * (attention + attention.T)\n",
    "        #degree_l = torch.diag(degree)+1e-5\n",
    "        \n",
    "        #deg=torch.diag(1 / (degree + 1e-5))\n",
    "        \n",
    "        #diagonal_degree_hat = torch.diag(1 / (torch.sqrt(degree) + 1e-5))\n",
    "        #attention = torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))#milan\n",
    "        #A=torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))\n",
    "        #r=torch.rand(1).cuda()\n",
    "        \n",
    "        #d1=torch.diag(1 / (torch.pow(degree,1-r) + 1e-5))\n",
    "        #d2=torch.diag(1 / (torch.pow(degree,r) + 1e-5))\n",
    "        \n",
    "        \n",
    "        #L = torch.eye(input_ids.size(2)).cuda()-torch.matmul(d1,torch.matmul(ADJ,d2))\n",
    "\n",
    "        I=torch.eye(input_ids.size(2)).cuda()\n",
    "\n",
    "        #L=I-ADJ\n",
    "\n",
    "        #out=self.mlp(xx)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        support_set = [I,ADJ]#(math.sqrt(2)/2)*L,(-math.sqrt(2)/2)*L]\n",
    "    \n",
    "        \n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "        weights = torch.einsum('nd,dkio->nkio', self.adj, self.weights_pool)  #N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)                       #N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, xx.permute(0,2,1))      #B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out = torch.einsum('bnki,nkio->bno', x_g, weights) + bias#B,N,D_OUT\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return self.proj(out.permute(0,2,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs,hid):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "        self.nl=args.n_layer\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        self.layers2 = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        #self.layers3 = nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "\n",
    "        #self.layers3=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "        \n",
    "        #self.layers4=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),gconv(args.seq_in,hid,2,10,args.d_model),nn.ReLU(),gconv(hid,args.seq_in,2,10,args.d_model)) for _ in range(args.n_layer)])\n",
    "      \n",
    "       \n",
    "\n",
    "        self.lin=nn.ModuleList([nn.Sequential(nn.LayerNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "        \n",
    "        #self.lin2=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "        \n",
    "        \n",
    "        self.norm_f = nn.LayerNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size)\n",
    "\n",
    "\n",
    "        self.proj=nn.Sequential(nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))\n",
    "\n",
    "        self.nnl=nn.LayerNorm(args.vocab_size)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "      \n",
    "        #self.proj=nn.Linear(2*ModelArgs.vocab_size, ModelArgs.vocab_size)\n",
    "        #self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "                                                     # See \"Weight Tying\" paper\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        x1=x\n",
    "        x2=x\n",
    "    \n",
    "\n",
    "        for i in range(self.nl):\n",
    "            \n",
    "            x1 = self.layers[i](x1)\n",
    "            x2=self.layers2[i](x2.flip([1]))\n",
    "            \n",
    "            x=x1+x2.flip([1])+x\n",
    "\n",
    "            x=self.lin[i](x.permute(0,2,1)).permute(0,2,1)+x\n",
    "\n",
    "            x1=x\n",
    "            x2=x\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        x = self.norm_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        \n",
    "\n",
    "#        a=logits.shape\n",
    "\n",
    " #       #sq=torch.reshape(logits,(a[0],a[2],a[1]))\n",
    "\n",
    "  #      out=self.out(sq)\n",
    "\n",
    "   #     b=out.shape\n",
    "\n",
    "    #    out=torch.reshape(out,(b[0],b[2],b[1]))\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name: str):\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name: One of\n",
    "                * 'state-spaces/mamba-2.8b-slimpj'\n",
    "                * 'state-spaces/mamba-2.8b'\n",
    "                * 'state-spaces/mamba-1.4b'\n",
    "                * 'state-spaces/mamba-790m'\n",
    "                * 'state-spaces/mamba-370m'\n",
    "                * 'state-spaces/mamba-130m'\n",
    "\n",
    "        Returns:\n",
    "            model: Mamba model with weights loaded\n",
    "\n",
    "        \"\"\"\n",
    "        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "        from transformers.utils.hub import cached_file\n",
    "\n",
    "        def load_config_hf(model_name):\n",
    "            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return json.load(open(resolved_archive_file))\n",
    "\n",
    "\n",
    "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
    "            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n",
    "\n",
    "        config_data = load_config_hf(pretrained_model_name)\n",
    "        args = ModelArgs(\n",
    "            d_model=config_data['d_model'],\n",
    "            n_layer=config_data['n_layer'],\n",
    "            vocab_size=config_data['vocab_size']\n",
    "        )\n",
    "        model = Mamba(args)\n",
    "\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
    "        new_state_dict = {}\n",
    "        for key in state_dict:\n",
    "            new_key = key.replace('backbone.', '')\n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = nn.LayerNorm(args.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "\n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.mixer(self.norm(x)) \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class gconv(nn.Module):\n",
    "    def __init__(self, inp, hid,embed,cheb_k,n):\n",
    "        super(gconv, self).__init__()\n",
    "\n",
    "        self.node_num=n\n",
    "\n",
    "        self.inp=inp\n",
    "\n",
    "        self.cheb_k=cheb_k\n",
    "\n",
    "        self.adj=nn.Parameter(torch.randn(n,embed), requires_grad=True)\n",
    "       \n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, hid))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed,hid))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        #output shape [B, N, C]\n",
    "        \n",
    "        ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        support_set = [torch.eye(self.node_num).cuda(),ADJ]\n",
    "    \n",
    "        \n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "        weights = torch.einsum('nd,dkio->nkio', self.adj, self.weights_pool)  #N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)                       #N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, x)      #B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out_6 = torch.einsum('bnki,nkio->bno', x_g, weights) + bias   #B,N,D_OUT\n",
    "\n",
    "        return out_6\n",
    "\n",
    "class AVWGCN(nn.Module):\n",
    "    def __init__(self, dim_in, hid, cheb_k,n):\n",
    "        super(AVWGCN, self).__init__()\n",
    "\n",
    "        self.node_num=n\n",
    "\n",
    "        self.inp=dim_in\n",
    "\n",
    "        self.cheb_k = cheb_k\n",
    "        self.node_embeddings = nn.Parameter(torch.randn(n,dim_in,dim_in), requires_grad=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(cheb_k,n,dim_in, hid))\n",
    "        \n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(n, hid))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        #output shape [B, N, C]\n",
    "        \n",
    "        supports = F.softmax(F.relu(self.node_embeddings), dim=2)\n",
    "\n",
    "        I=torch.eye(self.inp).cuda()\n",
    "\n",
    "        I2=I[None,:,:].repeat(x.size(1),1,1)\n",
    "        \n",
    "\n",
    "        support_set = [I2, supports]\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "        \n",
    "                              #N, dim_out\n",
    "        x_g = torch.einsum(\"bnc,kncm->bknm\", x, supports)      #B, cheb_k, N, dim_in\n",
    "        #x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        x_gconv = torch.einsum('bknm,knmo->bno', x_g, self.weights_pool) + self.bias_pool     #b, N, dim_out\n",
    "        return x_gconv\n",
    "\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.in_proj_r = nn.Linear(args.d_model, args.d_inner, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size,bias=False)\n",
    "\n",
    "        #self.x_proj_r = nn.Linear(args.d_inner, args.dt_rank + args.d_state, bias=True)\n",
    "\n",
    "        #self.x_proj = FourierKANLayer(args.d_inner, args.dt_rank + args.d_state * 2, 100)\n",
    "\n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "        #self.dt_proj=FourierKANLayer(args.dt_rank, args.d_inner, 100)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "\n",
    "        #x=self.embedding(x)\n",
    "\n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
    "\n",
    "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
    "    \n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
    "\n",
    "\n",
    "\n",
    "        x = F.silu(x)\n",
    "\n",
    "        gate=x*(1-F.sigmoid(res))\n",
    "\n",
    "        \n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * F.silu(res)\n",
    "\n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        #o1=self.norm_f(output)\n",
    "\n",
    "        #o2=self.lm_head(o1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "\n",
    "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "\n",
    "        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def selective_scan(self, u, delta, A, B, C, D):\n",
    "        \"\"\"Does selective scan algorithm. See:\n",
    "            - Section 2 State Space Models in the Mamba paper [1]\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        This is the classic discrete state space formula:\n",
    "            x(t + 1) = Ax(t) + Bu(t)\n",
    "            y(t)     = Cx(t) + Du(t)\n",
    "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
    "\n",
    "        Args:\n",
    "            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "            delta: shape (b, l, d_in)\n",
    "            A: shape (d_in, n)\n",
    "            B: shape (b, l, n)\n",
    "            C: shape (b, l, n)\n",
    "            D: shape (d_in,)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
    "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d_in) = u.shape\n",
    "        n = A.shape[1]\n",
    "\n",
    "        # Discretize continuous parameters (A, B)\n",
    "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
    "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
    "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
    "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n",
    "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
    "\n",
    "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
    "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n",
    "        # is additionally hardware-aware (like FlashAttention).\n",
    "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
    "        ys = []\n",
    "        for i in range(l):\n",
    "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
    "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
    "\n",
    "        y = y + u * D\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293f11fd-3038-417c-bef4-4368a25e7d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          price      d12       e12     AAA     BAA     lty     tbl     Rfree  \\\n",
      "yyyymm                                                                         \n",
      "187101     4.44   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.000000   \n",
      "187102     4.50   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004967   \n",
      "187103     4.61   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004525   \n",
      "187104     4.74   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004252   \n",
      "187105     4.86   0.2600    0.4000  0.0000  0.0000  0.0000  0.0000  0.004643   \n",
      "...         ...      ...       ...     ...     ...     ...     ...       ...   \n",
      "202308  4507.66  69.1137  183.1700  0.0495  0.0602  0.0417  0.0530  0.004500   \n",
      "202309  4288.05  69.3131  184.2500  0.0513  0.0616  0.0438  0.0532  0.004300   \n",
      "202310  4193.80  69.6433  186.9767  0.0561  0.0663  0.0480  0.0534  0.004700   \n",
      "202311  4567.80  69.9735  189.7033  0.0528  0.0629  0.0450  0.0527  0.004400   \n",
      "202312  4769.83  70.3037  192.4300  0.0474  0.0564  0.0402  0.0524  0.004300   \n",
      "\n",
      "             d/p       d/y       e/p       d/e       b/m     tms     dfy  \\\n",
      "yyyymm                                                                     \n",
      "187101  0.058559  0.000000  0.090090  0.650000  0.000000  0.0000  0.0000   \n",
      "187102  0.057778  0.058559  0.088889  0.650000  0.000000  0.0000  0.0000   \n",
      "187103  0.056399  0.057778  0.086768  0.650000  0.000000  0.0000  0.0000   \n",
      "187104  0.054852  0.056399  0.084388  0.650000  0.000000  0.0000  0.0000   \n",
      "187105  0.053498  0.054852  0.082305  0.650000  0.000000  0.0000  0.0000   \n",
      "...          ...       ...       ...       ...       ...     ...     ...   \n",
      "202308  0.015333  0.015061  0.040635  0.377320  0.210885 -0.0113  0.0107   \n",
      "202309  0.016164  0.015377  0.042968  0.376191  0.218528 -0.0094  0.0103   \n",
      "202310  0.016606  0.016241  0.044584  0.372471  0.221534 -0.0054  0.0102   \n",
      "202311  0.015319  0.016685  0.041531  0.368858  0.203676 -0.0077  0.0101   \n",
      "202312  0.014739  0.015391  0.040343  0.365347  0.194280 -0.0122  0.0090   \n",
      "\n",
      "            infl      svar  trend  \n",
      "yyyymm                             \n",
      "187101  0.000000  0.000000      1  \n",
      "187102  0.000000  0.000000      1  \n",
      "187103  0.000000  0.000000      1  \n",
      "187104  0.000000  0.000000      1  \n",
      "187105  0.000000  0.000000      0  \n",
      "...          ...       ...    ...  \n",
      "202308  0.004367  0.001337      0  \n",
      "202309  0.002485  0.001023      0  \n",
      "202310 -0.000383  0.001678      1  \n",
      "202311 -0.002015  0.001341      1  \n",
      "202312 -0.000993  0.000797      0  \n",
      "\n",
      "[1836 rows x 18 columns]\n",
      "tensor([[[ 2.4920e-01,  2.6920e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           2.2240e-03,  0.0000e+00],\n",
      "         [ 2.5000e-01,  2.6000e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.6700e-03,  0.0000e+00],\n",
      "         [ 2.4670e-01,  2.5170e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           3.0610e-03,  1.0000e+00],\n",
      "         [ 2.4330e-01,  2.4330e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.6900e-04,  1.0000e+00],\n",
      "         [ 2.4000e-01,  2.3500e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.1850e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 2.5000e-01,  2.6000e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.6700e-03,  0.0000e+00],\n",
      "         [ 2.4670e-01,  2.5170e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           3.0610e-03,  1.0000e+00],\n",
      "         [ 2.4330e-01,  2.4330e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.6900e-04,  1.0000e+00],\n",
      "         [ 2.4000e-01,  2.3500e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.1850e-03,  1.0000e+00],\n",
      "         [ 2.3670e-01,  2.2670e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           7.0700e-04,  0.0000e+00]],\n",
      "\n",
      "        [[ 2.4670e-01,  2.5170e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           3.0610e-03,  1.0000e+00],\n",
      "         [ 2.4330e-01,  2.4330e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.6900e-04,  1.0000e+00],\n",
      "         [ 2.4000e-01,  2.3500e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.1850e-03,  1.0000e+00],\n",
      "         [ 2.3670e-01,  2.2670e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           7.0700e-04,  0.0000e+00],\n",
      "         [ 2.3330e-01,  2.1830e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.6140e-03,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.2803e+01,  8.9283e+01,  3.9500e-02,  ..., -4.5000e-04,\n",
      "           1.6190e-03,  1.0000e+00],\n",
      "         [ 4.3095e+01,  8.7907e+01,  4.0600e-02,  ..., -2.1110e-03,\n",
      "           1.1190e-03,  0.0000e+00],\n",
      "         [ 4.3388e+01,  8.6530e+01,  3.9700e-02,  ..., -3.4170e-03,\n",
      "           2.8380e-03,  0.0000e+00],\n",
      "         [ 4.3551e+01,  8.6500e+01,  4.0000e-02,  ...,  1.6530e-03,\n",
      "           4.3010e-03,  0.0000e+00],\n",
      "         [ 4.3713e+01,  8.6470e+01,  3.9600e-02,  ...,  8.2300e-04,\n",
      "           2.6020e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 4.3095e+01,  8.7907e+01,  4.0600e-02,  ..., -2.1110e-03,\n",
      "           1.1190e-03,  0.0000e+00],\n",
      "         [ 4.3388e+01,  8.6530e+01,  3.9700e-02,  ..., -3.4170e-03,\n",
      "           2.8380e-03,  0.0000e+00],\n",
      "         [ 4.3551e+01,  8.6500e+01,  4.0000e-02,  ...,  1.6530e-03,\n",
      "           4.3010e-03,  0.0000e+00],\n",
      "         [ 4.3713e+01,  8.6470e+01,  3.9600e-02,  ...,  8.2300e-04,\n",
      "           2.6020e-03,  1.0000e+00],\n",
      "         [ 4.3876e+01,  8.6440e+01,  3.8200e-02,  ...,  4.3060e-03,\n",
      "           1.2740e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 4.3388e+01,  8.6530e+01,  3.9700e-02,  ..., -3.4170e-03,\n",
      "           2.8380e-03,  0.0000e+00],\n",
      "         [ 4.3551e+01,  8.6500e+01,  4.0000e-02,  ...,  1.6530e-03,\n",
      "           4.3010e-03,  0.0000e+00],\n",
      "         [ 4.3713e+01,  8.6470e+01,  3.9600e-02,  ...,  8.2300e-04,\n",
      "           2.6020e-03,  1.0000e+00],\n",
      "         [ 4.3876e+01,  8.6440e+01,  3.8200e-02,  ...,  4.3060e-03,\n",
      "           1.2740e-03,  1.0000e+00],\n",
      "         [ 4.4071e+01,  8.6600e+01,  3.6200e-02,  ...,  4.7410e-03,\n",
      "           8.1800e-04,  1.0000e+00]]], device='cuda:0') tensor([[[ 1.6322e+01,  5.2467e+01,  7.5500e-02,  ...,  1.7270e-03,\n",
      "           5.5680e-03,  0.0000e+00],\n",
      "         [ 1.6296e+01,  5.1233e+01,  7.4500e-02,  ...,  5.7500e-04,\n",
      "           3.2720e-03,  1.0000e+00],\n",
      "         [ 1.6271e+01,  5.0000e+01,  7.2100e-02,  ..., -5.7400e-04,\n",
      "           5.2990e-03,  1.0000e+00],\n",
      "         [ 1.6172e+01,  4.8480e+01,  7.1500e-02,  ...,  6.3220e-03,\n",
      "           4.9410e-03,  0.0000e+00],\n",
      "         [ 1.6072e+01,  4.6960e+01,  7.1000e-02,  ...,  3.9980e-03,\n",
      "           2.5280e-03,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.6296e+01,  5.1233e+01,  7.4500e-02,  ...,  5.7500e-04,\n",
      "           3.2720e-03,  1.0000e+00],\n",
      "         [ 1.6271e+01,  5.0000e+01,  7.2100e-02,  ..., -5.7400e-04,\n",
      "           5.2990e-03,  1.0000e+00],\n",
      "         [ 1.6172e+01,  4.8480e+01,  7.1500e-02,  ...,  6.3220e-03,\n",
      "           4.9410e-03,  0.0000e+00],\n",
      "         [ 1.6072e+01,  4.6960e+01,  7.1000e-02,  ...,  3.9980e-03,\n",
      "           2.5280e-03,  0.0000e+00],\n",
      "         [ 1.5973e+01,  4.5440e+01,  6.9800e-02,  ...,  2.2750e-03,\n",
      "           7.1400e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.6271e+01,  5.0000e+01,  7.2100e-02,  ..., -5.7400e-04,\n",
      "           5.2990e-03,  1.0000e+00],\n",
      "         [ 1.6172e+01,  4.8480e+01,  7.1500e-02,  ...,  6.3220e-03,\n",
      "           4.9410e-03,  0.0000e+00],\n",
      "         [ 1.6072e+01,  4.6960e+01,  7.1000e-02,  ...,  3.9980e-03,\n",
      "           2.5280e-03,  0.0000e+00],\n",
      "         [ 1.5973e+01,  4.5440e+01,  6.9800e-02,  ...,  2.2750e-03,\n",
      "           7.1400e-03,  1.0000e+00],\n",
      "         [ 1.5877e+01,  4.2557e+01,  7.2000e-02,  ...,  3.9730e-03,\n",
      "           7.4260e-03,  1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.8547e+01,  1.7906e+02,  4.6700e-02,  ...,  2.5180e-03,\n",
      "           1.3690e-03,  1.0000e+00],\n",
      "         [ 6.8715e+01,  1.8101e+02,  4.6500e-02,  ...,  3.2290e-03,\n",
      "           1.1140e-03,  1.0000e+00],\n",
      "         [ 6.8914e+01,  1.8209e+02,  4.6600e-02,  ...,  1.9080e-03,\n",
      "           5.3700e-04,  0.0000e+00],\n",
      "         [ 6.9114e+01,  1.8317e+02,  4.9500e-02,  ...,  4.3670e-03,\n",
      "           1.3370e-03,  0.0000e+00],\n",
      "         [ 6.9313e+01,  1.8425e+02,  5.1300e-02,  ...,  2.4850e-03,\n",
      "           1.0230e-03,  0.0000e+00]],\n",
      "\n",
      "        [[ 6.8715e+01,  1.8101e+02,  4.6500e-02,  ...,  3.2290e-03,\n",
      "           1.1140e-03,  1.0000e+00],\n",
      "         [ 6.8914e+01,  1.8209e+02,  4.6600e-02,  ...,  1.9080e-03,\n",
      "           5.3700e-04,  0.0000e+00],\n",
      "         [ 6.9114e+01,  1.8317e+02,  4.9500e-02,  ...,  4.3670e-03,\n",
      "           1.3370e-03,  0.0000e+00],\n",
      "         [ 6.9313e+01,  1.8425e+02,  5.1300e-02,  ...,  2.4850e-03,\n",
      "           1.0230e-03,  0.0000e+00],\n",
      "         [ 6.9643e+01,  1.8698e+02,  5.6100e-02,  ..., -3.8300e-04,\n",
      "           1.6780e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 6.8914e+01,  1.8209e+02,  4.6600e-02,  ...,  1.9080e-03,\n",
      "           5.3700e-04,  0.0000e+00],\n",
      "         [ 6.9114e+01,  1.8317e+02,  4.9500e-02,  ...,  4.3670e-03,\n",
      "           1.3370e-03,  0.0000e+00],\n",
      "         [ 6.9313e+01,  1.8425e+02,  5.1300e-02,  ...,  2.4850e-03,\n",
      "           1.0230e-03,  0.0000e+00],\n",
      "         [ 6.9643e+01,  1.8698e+02,  5.6100e-02,  ..., -3.8300e-04,\n",
      "           1.6780e-03,  1.0000e+00],\n",
      "         [ 6.9974e+01,  1.8970e+02,  5.2800e-02,  ..., -2.0150e-03,\n",
      "           1.3410e-03,  1.0000e+00]]], device='cuda:0')\n",
      "tensor([[[   4.5700]],\n",
      "\n",
      "        [[   4.4000]],\n",
      "\n",
      "        [[   4.3400]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2059.7400]],\n",
      "\n",
      "        [[2065.3000]],\n",
      "\n",
      "        [[2096.9500]]], device='cuda:0') tensor([[[1160.3300]],\n",
      "\n",
      "        [[1249.4600]],\n",
      "\n",
      "        [[1255.8199]],\n",
      "\n",
      "        [[1224.4200]],\n",
      "\n",
      "        [[1211.2300]],\n",
      "\n",
      "        [[1133.5800]],\n",
      "\n",
      "        [[1040.9399]],\n",
      "\n",
      "        [[1059.7800]],\n",
      "\n",
      "        [[1139.4500]],\n",
      "\n",
      "        [[1148.0800]],\n",
      "\n",
      "        [[1130.2000]],\n",
      "\n",
      "        [[1106.7300]],\n",
      "\n",
      "        [[1147.3900]],\n",
      "\n",
      "        [[1076.9200]],\n",
      "\n",
      "        [[1067.1400]],\n",
      "\n",
      "        [[ 989.8100]],\n",
      "\n",
      "        [[ 911.6200]],\n",
      "\n",
      "        [[ 916.0700]],\n",
      "\n",
      "        [[ 815.2900]],\n",
      "\n",
      "        [[ 885.7600]],\n",
      "\n",
      "        [[ 936.3100]],\n",
      "\n",
      "        [[ 879.8200]],\n",
      "\n",
      "        [[ 855.7000]],\n",
      "\n",
      "        [[ 841.1500]],\n",
      "\n",
      "        [[ 848.1800]],\n",
      "\n",
      "        [[ 916.9200]],\n",
      "\n",
      "        [[ 963.5900]],\n",
      "\n",
      "        [[ 974.5100]],\n",
      "\n",
      "        [[ 990.3100]],\n",
      "\n",
      "        [[1008.0100]],\n",
      "\n",
      "        [[ 995.9700]],\n",
      "\n",
      "        [[1050.7100]],\n",
      "\n",
      "        [[1058.2000]],\n",
      "\n",
      "        [[1111.9200]],\n",
      "\n",
      "        [[1131.1300]],\n",
      "\n",
      "        [[1144.9399]],\n",
      "\n",
      "        [[1126.2100]],\n",
      "\n",
      "        [[1107.3000]],\n",
      "\n",
      "        [[1120.6801]],\n",
      "\n",
      "        [[1140.8400]],\n",
      "\n",
      "        [[1101.7200]],\n",
      "\n",
      "        [[1104.2400]],\n",
      "\n",
      "        [[1114.5800]],\n",
      "\n",
      "        [[1130.2000]],\n",
      "\n",
      "        [[1173.8199]],\n",
      "\n",
      "        [[1211.9200]],\n",
      "\n",
      "        [[1181.2700]],\n",
      "\n",
      "        [[1203.6000]],\n",
      "\n",
      "        [[1180.5900]],\n",
      "\n",
      "        [[1156.8500]],\n",
      "\n",
      "        [[1191.5000]],\n",
      "\n",
      "        [[1191.3300]],\n",
      "\n",
      "        [[1234.1801]],\n",
      "\n",
      "        [[1220.3300]],\n",
      "\n",
      "        [[1228.8101]],\n",
      "\n",
      "        [[1207.0100]],\n",
      "\n",
      "        [[1249.4800]],\n",
      "\n",
      "        [[1248.2900]],\n",
      "\n",
      "        [[1280.0800]],\n",
      "\n",
      "        [[1280.6600]],\n",
      "\n",
      "        [[1294.8700]],\n",
      "\n",
      "        [[1310.6100]],\n",
      "\n",
      "        [[1270.0900]],\n",
      "\n",
      "        [[1270.2000]],\n",
      "\n",
      "        [[1276.6600]],\n",
      "\n",
      "        [[1303.8199]],\n",
      "\n",
      "        [[1335.8500]],\n",
      "\n",
      "        [[1377.9399]],\n",
      "\n",
      "        [[1400.6300]],\n",
      "\n",
      "        [[1418.3000]],\n",
      "\n",
      "        [[1438.2400]],\n",
      "\n",
      "        [[1406.8199]],\n",
      "\n",
      "        [[1420.8600]],\n",
      "\n",
      "        [[1482.3700]],\n",
      "\n",
      "        [[1530.6200]],\n",
      "\n",
      "        [[1503.3500]],\n",
      "\n",
      "        [[1455.2700]],\n",
      "\n",
      "        [[1473.9900]],\n",
      "\n",
      "        [[1526.7500]],\n",
      "\n",
      "        [[1549.3800]],\n",
      "\n",
      "        [[1481.1400]],\n",
      "\n",
      "        [[1468.3600]],\n",
      "\n",
      "        [[1378.5500]],\n",
      "\n",
      "        [[1330.6300]],\n",
      "\n",
      "        [[1322.7000]],\n",
      "\n",
      "        [[1385.5900]],\n",
      "\n",
      "        [[1400.3800]],\n",
      "\n",
      "        [[1280.0000]],\n",
      "\n",
      "        [[1267.3800]],\n",
      "\n",
      "        [[1282.8300]],\n",
      "\n",
      "        [[1166.3600]],\n",
      "\n",
      "        [[ 968.7500]],\n",
      "\n",
      "        [[ 896.2400]],\n",
      "\n",
      "        [[ 903.2500]],\n",
      "\n",
      "        [[ 825.8800]],\n",
      "\n",
      "        [[ 735.0900]],\n",
      "\n",
      "        [[ 797.8700]],\n",
      "\n",
      "        [[ 872.8100]],\n",
      "\n",
      "        [[ 919.1400]],\n",
      "\n",
      "        [[ 919.3200]],\n",
      "\n",
      "        [[ 987.4800]],\n",
      "\n",
      "        [[1020.6200]],\n",
      "\n",
      "        [[1057.0800]],\n",
      "\n",
      "        [[1036.1899]],\n",
      "\n",
      "        [[1095.6300]],\n",
      "\n",
      "        [[1115.1000]],\n",
      "\n",
      "        [[1073.8700]],\n",
      "\n",
      "        [[1104.4900]],\n",
      "\n",
      "        [[1169.4301]],\n",
      "\n",
      "        [[1186.6899]],\n",
      "\n",
      "        [[1089.4100]],\n",
      "\n",
      "        [[1030.7100]],\n",
      "\n",
      "        [[1101.6000]],\n",
      "\n",
      "        [[1049.3300]],\n",
      "\n",
      "        [[1141.2000]],\n",
      "\n",
      "        [[1183.2600]],\n",
      "\n",
      "        [[1180.5500]],\n",
      "\n",
      "        [[1257.6400]],\n",
      "\n",
      "        [[1286.1200]],\n",
      "\n",
      "        [[1327.2200]],\n",
      "\n",
      "        [[1325.8300]],\n",
      "\n",
      "        [[1363.6100]],\n",
      "\n",
      "        [[1345.2000]],\n",
      "\n",
      "        [[1320.6400]],\n",
      "\n",
      "        [[1292.2800]],\n",
      "\n",
      "        [[1218.8900]],\n",
      "\n",
      "        [[1131.4200]],\n",
      "\n",
      "        [[1253.3000]],\n",
      "\n",
      "        [[1246.9600]],\n",
      "\n",
      "        [[1257.6000]],\n",
      "\n",
      "        [[1312.4100]],\n",
      "\n",
      "        [[1365.6801]],\n",
      "\n",
      "        [[1408.4700]],\n",
      "\n",
      "        [[1397.9100]],\n",
      "\n",
      "        [[1310.3300]],\n",
      "\n",
      "        [[1362.1600]],\n",
      "\n",
      "        [[1379.3199]],\n",
      "\n",
      "        [[1406.5800]],\n",
      "\n",
      "        [[1440.6700]],\n",
      "\n",
      "        [[1412.1600]],\n",
      "\n",
      "        [[1416.1801]],\n",
      "\n",
      "        [[1426.1899]],\n",
      "\n",
      "        [[1498.1100]],\n",
      "\n",
      "        [[1514.6801]],\n",
      "\n",
      "        [[1569.1899]],\n",
      "\n",
      "        [[1597.5699]],\n",
      "\n",
      "        [[1630.7400]],\n",
      "\n",
      "        [[1606.2800]],\n",
      "\n",
      "        [[1685.7300]],\n",
      "\n",
      "        [[1632.9700]],\n",
      "\n",
      "        [[1681.5500]],\n",
      "\n",
      "        [[1756.5400]],\n",
      "\n",
      "        [[1805.8101]],\n",
      "\n",
      "        [[1848.3600]],\n",
      "\n",
      "        [[1782.5900]],\n",
      "\n",
      "        [[1859.4500]],\n",
      "\n",
      "        [[1872.3400]],\n",
      "\n",
      "        [[1883.9500]],\n",
      "\n",
      "        [[1923.5699]],\n",
      "\n",
      "        [[1960.2300]],\n",
      "\n",
      "        [[1930.6700]],\n",
      "\n",
      "        [[2003.3700]],\n",
      "\n",
      "        [[1972.2900]],\n",
      "\n",
      "        [[2018.0500]],\n",
      "\n",
      "        [[2067.5601]],\n",
      "\n",
      "        [[2058.8999]],\n",
      "\n",
      "        [[1994.9900]],\n",
      "\n",
      "        [[2104.5000]],\n",
      "\n",
      "        [[2067.8899]],\n",
      "\n",
      "        [[2085.5100]],\n",
      "\n",
      "        [[2107.3899]],\n",
      "\n",
      "        [[2063.1101]],\n",
      "\n",
      "        [[2103.8401]],\n",
      "\n",
      "        [[1972.1801]],\n",
      "\n",
      "        [[1920.0300]],\n",
      "\n",
      "        [[2079.3601]],\n",
      "\n",
      "        [[2080.4099]],\n",
      "\n",
      "        [[2043.9399]],\n",
      "\n",
      "        [[1940.2400]],\n",
      "\n",
      "        [[1932.2300]],\n",
      "\n",
      "        [[2059.7400]],\n",
      "\n",
      "        [[2065.3000]],\n",
      "\n",
      "        [[2096.9500]],\n",
      "\n",
      "        [[2098.8601]],\n",
      "\n",
      "        [[2173.6001]],\n",
      "\n",
      "        [[2170.9500]],\n",
      "\n",
      "        [[2168.2700]],\n",
      "\n",
      "        [[2126.1499]],\n",
      "\n",
      "        [[2198.8101]],\n",
      "\n",
      "        [[2238.8301]],\n",
      "\n",
      "        [[2278.8701]],\n",
      "\n",
      "        [[2363.6399]],\n",
      "\n",
      "        [[2362.7200]],\n",
      "\n",
      "        [[2384.2000]],\n",
      "\n",
      "        [[2411.8000]],\n",
      "\n",
      "        [[2423.4099]],\n",
      "\n",
      "        [[2470.3000]],\n",
      "\n",
      "        [[2471.6499]],\n",
      "\n",
      "        [[2519.3601]],\n",
      "\n",
      "        [[2575.2600]],\n",
      "\n",
      "        [[2584.8401]],\n",
      "\n",
      "        [[2673.6101]],\n",
      "\n",
      "        [[2823.8101]],\n",
      "\n",
      "        [[2713.8301]],\n",
      "\n",
      "        [[2640.8701]],\n",
      "\n",
      "        [[2648.0500]],\n",
      "\n",
      "        [[2705.2700]],\n",
      "\n",
      "        [[2718.3701]],\n",
      "\n",
      "        [[2816.2900]],\n",
      "\n",
      "        [[2901.5200]],\n",
      "\n",
      "        [[2913.9800]],\n",
      "\n",
      "        [[2711.7400]],\n",
      "\n",
      "        [[2760.1699]],\n",
      "\n",
      "        [[2506.8501]],\n",
      "\n",
      "        [[2704.1001]],\n",
      "\n",
      "        [[2784.4900]],\n",
      "\n",
      "        [[2834.3999]],\n",
      "\n",
      "        [[2945.8301]],\n",
      "\n",
      "        [[2752.0601]],\n",
      "\n",
      "        [[2941.7600]],\n",
      "\n",
      "        [[2980.3799]],\n",
      "\n",
      "        [[2926.4600]],\n",
      "\n",
      "        [[2976.7400]],\n",
      "\n",
      "        [[3037.5601]],\n",
      "\n",
      "        [[3140.9800]],\n",
      "\n",
      "        [[3230.7800]],\n",
      "\n",
      "        [[3225.5200]],\n",
      "\n",
      "        [[2954.2200]],\n",
      "\n",
      "        [[2584.5901]],\n",
      "\n",
      "        [[2912.4299]],\n",
      "\n",
      "        [[3044.3101]],\n",
      "\n",
      "        [[3100.2900]],\n",
      "\n",
      "        [[3271.1201]],\n",
      "\n",
      "        [[3500.3101]],\n",
      "\n",
      "        [[3363.0000]],\n",
      "\n",
      "        [[3269.9600]],\n",
      "\n",
      "        [[3621.6299]],\n",
      "\n",
      "        [[3756.0701]],\n",
      "\n",
      "        [[3714.2400]],\n",
      "\n",
      "        [[3811.1499]],\n",
      "\n",
      "        [[3972.8899]],\n",
      "\n",
      "        [[4181.1699]],\n",
      "\n",
      "        [[4204.1099]],\n",
      "\n",
      "        [[4297.5000]],\n",
      "\n",
      "        [[4395.2598]],\n",
      "\n",
      "        [[4522.6802]],\n",
      "\n",
      "        [[4307.5400]],\n",
      "\n",
      "        [[4605.3799]],\n",
      "\n",
      "        [[4567.0000]],\n",
      "\n",
      "        [[4766.1802]],\n",
      "\n",
      "        [[4515.5498]],\n",
      "\n",
      "        [[4373.9399]],\n",
      "\n",
      "        [[4530.4102]],\n",
      "\n",
      "        [[4131.9302]],\n",
      "\n",
      "        [[4132.1499]],\n",
      "\n",
      "        [[3785.3799]],\n",
      "\n",
      "        [[4130.2900]],\n",
      "\n",
      "        [[3955.0000]],\n",
      "\n",
      "        [[3585.6201]],\n",
      "\n",
      "        [[3871.9800]],\n",
      "\n",
      "        [[4080.1101]],\n",
      "\n",
      "        [[3839.5000]],\n",
      "\n",
      "        [[4076.6001]],\n",
      "\n",
      "        [[3970.1499]],\n",
      "\n",
      "        [[4109.3101]],\n",
      "\n",
      "        [[4169.4800]],\n",
      "\n",
      "        [[4179.8301]],\n",
      "\n",
      "        [[4450.3799]],\n",
      "\n",
      "        [[4588.9600]],\n",
      "\n",
      "        [[4507.6602]],\n",
      "\n",
      "        [[4288.0498]],\n",
      "\n",
      "        [[4193.7998]],\n",
      "\n",
      "        [[4567.7998]],\n",
      "\n",
      "        [[4769.8301]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_498255/1221935694.py:95: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  X, Y = TensorFloat(X), TensorFloat(Y)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"Data2023.csv\", index_col=\"yyyymm\")\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "\n",
    "missing_ratio = X.isnull().mean()  # 计算每列的缺失值比例\n",
    "columns_to_drop = missing_ratio[missing_ratio > 0.35].index  # 找到缺失值比例超过 35% 的列\n",
    "X = X.drop(columns=columns_to_drop)  # 删除这些列\n",
    "\n",
    "X = X.fillna(value=0)\n",
    "\n",
    "\n",
    "X[\"trend\"] = (X[\"price\"].pct_change().shift(-1) > 0).astype(int)\n",
    "\n",
    "\n",
    "print(X)\n",
    "\n",
    "\n",
    "class MinMaxNorm01(object):\n",
    "    \"\"\"scale data to range [0, 1]\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.min = x.min()\n",
    "        self.max = x.max()\n",
    "        #print('Min:{}, Max:{}'.format(self.min, self.max))\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = 1.0 * (x - self.min) / (self.max - self.min)\n",
    "        return x\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        x = x * (self.max - self.min) + self.min\n",
    "        return x\n",
    "\n",
    "a=X.to_numpy()\n",
    "\n",
    "#data1=train_data.to_numpy()\n",
    "\n",
    "mmn = MinMaxNorm01()\n",
    "\n",
    "data=a\n",
    "\n",
    "\n",
    "dataset = data\n",
    "\n",
    "window=5\n",
    "predict=1\n",
    "\n",
    "ran=data.shape[0]\n",
    "i=0\n",
    "X=[]\n",
    "Y=[]\n",
    "while i+window<ran:\n",
    "\n",
    "    X.append(torch.Tensor(dataset[i:i+window,1:]))\n",
    "    Y.append(torch.Tensor(dataset[i+window:i+window+predict,0]))\n",
    "    i+=1\n",
    "\n",
    "XX=torch.stack(X,dim=0)\n",
    "YY=torch.stack(Y,dim=0)\n",
    "YY=YY[:,:,None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_len = int(0.15*XX.shape[0])\n",
    "val_len = int(0.05*XX.shape[0])\n",
    "train_len =  XX.shape[0]-test_len-val_len\n",
    "\n",
    "\n",
    "\n",
    "X_test=torch.Tensor.float(XX[-test_len:,:,:]).cuda()\n",
    "\n",
    "\n",
    "Y_test=torch.Tensor.float(YY[-test_len:,:,:]).cuda()\n",
    "\n",
    "X_train=torch.Tensor.float(XX[test_len:test_len+train_len,:,:]).cuda()\n",
    "Y_train=torch.Tensor.float(YY[test_len:test_len+train_len,:,:]).cuda()\n",
    "\n",
    "X_val=torch.Tensor.float(XX[-val_len:,:,:]).cuda()\n",
    "Y_val=torch.Tensor.float(YY[-val_len:,:,:]).cuda()\n",
    "\n",
    "print(X_train, X_test)\n",
    "print(Y_train, Y_test)\n",
    "\n",
    "\n",
    "def data_loader(X, Y, batch_size, shuffle=True, drop_last=True):\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    TensorFloat = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    X, Y = TensorFloat(X), TensorFloat(Y)\n",
    "    data = torch.utils.data.TensorDataset(X, Y)\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                             shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader\n",
    "\n",
    "train_loader = data_loader(X_train, Y_train, 64, shuffle=False, drop_last=False)\n",
    "val_loader = data_loader(X_val, Y_val, 64, shuffle=False, drop_last=False)\n",
    "test_loader = data_loader(X_test, Y_test, 64, shuffle=False, drop_last=False)\n",
    "\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c59ab7c2-1c15-4762-b601-33f5ae688764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(root, name=None, debug=True):\n",
    "    #when debug is true, show DEBUG and INFO in screen\n",
    "    #when debug is false, show DEBUG in file and info in both screen&file\n",
    "    #INFO will always be in screen\n",
    "    # create a logger\n",
    "    logger = logging.getLogger(name)\n",
    "    #critical > error > warning > info > debug > notset\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # define the formate\n",
    "    formatter = logging.Formatter('%(asctime)s: %(message)s', \"%Y-%m-%d %H:%M\")\n",
    "    # create another handler for output log to console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    if debug:\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        # create a handler for write log to file\n",
    "        logfile = os.path.join(root, 'run.log')\n",
    "        print('Creat Log File in: ', logfile)\n",
    "        file_handler = logging.FileHandler(logfile, mode='w')\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    # add Handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    if not debug:\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def init_seed(seed):\n",
    "    '''\n",
    "    Disable cudnn to maximize reproducibility\n",
    "    '''\n",
    "    torch.cuda.cudnn_enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def init_device(opt):\n",
    "    if torch.cuda.is_available():\n",
    "        opt.cuda = True\n",
    "        torch.cuda.set_device(int(opt.device[5]))\n",
    "    else:\n",
    "        opt.cuda = False\n",
    "        opt.device = 'cpu'\n",
    "    return opt\n",
    "\n",
    "def init_optim(model, opt):\n",
    "    '''\n",
    "    Initialize optimizer\n",
    "    '''\n",
    "    return torch.optim.Adam(params=model.parameters(),lr=opt.lr_init)\n",
    "\n",
    "def init_lr_scheduler(optim, opt):\n",
    "    '''\n",
    "    Initialize the learning rate scheduler\n",
    "    '''\n",
    "    #return torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=opt.lr_scheduler_rate,step_size=opt.lr_scheduler_step)\n",
    "    return torch.optim.lr_scheduler.MultiStepLR(optimizer=optim, milestones=opt.lr_decay_steps,\n",
    "                                                gamma = opt.lr_scheduler_rate)\n",
    "\n",
    "def print_model_parameters(model, only_num = True):\n",
    "    \n",
    "\n",
    "    \n",
    "    print('*****************Model Parameter*****************')\n",
    "    if not only_num:\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape, param.requires_grad)\n",
    "    total_num = sum([param.nelement() for param in model.parameters()])\n",
    "    print('Total params num: {}'.format(total_num))\n",
    "    print('*****************Finish Parameter****************')\n",
    "\n",
    "def get_memory_usage(device):\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024*1024.)\n",
    "    cached_memory = torch.cuda.memory_cached(device) / (1024*1024.)\n",
    "    return allocated_memory, cached_memory\n",
    "    #print('Allocated Memory: {:.2f} MB, Cached Memory: {:.2f} MB'.format(allocated_memory, cached_memory))\n",
    "\n",
    "\n",
    "def MAE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true-pred))\n",
    "\n",
    "def MSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean((pred - true) ** 2)\n",
    "\n",
    "def RMSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.mean((pred - true) ** 2))\n",
    "\n",
    "def RRSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.sum((pred - true) ** 2)) / torch.sqrt(torch.sum((pred - true.mean()) ** 2))\n",
    "\n",
    "\n",
    "\n",
    "def MAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(torch.div((true - pred), true)))\n",
    "\n",
    "def PNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    indicator = torch.gt(pred - true, 0).float()\n",
    "    return indicator.mean()\n",
    "\n",
    "def oPNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    bias = (true+pred) / (2*true)\n",
    "    return bias.mean()\n",
    "\n",
    "def MARE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.div(torch.sum(torch.abs((true - pred))), torch.sum(true))\n",
    "\n",
    "def SMAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true-pred)/(torch.abs(true)+torch.abs(pred)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def All_Metrics(pred, true, mask1, mask2):\n",
    "    #mask1 filter the very small value, mask2 filter the value lower than a defined threshold\n",
    "    assert type(pred) == type(true)\n",
    "    #if type(pred) == np.ndarray:\n",
    "    #    mae  = MAE_np(pred, true, mask1)\n",
    "    #    rmse = RMSE_np(pred, true, mask1)\n",
    "    #    mape = MAPE_np(pred, true, mask2)\n",
    "    #    rrse = RRSE_np(pred, true, mask1)\n",
    "\n",
    "        #corr = CORR_np(pred, true, mask1)\n",
    "        #pnbi = PNBI_np(pred, true, mask1)\n",
    "        #opnbi = oPNBI_np(pred, true, mask2)\n",
    "    if type(pred) == torch.Tensor:\n",
    "        mae  = MAE_torch(pred, true, mask1)\n",
    "        rmse = RMSE_torch(pred, true, mask1)\n",
    "        rrse = RRSE_torch(pred, true, mask1)\n",
    "\n",
    "        #pnbi = PNBI_torch(pred, true, mask1)\n",
    "        #opnbi = oPNBI_torch(pred, true, mask2)\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return mae, rmse, rrse\n",
    "\n",
    "def SIGIR_Metrics(pred, true, mask1, mask2):\n",
    "    rrse = RRSE_torch(pred, true, mask1)\n",
    "    corr = CORR_torch(pred, true, 0)\n",
    "    return rrse, corr\n",
    "\n",
    "def save_model(model, model_dir, epoch=None):\n",
    "    if model_dir is None:\n",
    "        return\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    epoch = str(epoch) if epoch else \"\"\n",
    "    file_name = os.path.join(model_dir, epoch + \"_stemgnn.pt\")\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        torch.save(model, f)\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, loss, optimizer, train_loader, val_loader, test_loader,\n",
    "                 args, lr_scheduler=None):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        # self.scaler = scaler\n",
    "        self.args = args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_per_epoch = len(train_loader)\n",
    "        if val_loader != None:\n",
    "            self.val_per_epoch = len(val_loader)\n",
    "        self.best_path = os.path.join(self.args.get('log_dir'), 'best_model.pth')\n",
    "        self.loss_figure_path = os.path.join(self.args.get('log_dir'), 'loss.png')\n",
    "        # log\n",
    "        if os.path.isdir(args.get('log_dir')) == False and not args.get('debug'):\n",
    "            os.makedirs(args.get('log_dir'), exist_ok=True)\n",
    "        self.logger = get_logger(args.get('log_dir'), name=args.get('model'), debug=args.get('debug'))\n",
    "        self.logger.info('Experiment log path in: {}'.format(args.get('log_dir')))\n",
    "        # if not args.debug:\n",
    "        # self.logger.info(\"Argument: %r\", args)\n",
    "        # for arg, value in sorted(vars(args).items()):\n",
    "        #     self.logger.info(\"Argument %s: %r\", arg, value)\n",
    "\n",
    "    def val_epoch(self, epoch, val_dataloader):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = self.model(data)\n",
    "                # if self.args.get('real_value'):\n",
    "                # label = self.scaler.inverse_transform(label)\n",
    "                loss = self.loss(output, label)\n",
    "                # a whole batch of Metr_LA is filtered\n",
    "                if not torch.isnan(loss):\n",
    "                    total_val_loss += loss.item()\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        self.logger.info('**********Val Epoch {}: average Loss: {:.6f}'.format(epoch, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        loss_values=[]\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data = data\n",
    "            label = target  # (..., 1)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # data and target shape: B, T, N, F; output shape: B, T, N, F\n",
    "            output = self.model(data)\n",
    "            # if self.args.get('real_value'):\n",
    "            #   label = self.scaler.inverse_transform(label)\n",
    "\n",
    "\n",
    "            loss = self.loss(output, label)\n",
    "            loss = self.loss(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # add max grad clipping\n",
    "            if self.args.get('grad_norm'):\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.get('max_grad_norm'))\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # log information\n",
    "            if batch_idx % self.args.get('log_step') == 0:\n",
    "                self.logger.info('Train Epoch {}: {}/{} Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx, self.train_per_epoch, loss.item()))\n",
    "        train_epoch_loss = total_loss / self.train_per_epoch\n",
    "        self.logger.info(\n",
    "            '**********Train Epoch {}: averaged Loss: {:.6f}'.format(epoch, train_epoch_loss))\n",
    "\n",
    "        # learning rate decay\n",
    "        if self.args.get('lr_decay'):\n",
    "            self.lr_scheduler.step()\n",
    "        return train_epoch_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        not_improved_count = 0\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, self.args.get('epochs') + 1):\n",
    "            # epoch_time = time.time()\n",
    "            train_epoch_loss = self.train_epoch(epoch)\n",
    "            # print(time.time()-epoch_time)\n",
    "            # exit()\n",
    "            if self.val_loader == None:\n",
    "                val_dataloader = self.test_loader\n",
    "            else:\n",
    "                val_dataloader = self.val_loader\n",
    "            val_epoch_loss = self.val_epoch(epoch, val_dataloader)\n",
    "\n",
    "            # print('LR:', self.optimizer.param_groups[0]['lr'])\n",
    "            train_loss_list.append(train_epoch_loss)\n",
    "            val_loss_list.append(val_epoch_loss)\n",
    "            if train_epoch_loss > 1e6:\n",
    "                self.logger.warning('Gradient explosion detected. Ending...')\n",
    "                break\n",
    "            # if self.val_loader == None:\n",
    "            # val_epoch_loss = train_epoch_loss\n",
    "            if val_epoch_loss < best_loss:\n",
    "                best_loss = val_epoch_loss\n",
    "                not_improved_count = 0\n",
    "                best_state = True\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "                best_state = False\n",
    "            # early stop\n",
    "            if self.args.get('early_stop'):\n",
    "                if not_improved_count == self.args.get('early_stop_patience'):\n",
    "                    self.logger.info(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                                     \"Training stops.\".format(self.args.get('early_stop_patience')))\n",
    "                    break\n",
    "            # save the best state\n",
    "            if best_state == True:\n",
    "                self.logger.info('*********************************Current best model saved!')\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        self.logger.info(\"Total training time: {:.4f}min, best loss: {:.6f}\".format((training_time / 60), best_loss))\n",
    "\n",
    "        with open('milan_sms_mamaba.txt', 'a') as f:\n",
    "            f.write(str(epoch))\n",
    "            f.write('\\n')\n",
    "            f.write(str(training_time / 60))\n",
    "            f.write('\\n')\n",
    "\n",
    "        # save the best model to file\n",
    "        if not self.args.get('debug'):\n",
    "            torch.save(best_model, self.best_path)\n",
    "            self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "        # test\n",
    "        self.model.load_state_dict(best_model)\n",
    "        # self.val_epoch(self.args.epochs, self.test_loader)\n",
    "        y1, y2 = self.test(self.model, self.args, self.test_loader, self.logger)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        state = {\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'config': self.args\n",
    "        }\n",
    "        torch.save(state, self.best_path)\n",
    "        self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def test(model, args, data_loader, logger, path=None):\n",
    "        if path != None:\n",
    "            check_point = torch.load(path)\n",
    "            state_dict = check_point['state_dict']\n",
    "            args = check_point['config']\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(args.get('device'))\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = model(data)\n",
    "\n",
    "                y_true.append(label)\n",
    "                y_pred.append(output)\n",
    "\n",
    "                #print(model.forward(data, [], teacher_forcing_ratio=0))\n",
    "        # y_true = scaler.inverse_transform(torch.cat(y_true, dim=0))\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        # if not args.get('real_value'):\n",
    "        #    y_pred = torch.cat(y_pred, dim=0)\n",
    "        # else:\n",
    "        # y_pred = scaler.inverse_transform(torch.cat(y_pred, dim=0))\n",
    "        # np.save('./{}_true.npy'.format(args.get('dataset')), y_true.cpu().numpy())\n",
    "        # np.save('./{}_pred.npy'.format(args.get('dataset')), y_pred.cpu().numpy())\n",
    "        # for t in range(y_true.shape[1]):\n",
    "        #    mae, rmse, mape, _ = All_Metrics(y_pred[:, t, ...], y_true[:, t, ...],\n",
    "        #                                        args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        #    logger.info(\"Horizon {:02d}, MAE: {:.2f}, RMSE: {:.2f}, MAPE: {:.4f}%\".format(\n",
    "        #        t + 1, mae, rmse, mape*100))\n",
    "        mae, rmse, _ = All_Metrics(y_pred, y_true, args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        logger.info(\"Average Horizon, MAE: {:.4f}, MSE: {:.4f}\".format(\n",
    "            mae, rmse))\n",
    "        return y_pred, y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_sampling_threshold(global_step, k):\n",
    "        \"\"\"\n",
    "        Computes the sampling probability for scheduled sampling using inverse sigmoid.\n",
    "        :param global_step:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return k / (k + math.exp(global_step / k))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542fef61-7668-4bf3-ae0f-abe8a0ab7f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 23:00: Experiment log path in: ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([17, 10]) True\n",
      "embed_w torch.Size([10, 10]) True\n",
      "weights_pool torch.Size([10, 3, 5, 1]) True\n",
      "bias_pool torch.Size([10, 1]) True\n",
      "mam1.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers.3.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.3.mixer.D torch.Size([64]) True\n",
      "mam1.layers.3.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.3.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.3.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.3.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.3.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.3.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.3.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.3.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.3.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.3.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.3.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.3.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.3.norm.weight torch.Size([32]) True\n",
      "mam1.layers.3.norm.bias torch.Size([32]) True\n",
      "mam1.layers.4.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.4.mixer.D torch.Size([64]) True\n",
      "mam1.layers.4.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.4.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.4.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.4.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.4.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.4.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.4.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.4.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.4.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.4.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.4.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.4.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.4.norm.weight torch.Size([32]) True\n",
      "mam1.layers.4.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.3.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.3.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.3.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.3.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.3.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.3.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.3.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.3.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.3.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.3.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.3.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.3.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.3.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.3.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.3.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.3.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.4.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.4.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.4.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.4.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.4.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.4.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.4.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.4.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.4.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.4.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.4.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.4.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.4.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.4.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.4.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.4.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.lin.3.0.weight torch.Size([5]) True\n",
      "mam1.lin.3.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.3.1.bias torch.Size([32]) True\n",
      "mam1.lin.3.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.3.3.bias torch.Size([5]) True\n",
      "mam1.lin.4.0.weight torch.Size([5]) True\n",
      "mam1.lin.4.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.4.1.bias torch.Size([32]) True\n",
      "mam1.lin.4.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.4.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.lm_head.bias torch.Size([17]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([17]) True\n",
      "mam1.nnl.bias torch.Size([17]) True\n",
      "proj.weight torch.Size([1, 17]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 350102\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 23:00: Train Epoch 1: 0/23 Loss: 5.124785\n",
      "2025-03-07 23:00: Train Epoch 1: 20/23 Loss: 1109.498169\n",
      "2025-03-07 23:00: **********Train Epoch 1: averaged Loss: 279.508839\n",
      "2025-03-07 23:00: **********Val Epoch 1: average Loss: 3600.260742\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 2: 0/23 Loss: 4.068334\n",
      "2025-03-07 23:00: Train Epoch 2: 20/23 Loss: 1109.014404\n",
      "2025-03-07 23:00: **********Train Epoch 2: averaged Loss: 278.840287\n",
      "2025-03-07 23:00: **********Val Epoch 2: average Loss: 3600.158203\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 3: 0/23 Loss: 3.531868\n",
      "2025-03-07 23:00: Train Epoch 3: 20/23 Loss: 1108.904175\n",
      "2025-03-07 23:00: **********Train Epoch 3: averaged Loss: 278.359463\n",
      "2025-03-07 23:00: **********Val Epoch 3: average Loss: 3599.847778\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 4: 0/23 Loss: 2.943418\n",
      "2025-03-07 23:00: Train Epoch 4: 20/23 Loss: 1108.541138\n",
      "2025-03-07 23:00: **********Train Epoch 4: averaged Loss: 277.897440\n",
      "2025-03-07 23:00: **********Val Epoch 4: average Loss: 3599.756104\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 5: 0/23 Loss: 2.598373\n",
      "2025-03-07 23:00: Train Epoch 5: 20/23 Loss: 1108.282715\n",
      "2025-03-07 23:00: **********Train Epoch 5: averaged Loss: 277.437350\n",
      "2025-03-07 23:00: **********Val Epoch 5: average Loss: 3599.511108\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 6: 0/23 Loss: 2.104258\n",
      "2025-03-07 23:00: Train Epoch 6: 20/23 Loss: 1107.967041\n",
      "2025-03-07 23:00: **********Train Epoch 6: averaged Loss: 276.870459\n",
      "2025-03-07 23:00: **********Val Epoch 6: average Loss: 3599.348999\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 7: 0/23 Loss: 1.310028\n",
      "2025-03-07 23:00: Train Epoch 7: 20/23 Loss: 1107.554077\n",
      "2025-03-07 23:00: **********Train Epoch 7: averaged Loss: 276.355149\n",
      "2025-03-07 23:00: **********Val Epoch 7: average Loss: 3599.095947\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 8: 0/23 Loss: 0.832431\n",
      "2025-03-07 23:00: Train Epoch 8: 20/23 Loss: 1107.043091\n",
      "2025-03-07 23:00: **********Train Epoch 8: averaged Loss: 275.689619\n",
      "2025-03-07 23:00: **********Val Epoch 8: average Loss: 3598.769287\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 9: 0/23 Loss: 0.472691\n",
      "2025-03-07 23:00: Train Epoch 9: 20/23 Loss: 1106.437256\n",
      "2025-03-07 23:00: **********Train Epoch 9: averaged Loss: 275.103890\n",
      "2025-03-07 23:00: **********Val Epoch 9: average Loss: 3598.390747\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 10: 0/23 Loss: 0.896235\n",
      "2025-03-07 23:00: Train Epoch 10: 20/23 Loss: 1105.639160\n",
      "2025-03-07 23:00: **********Train Epoch 10: averaged Loss: 274.478459\n",
      "2025-03-07 23:00: **********Val Epoch 10: average Loss: 3598.153076\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 11: 0/23 Loss: 1.365693\n",
      "2025-03-07 23:00: Train Epoch 11: 20/23 Loss: 1104.943481\n",
      "2025-03-07 23:00: **********Train Epoch 11: averaged Loss: 273.799876\n",
      "2025-03-07 23:00: **********Val Epoch 11: average Loss: 3597.502441\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 12: 0/23 Loss: 2.048759\n",
      "2025-03-07 23:00: Train Epoch 12: 20/23 Loss: 1103.851562\n",
      "2025-03-07 23:00: **********Train Epoch 12: averaged Loss: 273.092068\n",
      "2025-03-07 23:00: **********Val Epoch 12: average Loss: 3597.076172\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 13: 0/23 Loss: 2.772491\n",
      "2025-03-07 23:00: Train Epoch 13: 20/23 Loss: 1103.132935\n",
      "2025-03-07 23:00: **********Train Epoch 13: averaged Loss: 272.514404\n",
      "2025-03-07 23:00: **********Val Epoch 13: average Loss: 3596.397705\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 14: 0/23 Loss: 3.003632\n",
      "2025-03-07 23:00: Train Epoch 14: 20/23 Loss: 1102.151611\n",
      "2025-03-07 23:00: **********Train Epoch 14: averaged Loss: 271.953878\n",
      "2025-03-07 23:00: **********Val Epoch 14: average Loss: 3595.796875\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 15: 0/23 Loss: 4.088472\n",
      "2025-03-07 23:00: Train Epoch 15: 20/23 Loss: 1100.936279\n",
      "2025-03-07 23:00: **********Train Epoch 15: averaged Loss: 271.405799\n",
      "2025-03-07 23:00: **********Val Epoch 15: average Loss: 3595.161865\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 16: 0/23 Loss: 3.998491\n",
      "2025-03-07 23:00: Train Epoch 16: 20/23 Loss: 1100.729248\n",
      "2025-03-07 23:00: **********Train Epoch 16: averaged Loss: 271.163333\n",
      "2025-03-07 23:00: **********Val Epoch 16: average Loss: 3594.408691\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 17: 0/23 Loss: 5.495224\n",
      "2025-03-07 23:00: Train Epoch 17: 20/23 Loss: 1099.477051\n",
      "2025-03-07 23:00: **********Train Epoch 17: averaged Loss: 270.831594\n",
      "2025-03-07 23:00: **********Val Epoch 17: average Loss: 3593.825684\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 18: 0/23 Loss: 5.321446\n",
      "2025-03-07 23:00: Train Epoch 18: 20/23 Loss: 1099.477539\n",
      "2025-03-07 23:00: **********Train Epoch 18: averaged Loss: 270.795546\n",
      "2025-03-07 23:00: **********Val Epoch 18: average Loss: 3593.040405\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 19: 0/23 Loss: 5.916399\n",
      "2025-03-07 23:00: Train Epoch 19: 20/23 Loss: 1098.647949\n",
      "2025-03-07 23:00: **********Train Epoch 19: averaged Loss: 270.519133\n",
      "2025-03-07 23:00: **********Val Epoch 19: average Loss: 3592.217773\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 20: 0/23 Loss: 6.163722\n",
      "2025-03-07 23:00: Train Epoch 20: 20/23 Loss: 1098.101807\n",
      "2025-03-07 23:00: **********Train Epoch 20: averaged Loss: 270.183986\n",
      "2025-03-07 23:00: **********Val Epoch 20: average Loss: 3591.980469\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 21: 0/23 Loss: 6.509824\n",
      "2025-03-07 23:00: Train Epoch 21: 20/23 Loss: 1097.067627\n",
      "2025-03-07 23:00: **********Train Epoch 21: averaged Loss: 269.992433\n",
      "2025-03-07 23:00: **********Val Epoch 21: average Loss: 3591.430664\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 22: 0/23 Loss: 6.083712\n",
      "2025-03-07 23:00: Train Epoch 22: 20/23 Loss: 1096.562012\n",
      "2025-03-07 23:00: **********Train Epoch 22: averaged Loss: 269.625204\n",
      "2025-03-07 23:00: **********Val Epoch 22: average Loss: 3590.618896\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 23: 0/23 Loss: 5.851028\n",
      "2025-03-07 23:00: Train Epoch 23: 20/23 Loss: 1095.849731\n",
      "2025-03-07 23:00: **********Train Epoch 23: averaged Loss: 269.239861\n",
      "2025-03-07 23:00: **********Val Epoch 23: average Loss: 3589.843506\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 24: 0/23 Loss: 5.623146\n",
      "2025-03-07 23:00: Train Epoch 24: 20/23 Loss: 1095.016602\n",
      "2025-03-07 23:00: **********Train Epoch 24: averaged Loss: 268.917129\n",
      "2025-03-07 23:00: **********Val Epoch 24: average Loss: 3589.668945\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 25: 0/23 Loss: 4.563601\n",
      "2025-03-07 23:00: Train Epoch 25: 20/23 Loss: 1094.932861\n",
      "2025-03-07 23:00: **********Train Epoch 25: averaged Loss: 268.390814\n",
      "2025-03-07 23:00: **********Val Epoch 25: average Loss: 3589.017090\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 26: 0/23 Loss: 3.876115\n",
      "2025-03-07 23:00: Train Epoch 26: 20/23 Loss: 1094.018677\n",
      "2025-03-07 23:00: **********Train Epoch 26: averaged Loss: 267.777132\n",
      "2025-03-07 23:00: **********Val Epoch 26: average Loss: 3588.317261\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 27: 0/23 Loss: 4.030367\n",
      "2025-03-07 23:00: Train Epoch 27: 20/23 Loss: 1092.595703\n",
      "2025-03-07 23:00: **********Train Epoch 27: averaged Loss: 267.383381\n",
      "2025-03-07 23:00: **********Val Epoch 27: average Loss: 3587.359375\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 28: 0/23 Loss: 2.931337\n",
      "2025-03-07 23:00: Train Epoch 28: 20/23 Loss: 1092.201294\n",
      "2025-03-07 23:00: **********Train Epoch 28: averaged Loss: 266.596674\n",
      "2025-03-07 23:00: **********Val Epoch 28: average Loss: 3586.481689\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 29: 0/23 Loss: 3.157749\n",
      "2025-03-07 23:00: Train Epoch 29: 20/23 Loss: 1091.125977\n",
      "2025-03-07 23:00: **********Train Epoch 29: averaged Loss: 266.096319\n",
      "2025-03-07 23:00: **********Val Epoch 29: average Loss: 3585.109619\n",
      "2025-03-07 23:00: *********************************Current best model saved!\n",
      "2025-03-07 23:00: Train Epoch 30: 0/23 Loss: 2.808386\n",
      "2025-03-07 23:01: Train Epoch 30: 20/23 Loss: 1089.814941\n",
      "2025-03-07 23:01: **********Train Epoch 30: averaged Loss: 265.647099\n",
      "2025-03-07 23:01: **********Val Epoch 30: average Loss: 3584.297852\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 31: 0/23 Loss: 2.307525\n",
      "2025-03-07 23:01: Train Epoch 31: 20/23 Loss: 1088.762573\n",
      "2025-03-07 23:01: **********Train Epoch 31: averaged Loss: 264.749980\n",
      "2025-03-07 23:01: **********Val Epoch 31: average Loss: 3583.057129\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 32: 0/23 Loss: 3.504885\n",
      "2025-03-07 23:01: Train Epoch 32: 20/23 Loss: 1087.558228\n",
      "2025-03-07 23:01: **********Train Epoch 32: averaged Loss: 264.414599\n",
      "2025-03-07 23:01: **********Val Epoch 32: average Loss: 3581.646362\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 33: 0/23 Loss: 2.172023\n",
      "2025-03-07 23:01: Train Epoch 33: 20/23 Loss: 1085.793335\n",
      "2025-03-07 23:01: **********Train Epoch 33: averaged Loss: 263.663874\n",
      "2025-03-07 23:01: **********Val Epoch 33: average Loss: 3580.753174\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 34: 0/23 Loss: 3.521248\n",
      "2025-03-07 23:01: Train Epoch 34: 20/23 Loss: 1085.859253\n",
      "2025-03-07 23:01: **********Train Epoch 34: averaged Loss: 263.107750\n",
      "2025-03-07 23:01: **********Val Epoch 34: average Loss: 3579.940918\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 35: 0/23 Loss: 2.108950\n",
      "2025-03-07 23:01: Train Epoch 35: 20/23 Loss: 1083.660522\n",
      "2025-03-07 23:01: **********Train Epoch 35: averaged Loss: 262.461099\n",
      "2025-03-07 23:01: **********Val Epoch 35: average Loss: 3577.695557\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 36: 0/23 Loss: 3.104301\n",
      "2025-03-07 23:01: Train Epoch 36: 20/23 Loss: 1082.962891\n",
      "2025-03-07 23:01: **********Train Epoch 36: averaged Loss: 261.627305\n",
      "2025-03-07 23:01: **********Val Epoch 36: average Loss: 3575.953125\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 37: 0/23 Loss: 3.652380\n",
      "2025-03-07 23:01: Train Epoch 37: 20/23 Loss: 1081.251343\n",
      "2025-03-07 23:01: **********Train Epoch 37: averaged Loss: 261.055098\n",
      "2025-03-07 23:01: **********Val Epoch 37: average Loss: 3575.110718\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 38: 0/23 Loss: 1.766286\n",
      "2025-03-07 23:01: Train Epoch 38: 20/23 Loss: 1079.983887\n",
      "2025-03-07 23:01: **********Train Epoch 38: averaged Loss: 260.177818\n",
      "2025-03-07 23:01: **********Val Epoch 38: average Loss: 3573.179443\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 39: 0/23 Loss: 5.247577\n",
      "2025-03-07 23:01: Train Epoch 39: 20/23 Loss: 1078.170776\n",
      "2025-03-07 23:01: **********Train Epoch 39: averaged Loss: 259.950965\n",
      "2025-03-07 23:01: **********Val Epoch 39: average Loss: 3572.096313\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 40: 0/23 Loss: 1.205206\n",
      "2025-03-07 23:01: Train Epoch 40: 20/23 Loss: 1077.053223\n",
      "2025-03-07 23:01: **********Train Epoch 40: averaged Loss: 258.805616\n",
      "2025-03-07 23:01: **********Val Epoch 40: average Loss: 3569.743652\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 41: 0/23 Loss: 6.210300\n",
      "2025-03-07 23:01: Train Epoch 41: 20/23 Loss: 1075.748047\n",
      "2025-03-07 23:01: **********Train Epoch 41: averaged Loss: 258.596967\n",
      "2025-03-07 23:01: **********Val Epoch 41: average Loss: 3568.525635\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 42: 0/23 Loss: 2.075243\n",
      "2025-03-07 23:01: Train Epoch 42: 20/23 Loss: 1072.137573\n",
      "2025-03-07 23:01: **********Train Epoch 42: averaged Loss: 257.135455\n",
      "2025-03-07 23:01: **********Val Epoch 42: average Loss: 3566.458374\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 43: 0/23 Loss: 6.820331\n",
      "2025-03-07 23:01: Train Epoch 43: 20/23 Loss: 1070.781250\n",
      "2025-03-07 23:01: **********Train Epoch 43: averaged Loss: 256.609567\n",
      "2025-03-07 23:01: **********Val Epoch 43: average Loss: 3564.446777\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 44: 0/23 Loss: 3.413836\n",
      "2025-03-07 23:01: Train Epoch 44: 20/23 Loss: 1067.755493\n",
      "2025-03-07 23:01: **********Train Epoch 44: averaged Loss: 255.148911\n",
      "2025-03-07 23:01: **********Val Epoch 44: average Loss: 3561.733887\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 45: 0/23 Loss: 4.586179\n",
      "2025-03-07 23:01: Train Epoch 45: 20/23 Loss: 1067.122070\n",
      "2025-03-07 23:01: **********Train Epoch 45: averaged Loss: 254.188195\n",
      "2025-03-07 23:01: **********Val Epoch 45: average Loss: 3560.099121\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 46: 0/23 Loss: 2.851513\n",
      "2025-03-07 23:01: Train Epoch 46: 20/23 Loss: 1064.688477\n",
      "2025-03-07 23:01: **********Train Epoch 46: averaged Loss: 253.059636\n",
      "2025-03-07 23:01: **********Val Epoch 46: average Loss: 3557.510498\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 47: 0/23 Loss: 4.208910\n",
      "2025-03-07 23:01: Train Epoch 47: 20/23 Loss: 1061.800781\n",
      "2025-03-07 23:01: **********Train Epoch 47: averaged Loss: 251.724382\n",
      "2025-03-07 23:01: **********Val Epoch 47: average Loss: 3555.079102\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 48: 0/23 Loss: 3.710839\n",
      "2025-03-07 23:01: Train Epoch 48: 20/23 Loss: 1059.436035\n",
      "2025-03-07 23:01: **********Train Epoch 48: averaged Loss: 250.674444\n",
      "2025-03-07 23:01: **********Val Epoch 48: average Loss: 3552.359131\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 49: 0/23 Loss: 3.563064\n",
      "2025-03-07 23:01: Train Epoch 49: 20/23 Loss: 1054.796509\n",
      "2025-03-07 23:01: **********Train Epoch 49: averaged Loss: 248.827612\n",
      "2025-03-07 23:01: **********Val Epoch 49: average Loss: 3550.118164\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 50: 0/23 Loss: 5.230790\n",
      "2025-03-07 23:01: Train Epoch 50: 20/23 Loss: 1053.251953\n",
      "2025-03-07 23:01: **********Train Epoch 50: averaged Loss: 248.530504\n",
      "2025-03-07 23:01: **********Val Epoch 50: average Loss: 3547.820801\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 51: 0/23 Loss: 1.240059\n",
      "2025-03-07 23:01: Train Epoch 51: 20/23 Loss: 1050.438965\n",
      "2025-03-07 23:01: **********Train Epoch 51: averaged Loss: 246.666396\n",
      "2025-03-07 23:01: **********Val Epoch 51: average Loss: 3542.764404\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 52: 0/23 Loss: 8.164003\n",
      "2025-03-07 23:01: Train Epoch 52: 20/23 Loss: 1044.354248\n",
      "2025-03-07 23:01: **********Train Epoch 52: averaged Loss: 247.771523\n",
      "2025-03-07 23:01: **********Val Epoch 52: average Loss: 3542.444580\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 53: 0/23 Loss: 1.433736\n",
      "2025-03-07 23:01: Train Epoch 53: 20/23 Loss: 1042.642822\n",
      "2025-03-07 23:01: **********Train Epoch 53: averaged Loss: 244.158652\n",
      "2025-03-07 23:01: **********Val Epoch 53: average Loss: 3539.058594\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 54: 0/23 Loss: 8.360982\n",
      "2025-03-07 23:01: Train Epoch 54: 20/23 Loss: 1041.427856\n",
      "2025-03-07 23:01: **********Train Epoch 54: averaged Loss: 244.130390\n",
      "2025-03-07 23:01: **********Val Epoch 54: average Loss: 3533.814819\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 55: 0/23 Loss: 3.443285\n",
      "2025-03-07 23:01: Train Epoch 55: 20/23 Loss: 1036.651611\n",
      "2025-03-07 23:01: **********Train Epoch 55: averaged Loss: 241.868099\n",
      "2025-03-07 23:01: **********Val Epoch 55: average Loss: 3531.901611\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 56: 0/23 Loss: 6.223634\n",
      "2025-03-07 23:01: Train Epoch 56: 20/23 Loss: 1033.543457\n",
      "2025-03-07 23:01: **********Train Epoch 56: averaged Loss: 242.596365\n",
      "2025-03-07 23:01: **********Val Epoch 56: average Loss: 3527.671875\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 57: 0/23 Loss: 1.588659\n",
      "2025-03-07 23:01: Train Epoch 57: 20/23 Loss: 1030.652588\n",
      "2025-03-07 23:01: **********Train Epoch 57: averaged Loss: 240.520734\n",
      "2025-03-07 23:01: **********Val Epoch 57: average Loss: 3524.489136\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 58: 0/23 Loss: 6.308992\n",
      "2025-03-07 23:01: Train Epoch 58: 20/23 Loss: 1027.000122\n",
      "2025-03-07 23:01: **********Train Epoch 58: averaged Loss: 237.476716\n",
      "2025-03-07 23:01: **********Val Epoch 58: average Loss: 3524.469849\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 59: 0/23 Loss: 2.772810\n",
      "2025-03-07 23:01: Train Epoch 59: 20/23 Loss: 1024.677368\n",
      "2025-03-07 23:01: **********Train Epoch 59: averaged Loss: 235.628640\n",
      "2025-03-07 23:01: **********Val Epoch 59: average Loss: 3517.926758\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 60: 0/23 Loss: 6.644293\n",
      "2025-03-07 23:01: Train Epoch 60: 20/23 Loss: 1023.308472\n",
      "2025-03-07 23:01: **********Train Epoch 60: averaged Loss: 236.212525\n",
      "2025-03-07 23:01: **********Val Epoch 60: average Loss: 3514.835327\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 61: 0/23 Loss: 2.399912\n",
      "2025-03-07 23:01: Train Epoch 61: 20/23 Loss: 1017.590332\n",
      "2025-03-07 23:01: **********Train Epoch 61: averaged Loss: 236.366076\n",
      "2025-03-07 23:01: **********Val Epoch 61: average Loss: 3512.293945\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 62: 0/23 Loss: 9.143366\n",
      "2025-03-07 23:01: Train Epoch 62: 20/23 Loss: 1014.287476\n",
      "2025-03-07 23:01: **********Train Epoch 62: averaged Loss: 236.118284\n",
      "2025-03-07 23:01: **********Val Epoch 62: average Loss: 3508.410278\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 63: 0/23 Loss: 2.446206\n",
      "2025-03-07 23:01: Train Epoch 63: 20/23 Loss: 1012.049072\n",
      "2025-03-07 23:01: **********Train Epoch 63: averaged Loss: 231.807496\n",
      "2025-03-07 23:01: **********Val Epoch 63: average Loss: 3504.300415\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 64: 0/23 Loss: 7.882077\n",
      "2025-03-07 23:01: Train Epoch 64: 20/23 Loss: 1004.821655\n",
      "2025-03-07 23:01: **********Train Epoch 64: averaged Loss: 231.781552\n",
      "2025-03-07 23:01: **********Val Epoch 64: average Loss: 3501.953735\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 65: 0/23 Loss: 2.573035\n",
      "2025-03-07 23:01: Train Epoch 65: 20/23 Loss: 1004.924561\n",
      "2025-03-07 23:01: **********Train Epoch 65: averaged Loss: 230.396981\n",
      "2025-03-07 23:01: **********Val Epoch 65: average Loss: 3497.795898\n",
      "2025-03-07 23:01: *********************************Current best model saved!\n",
      "2025-03-07 23:01: Train Epoch 66: 0/23 Loss: 7.275782\n",
      "2025-03-07 23:02: Train Epoch 66: 20/23 Loss: 1005.115479\n",
      "2025-03-07 23:02: **********Train Epoch 66: averaged Loss: 231.480188\n",
      "2025-03-07 23:02: **********Val Epoch 66: average Loss: 3498.851562\n",
      "2025-03-07 23:02: Train Epoch 67: 0/23 Loss: 2.637510\n",
      "2025-03-07 23:02: Train Epoch 67: 20/23 Loss: 1005.240234\n",
      "2025-03-07 23:02: **********Train Epoch 67: averaged Loss: 229.886612\n",
      "2025-03-07 23:02: **********Val Epoch 67: average Loss: 3496.010986\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 68: 0/23 Loss: 5.833430\n",
      "2025-03-07 23:02: Train Epoch 68: 20/23 Loss: 998.329468\n",
      "2025-03-07 23:02: **********Train Epoch 68: averaged Loss: 229.200259\n",
      "2025-03-07 23:02: **********Val Epoch 68: average Loss: 3494.021729\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 69: 0/23 Loss: 3.145376\n",
      "2025-03-07 23:02: Train Epoch 69: 20/23 Loss: 993.639038\n",
      "2025-03-07 23:02: **********Train Epoch 69: averaged Loss: 227.210656\n",
      "2025-03-07 23:02: **********Val Epoch 69: average Loss: 3490.826172\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 70: 0/23 Loss: 7.703942\n",
      "2025-03-07 23:02: Train Epoch 70: 20/23 Loss: 992.208801\n",
      "2025-03-07 23:02: **********Train Epoch 70: averaged Loss: 227.801198\n",
      "2025-03-07 23:02: **********Val Epoch 70: average Loss: 3487.535034\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 71: 0/23 Loss: 2.240950\n",
      "2025-03-07 23:02: Train Epoch 71: 20/23 Loss: 987.247803\n",
      "2025-03-07 23:02: **********Train Epoch 71: averaged Loss: 225.515702\n",
      "2025-03-07 23:02: **********Val Epoch 71: average Loss: 3487.203613\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 72: 0/23 Loss: 5.914256\n",
      "2025-03-07 23:02: Train Epoch 72: 20/23 Loss: 987.850098\n",
      "2025-03-07 23:02: **********Train Epoch 72: averaged Loss: 226.888462\n",
      "2025-03-07 23:02: **********Val Epoch 72: average Loss: 3484.319458\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 73: 0/23 Loss: 0.625003\n",
      "2025-03-07 23:02: Train Epoch 73: 20/23 Loss: 981.582275\n",
      "2025-03-07 23:02: **********Train Epoch 73: averaged Loss: 226.452621\n",
      "2025-03-07 23:02: **********Val Epoch 73: average Loss: 3482.158203\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 74: 0/23 Loss: 8.489195\n",
      "2025-03-07 23:02: Train Epoch 74: 20/23 Loss: 979.096558\n",
      "2025-03-07 23:02: **********Train Epoch 74: averaged Loss: 226.446055\n",
      "2025-03-07 23:02: **********Val Epoch 74: average Loss: 3480.829834\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 75: 0/23 Loss: 1.991251\n",
      "2025-03-07 23:02: Train Epoch 75: 20/23 Loss: 982.598083\n",
      "2025-03-07 23:02: **********Train Epoch 75: averaged Loss: 224.570153\n",
      "2025-03-07 23:02: **********Val Epoch 75: average Loss: 3474.790039\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 76: 0/23 Loss: 5.873054\n",
      "2025-03-07 23:02: Train Epoch 76: 20/23 Loss: 975.559448\n",
      "2025-03-07 23:02: **********Train Epoch 76: averaged Loss: 224.043606\n",
      "2025-03-07 23:02: **********Val Epoch 76: average Loss: 3471.251099\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 77: 0/23 Loss: 1.645522\n",
      "2025-03-07 23:02: Train Epoch 77: 20/23 Loss: 971.027466\n",
      "2025-03-07 23:02: **********Train Epoch 77: averaged Loss: 223.043739\n",
      "2025-03-07 23:02: **********Val Epoch 77: average Loss: 3470.708984\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 78: 0/23 Loss: 8.240847\n",
      "2025-03-07 23:02: Train Epoch 78: 20/23 Loss: 972.510620\n",
      "2025-03-07 23:02: **********Train Epoch 78: averaged Loss: 223.057944\n",
      "2025-03-07 23:02: **********Val Epoch 78: average Loss: 3469.793823\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 79: 0/23 Loss: 2.166986\n",
      "2025-03-07 23:02: Train Epoch 79: 20/23 Loss: 966.739380\n",
      "2025-03-07 23:02: **********Train Epoch 79: averaged Loss: 222.078606\n",
      "2025-03-07 23:02: **********Val Epoch 79: average Loss: 3464.797607\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 80: 0/23 Loss: 8.520424\n",
      "2025-03-07 23:02: Train Epoch 80: 20/23 Loss: 961.528137\n",
      "2025-03-07 23:02: **********Train Epoch 80: averaged Loss: 221.471859\n",
      "2025-03-07 23:02: **********Val Epoch 80: average Loss: 3463.792236\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 81: 0/23 Loss: 1.933414\n",
      "2025-03-07 23:02: Train Epoch 81: 20/23 Loss: 960.300049\n",
      "2025-03-07 23:02: **********Train Epoch 81: averaged Loss: 219.570564\n",
      "2025-03-07 23:02: **********Val Epoch 81: average Loss: 3463.459106\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 82: 0/23 Loss: 6.275267\n",
      "2025-03-07 23:02: Train Epoch 82: 20/23 Loss: 962.081787\n",
      "2025-03-07 23:02: **********Train Epoch 82: averaged Loss: 219.450025\n",
      "2025-03-07 23:02: **********Val Epoch 82: average Loss: 3459.040039\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 83: 0/23 Loss: 1.097245\n",
      "2025-03-07 23:02: Train Epoch 83: 20/23 Loss: 947.463135\n",
      "2025-03-07 23:02: **********Train Epoch 83: averaged Loss: 216.197780\n",
      "2025-03-07 23:02: **********Val Epoch 83: average Loss: 3455.025635\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 84: 0/23 Loss: 7.665128\n",
      "2025-03-07 23:02: Train Epoch 84: 20/23 Loss: 946.218628\n",
      "2025-03-07 23:02: **********Train Epoch 84: averaged Loss: 216.391548\n",
      "2025-03-07 23:02: **********Val Epoch 84: average Loss: 3452.293457\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 85: 0/23 Loss: 0.946219\n",
      "2025-03-07 23:02: Train Epoch 85: 20/23 Loss: 946.744080\n",
      "2025-03-07 23:02: **********Train Epoch 85: averaged Loss: 214.723654\n",
      "2025-03-07 23:02: **********Val Epoch 85: average Loss: 3443.937622\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 86: 0/23 Loss: 8.152183\n",
      "2025-03-07 23:02: Train Epoch 86: 20/23 Loss: 940.481445\n",
      "2025-03-07 23:02: **********Train Epoch 86: averaged Loss: 214.213903\n",
      "2025-03-07 23:02: **********Val Epoch 86: average Loss: 3442.449951\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 87: 0/23 Loss: 0.564956\n",
      "2025-03-07 23:02: Train Epoch 87: 20/23 Loss: 938.680176\n",
      "2025-03-07 23:02: **********Train Epoch 87: averaged Loss: 212.334221\n",
      "2025-03-07 23:02: **********Val Epoch 87: average Loss: 3436.527100\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 88: 0/23 Loss: 7.544470\n",
      "2025-03-07 23:02: Train Epoch 88: 20/23 Loss: 926.807983\n",
      "2025-03-07 23:02: **********Train Epoch 88: averaged Loss: 213.051268\n",
      "2025-03-07 23:02: **********Val Epoch 88: average Loss: 3433.975220\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 89: 0/23 Loss: 0.552712\n",
      "2025-03-07 23:02: Train Epoch 89: 20/23 Loss: 929.175293\n",
      "2025-03-07 23:02: **********Train Epoch 89: averaged Loss: 211.427271\n",
      "2025-03-07 23:02: **********Val Epoch 89: average Loss: 3423.449097\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 90: 0/23 Loss: 7.821304\n",
      "2025-03-07 23:02: Train Epoch 90: 20/23 Loss: 933.465942\n",
      "2025-03-07 23:02: **********Train Epoch 90: averaged Loss: 214.755825\n",
      "2025-03-07 23:02: **********Val Epoch 90: average Loss: 3422.891357\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 91: 0/23 Loss: 0.806565\n",
      "2025-03-07 23:02: Train Epoch 91: 20/23 Loss: 925.120911\n",
      "2025-03-07 23:02: **********Train Epoch 91: averaged Loss: 208.306990\n",
      "2025-03-07 23:02: **********Val Epoch 91: average Loss: 3422.870728\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 92: 0/23 Loss: 8.609678\n",
      "2025-03-07 23:02: Train Epoch 92: 20/23 Loss: 918.899963\n",
      "2025-03-07 23:02: **********Train Epoch 92: averaged Loss: 208.840091\n",
      "2025-03-07 23:02: **********Val Epoch 92: average Loss: 3416.200928\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 93: 0/23 Loss: 0.596945\n",
      "2025-03-07 23:02: Train Epoch 93: 20/23 Loss: 914.108215\n",
      "2025-03-07 23:02: **********Train Epoch 93: averaged Loss: 206.926967\n",
      "2025-03-07 23:02: **********Val Epoch 93: average Loss: 3416.048950\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 94: 0/23 Loss: 6.592058\n",
      "2025-03-07 23:02: Train Epoch 94: 20/23 Loss: 906.642639\n",
      "2025-03-07 23:02: **********Train Epoch 94: averaged Loss: 208.712650\n",
      "2025-03-07 23:02: **********Val Epoch 94: average Loss: 3410.266724\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 95: 0/23 Loss: 1.303504\n",
      "2025-03-07 23:02: Train Epoch 95: 20/23 Loss: 903.574829\n",
      "2025-03-07 23:02: **********Train Epoch 95: averaged Loss: 203.887114\n",
      "2025-03-07 23:02: **********Val Epoch 95: average Loss: 3403.613525\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 96: 0/23 Loss: 6.827526\n",
      "2025-03-07 23:02: Train Epoch 96: 20/23 Loss: 894.882751\n",
      "2025-03-07 23:02: **********Train Epoch 96: averaged Loss: 203.442076\n",
      "2025-03-07 23:02: **********Val Epoch 96: average Loss: 3401.745728\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 97: 0/23 Loss: 2.451581\n",
      "2025-03-07 23:02: Train Epoch 97: 20/23 Loss: 896.030273\n",
      "2025-03-07 23:02: **********Train Epoch 97: averaged Loss: 201.793165\n",
      "2025-03-07 23:02: **********Val Epoch 97: average Loss: 3393.869263\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 98: 0/23 Loss: 4.021866\n",
      "2025-03-07 23:02: Train Epoch 98: 20/23 Loss: 892.502441\n",
      "2025-03-07 23:02: **********Train Epoch 98: averaged Loss: 202.952999\n",
      "2025-03-07 23:02: **********Val Epoch 98: average Loss: 3395.002686\n",
      "2025-03-07 23:02: Train Epoch 99: 0/23 Loss: 1.537506\n",
      "2025-03-07 23:02: Train Epoch 99: 20/23 Loss: 885.883179\n",
      "2025-03-07 23:02: **********Train Epoch 99: averaged Loss: 199.905953\n",
      "2025-03-07 23:02: **********Val Epoch 99: average Loss: 3389.861084\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 100: 0/23 Loss: 10.085789\n",
      "2025-03-07 23:02: Train Epoch 100: 20/23 Loss: 875.081177\n",
      "2025-03-07 23:02: **********Train Epoch 100: averaged Loss: 198.431533\n",
      "2025-03-07 23:02: **********Val Epoch 100: average Loss: 3380.952515\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 101: 0/23 Loss: 3.310991\n",
      "2025-03-07 23:02: Train Epoch 101: 20/23 Loss: 870.091797\n",
      "2025-03-07 23:02: **********Train Epoch 101: averaged Loss: 198.839073\n",
      "2025-03-07 23:02: **********Val Epoch 101: average Loss: 3379.031738\n",
      "2025-03-07 23:02: *********************************Current best model saved!\n",
      "2025-03-07 23:02: Train Epoch 102: 0/23 Loss: 5.477464\n",
      "2025-03-07 23:03: Train Epoch 102: 20/23 Loss: 864.334351\n",
      "2025-03-07 23:03: **********Train Epoch 102: averaged Loss: 201.127280\n",
      "2025-03-07 23:03: **********Val Epoch 102: average Loss: 3373.808960\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 103: 0/23 Loss: 0.922435\n",
      "2025-03-07 23:03: Train Epoch 103: 20/23 Loss: 862.355713\n",
      "2025-03-07 23:03: **********Train Epoch 103: averaged Loss: 194.557343\n",
      "2025-03-07 23:03: **********Val Epoch 103: average Loss: 3371.030151\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 104: 0/23 Loss: 7.833483\n",
      "2025-03-07 23:03: Train Epoch 104: 20/23 Loss: 862.072021\n",
      "2025-03-07 23:03: **********Train Epoch 104: averaged Loss: 198.496623\n",
      "2025-03-07 23:03: **********Val Epoch 104: average Loss: 3364.014404\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 105: 0/23 Loss: 0.602082\n",
      "2025-03-07 23:03: Train Epoch 105: 20/23 Loss: 857.866089\n",
      "2025-03-07 23:03: **********Train Epoch 105: averaged Loss: 193.465086\n",
      "2025-03-07 23:03: **********Val Epoch 105: average Loss: 3359.810303\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 106: 0/23 Loss: 9.037101\n",
      "2025-03-07 23:03: Train Epoch 106: 20/23 Loss: 853.103638\n",
      "2025-03-07 23:03: **********Train Epoch 106: averaged Loss: 196.918550\n",
      "2025-03-07 23:03: **********Val Epoch 106: average Loss: 3348.830811\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 107: 0/23 Loss: 0.649278\n",
      "2025-03-07 23:03: Train Epoch 107: 20/23 Loss: 848.007629\n",
      "2025-03-07 23:03: **********Train Epoch 107: averaged Loss: 191.330981\n",
      "2025-03-07 23:03: **********Val Epoch 107: average Loss: 3352.316528\n",
      "2025-03-07 23:03: Train Epoch 108: 0/23 Loss: 9.104508\n",
      "2025-03-07 23:03: Train Epoch 108: 20/23 Loss: 835.831177\n",
      "2025-03-07 23:03: **********Train Epoch 108: averaged Loss: 193.291117\n",
      "2025-03-07 23:03: **********Val Epoch 108: average Loss: 3347.526978\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 109: 0/23 Loss: 0.541664\n",
      "2025-03-07 23:03: Train Epoch 109: 20/23 Loss: 829.132507\n",
      "2025-03-07 23:03: **********Train Epoch 109: averaged Loss: 188.426907\n",
      "2025-03-07 23:03: **********Val Epoch 109: average Loss: 3343.698730\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 110: 0/23 Loss: 7.735256\n",
      "2025-03-07 23:03: Train Epoch 110: 20/23 Loss: 832.184998\n",
      "2025-03-07 23:03: **********Train Epoch 110: averaged Loss: 192.163131\n",
      "2025-03-07 23:03: **********Val Epoch 110: average Loss: 3329.646851\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 111: 0/23 Loss: 1.153003\n",
      "2025-03-07 23:03: Train Epoch 111: 20/23 Loss: 839.548218\n",
      "2025-03-07 23:03: **********Train Epoch 111: averaged Loss: 186.727547\n",
      "2025-03-07 23:03: **********Val Epoch 111: average Loss: 3333.005981\n",
      "2025-03-07 23:03: Train Epoch 112: 0/23 Loss: 7.737087\n",
      "2025-03-07 23:03: Train Epoch 112: 20/23 Loss: 824.729004\n",
      "2025-03-07 23:03: **********Train Epoch 112: averaged Loss: 190.245082\n",
      "2025-03-07 23:03: **********Val Epoch 112: average Loss: 3326.614746\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 113: 0/23 Loss: 1.624815\n",
      "2025-03-07 23:03: Train Epoch 113: 20/23 Loss: 818.078491\n",
      "2025-03-07 23:03: **********Train Epoch 113: averaged Loss: 183.867771\n",
      "2025-03-07 23:03: **********Val Epoch 113: average Loss: 3319.083740\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 114: 0/23 Loss: 9.417726\n",
      "2025-03-07 23:03: Train Epoch 114: 20/23 Loss: 810.564209\n",
      "2025-03-07 23:03: **********Train Epoch 114: averaged Loss: 181.672672\n",
      "2025-03-07 23:03: **********Val Epoch 114: average Loss: 3322.492920\n",
      "2025-03-07 23:03: Train Epoch 115: 0/23 Loss: 0.524968\n",
      "2025-03-07 23:03: Train Epoch 115: 20/23 Loss: 800.864014\n",
      "2025-03-07 23:03: **********Train Epoch 115: averaged Loss: 179.217943\n",
      "2025-03-07 23:03: **********Val Epoch 115: average Loss: 3312.224731\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 116: 0/23 Loss: 4.202137\n",
      "2025-03-07 23:03: Train Epoch 116: 20/23 Loss: 807.319946\n",
      "2025-03-07 23:03: **********Train Epoch 116: averaged Loss: 179.369448\n",
      "2025-03-07 23:03: **********Val Epoch 116: average Loss: 3301.561890\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 117: 0/23 Loss: 0.523750\n",
      "2025-03-07 23:03: Train Epoch 117: 20/23 Loss: 796.030884\n",
      "2025-03-07 23:03: **********Train Epoch 117: averaged Loss: 184.348982\n",
      "2025-03-07 23:03: **********Val Epoch 117: average Loss: 3294.072510\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 118: 0/23 Loss: 9.513308\n",
      "2025-03-07 23:03: Train Epoch 118: 20/23 Loss: 780.694214\n",
      "2025-03-07 23:03: **********Train Epoch 118: averaged Loss: 179.689343\n",
      "2025-03-07 23:03: **********Val Epoch 118: average Loss: 3286.616455\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 119: 0/23 Loss: 0.807043\n",
      "2025-03-07 23:03: Train Epoch 119: 20/23 Loss: 780.195557\n",
      "2025-03-07 23:03: **********Train Epoch 119: averaged Loss: 183.179795\n",
      "2025-03-07 23:03: **********Val Epoch 119: average Loss: 3291.622803\n",
      "2025-03-07 23:03: Train Epoch 120: 0/23 Loss: 9.217817\n",
      "2025-03-07 23:03: Train Epoch 120: 20/23 Loss: 776.149780\n",
      "2025-03-07 23:03: **********Train Epoch 120: averaged Loss: 176.693271\n",
      "2025-03-07 23:03: **********Val Epoch 120: average Loss: 3274.957642\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 121: 0/23 Loss: 4.761799\n",
      "2025-03-07 23:03: Train Epoch 121: 20/23 Loss: 771.713074\n",
      "2025-03-07 23:03: **********Train Epoch 121: averaged Loss: 175.007377\n",
      "2025-03-07 23:03: **********Val Epoch 121: average Loss: 3274.080200\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 122: 0/23 Loss: 6.067772\n",
      "2025-03-07 23:03: Train Epoch 122: 20/23 Loss: 760.363586\n",
      "2025-03-07 23:03: **********Train Epoch 122: averaged Loss: 176.387667\n",
      "2025-03-07 23:03: **********Val Epoch 122: average Loss: 3267.907959\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 123: 0/23 Loss: 3.345730\n",
      "2025-03-07 23:03: Train Epoch 123: 20/23 Loss: 759.836914\n",
      "2025-03-07 23:03: **********Train Epoch 123: averaged Loss: 177.880458\n",
      "2025-03-07 23:03: **********Val Epoch 123: average Loss: 3267.994263\n",
      "2025-03-07 23:03: Train Epoch 124: 0/23 Loss: 12.367552\n",
      "2025-03-07 23:03: Train Epoch 124: 20/23 Loss: 750.607483\n",
      "2025-03-07 23:03: **********Train Epoch 124: averaged Loss: 173.103816\n",
      "2025-03-07 23:03: **********Val Epoch 124: average Loss: 3255.466919\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 125: 0/23 Loss: 0.746111\n",
      "2025-03-07 23:03: Train Epoch 125: 20/23 Loss: 744.081177\n",
      "2025-03-07 23:03: **********Train Epoch 125: averaged Loss: 171.160361\n",
      "2025-03-07 23:03: **********Val Epoch 125: average Loss: 3247.829346\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 126: 0/23 Loss: 8.241635\n",
      "2025-03-07 23:03: Train Epoch 126: 20/23 Loss: 747.883301\n",
      "2025-03-07 23:03: **********Train Epoch 126: averaged Loss: 172.162222\n",
      "2025-03-07 23:03: **********Val Epoch 126: average Loss: 3246.752197\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 127: 0/23 Loss: 4.303478\n",
      "2025-03-07 23:03: Train Epoch 127: 20/23 Loss: 742.268982\n",
      "2025-03-07 23:03: **********Train Epoch 127: averaged Loss: 173.462336\n",
      "2025-03-07 23:03: **********Val Epoch 127: average Loss: 3233.116699\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 128: 0/23 Loss: 9.318871\n",
      "2025-03-07 23:03: Train Epoch 128: 20/23 Loss: 726.979980\n",
      "2025-03-07 23:03: **********Train Epoch 128: averaged Loss: 167.334868\n",
      "2025-03-07 23:03: **********Val Epoch 128: average Loss: 3238.213501\n",
      "2025-03-07 23:03: Train Epoch 129: 0/23 Loss: 1.169578\n",
      "2025-03-07 23:03: Train Epoch 129: 20/23 Loss: 727.996643\n",
      "2025-03-07 23:03: **********Train Epoch 129: averaged Loss: 165.033731\n",
      "2025-03-07 23:03: **********Val Epoch 129: average Loss: 3233.375854\n",
      "2025-03-07 23:03: Train Epoch 130: 0/23 Loss: 7.309993\n",
      "2025-03-07 23:03: Train Epoch 130: 20/23 Loss: 729.098389\n",
      "2025-03-07 23:03: **********Train Epoch 130: averaged Loss: 165.883913\n",
      "2025-03-07 23:03: **********Val Epoch 130: average Loss: 3222.634277\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 131: 0/23 Loss: 1.104258\n",
      "2025-03-07 23:03: Train Epoch 131: 20/23 Loss: 703.835754\n",
      "2025-03-07 23:03: **********Train Epoch 131: averaged Loss: 161.686599\n",
      "2025-03-07 23:03: **********Val Epoch 131: average Loss: 3208.280273\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 132: 0/23 Loss: 8.016613\n",
      "2025-03-07 23:03: Train Epoch 132: 20/23 Loss: 702.203186\n",
      "2025-03-07 23:03: **********Train Epoch 132: averaged Loss: 159.471135\n",
      "2025-03-07 23:03: **********Val Epoch 132: average Loss: 3204.178589\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 133: 0/23 Loss: 1.451249\n",
      "2025-03-07 23:03: Train Epoch 133: 20/23 Loss: 692.834961\n",
      "2025-03-07 23:03: **********Train Epoch 133: averaged Loss: 162.610779\n",
      "2025-03-07 23:03: **********Val Epoch 133: average Loss: 3202.565430\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 134: 0/23 Loss: 8.145243\n",
      "2025-03-07 23:03: Train Epoch 134: 20/23 Loss: 681.672424\n",
      "2025-03-07 23:03: **********Train Epoch 134: averaged Loss: 160.439637\n",
      "2025-03-07 23:03: **********Val Epoch 134: average Loss: 3192.815918\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 135: 0/23 Loss: 3.276566\n",
      "2025-03-07 23:03: Train Epoch 135: 20/23 Loss: 682.628662\n",
      "2025-03-07 23:03: **********Train Epoch 135: averaged Loss: 156.217376\n",
      "2025-03-07 23:03: **********Val Epoch 135: average Loss: 3191.583618\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 136: 0/23 Loss: 7.396605\n",
      "2025-03-07 23:03: Train Epoch 136: 20/23 Loss: 680.746826\n",
      "2025-03-07 23:03: **********Train Epoch 136: averaged Loss: 153.030719\n",
      "2025-03-07 23:03: **********Val Epoch 136: average Loss: 3170.839844\n",
      "2025-03-07 23:03: *********************************Current best model saved!\n",
      "2025-03-07 23:03: Train Epoch 137: 0/23 Loss: 0.580857\n",
      "2025-03-07 23:04: Train Epoch 137: 20/23 Loss: 665.445557\n",
      "2025-03-07 23:04: **********Train Epoch 137: averaged Loss: 152.270499\n",
      "2025-03-07 23:04: **********Val Epoch 137: average Loss: 3170.962158\n",
      "2025-03-07 23:04: Train Epoch 138: 0/23 Loss: 7.826981\n",
      "2025-03-07 23:04: Train Epoch 138: 20/23 Loss: 652.384033\n",
      "2025-03-07 23:04: **********Train Epoch 138: averaged Loss: 157.071268\n",
      "2025-03-07 23:04: **********Val Epoch 138: average Loss: 3173.483032\n",
      "2025-03-07 23:04: Train Epoch 139: 0/23 Loss: 4.309751\n",
      "2025-03-07 23:04: Train Epoch 139: 20/23 Loss: 660.474060\n",
      "2025-03-07 23:04: **********Train Epoch 139: averaged Loss: 156.395669\n",
      "2025-03-07 23:04: **********Val Epoch 139: average Loss: 3167.719727\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 140: 0/23 Loss: 13.392532\n",
      "2025-03-07 23:04: Train Epoch 140: 20/23 Loss: 667.958008\n",
      "2025-03-07 23:04: **********Train Epoch 140: averaged Loss: 164.032883\n",
      "2025-03-07 23:04: **********Val Epoch 140: average Loss: 3159.674927\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 141: 0/23 Loss: 1.729126\n",
      "2025-03-07 23:04: Train Epoch 141: 20/23 Loss: 654.507202\n",
      "2025-03-07 23:04: **********Train Epoch 141: averaged Loss: 154.599590\n",
      "2025-03-07 23:04: **********Val Epoch 141: average Loss: 3149.207764\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 142: 0/23 Loss: 9.565798\n",
      "2025-03-07 23:04: Train Epoch 142: 20/23 Loss: 633.176636\n",
      "2025-03-07 23:04: **********Train Epoch 142: averaged Loss: 150.847205\n",
      "2025-03-07 23:04: **********Val Epoch 142: average Loss: 3149.690308\n",
      "2025-03-07 23:04: Train Epoch 143: 0/23 Loss: 2.062334\n",
      "2025-03-07 23:04: Train Epoch 143: 20/23 Loss: 628.347168\n",
      "2025-03-07 23:04: **********Train Epoch 143: averaged Loss: 146.201893\n",
      "2025-03-07 23:04: **********Val Epoch 143: average Loss: 3133.749756\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 144: 0/23 Loss: 7.861896\n",
      "2025-03-07 23:04: Train Epoch 144: 20/23 Loss: 636.258667\n",
      "2025-03-07 23:04: **********Train Epoch 144: averaged Loss: 146.173536\n",
      "2025-03-07 23:04: **********Val Epoch 144: average Loss: 3132.142456\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 145: 0/23 Loss: 0.572357\n",
      "2025-03-07 23:04: Train Epoch 145: 20/23 Loss: 610.941895\n",
      "2025-03-07 23:04: **********Train Epoch 145: averaged Loss: 146.843973\n",
      "2025-03-07 23:04: **********Val Epoch 145: average Loss: 3118.825195\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 146: 0/23 Loss: 12.686754\n",
      "2025-03-07 23:04: Train Epoch 146: 20/23 Loss: 606.609253\n",
      "2025-03-07 23:04: **********Train Epoch 146: averaged Loss: 150.215735\n",
      "2025-03-07 23:04: **********Val Epoch 146: average Loss: 3107.359985\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 147: 0/23 Loss: 1.023434\n",
      "2025-03-07 23:04: Train Epoch 147: 20/23 Loss: 618.237305\n",
      "2025-03-07 23:04: **********Train Epoch 147: averaged Loss: 149.637343\n",
      "2025-03-07 23:04: **********Val Epoch 147: average Loss: 3123.420166\n",
      "2025-03-07 23:04: Train Epoch 148: 0/23 Loss: 16.213333\n",
      "2025-03-07 23:04: Train Epoch 148: 20/23 Loss: 596.976868\n",
      "2025-03-07 23:04: **********Train Epoch 148: averaged Loss: 150.482343\n",
      "2025-03-07 23:04: **********Val Epoch 148: average Loss: 3109.640747\n",
      "2025-03-07 23:04: Train Epoch 149: 0/23 Loss: 1.010495\n",
      "2025-03-07 23:04: Train Epoch 149: 20/23 Loss: 594.213379\n",
      "2025-03-07 23:04: **********Train Epoch 149: averaged Loss: 143.102112\n",
      "2025-03-07 23:04: **********Val Epoch 149: average Loss: 3100.196655\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 150: 0/23 Loss: 7.887942\n",
      "2025-03-07 23:04: Train Epoch 150: 20/23 Loss: 576.200928\n",
      "2025-03-07 23:04: **********Train Epoch 150: averaged Loss: 141.817399\n",
      "2025-03-07 23:04: **********Val Epoch 150: average Loss: 3088.229248\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 151: 0/23 Loss: 2.022109\n",
      "2025-03-07 23:04: Train Epoch 151: 20/23 Loss: 571.798950\n",
      "2025-03-07 23:04: **********Train Epoch 151: averaged Loss: 145.361133\n",
      "2025-03-07 23:04: **********Val Epoch 151: average Loss: 3091.716675\n",
      "2025-03-07 23:04: Train Epoch 152: 0/23 Loss: 1.505140\n",
      "2025-03-07 23:04: Train Epoch 152: 20/23 Loss: 577.969177\n",
      "2025-03-07 23:04: **********Train Epoch 152: averaged Loss: 139.502749\n",
      "2025-03-07 23:04: **********Val Epoch 152: average Loss: 3084.647583\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 153: 0/23 Loss: 8.428461\n",
      "2025-03-07 23:04: Train Epoch 153: 20/23 Loss: 558.620422\n",
      "2025-03-07 23:04: **********Train Epoch 153: averaged Loss: 140.975498\n",
      "2025-03-07 23:04: **********Val Epoch 153: average Loss: 3071.449463\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 154: 0/23 Loss: 2.606983\n",
      "2025-03-07 23:04: Train Epoch 154: 20/23 Loss: 561.040527\n",
      "2025-03-07 23:04: **********Train Epoch 154: averaged Loss: 135.009739\n",
      "2025-03-07 23:04: **********Val Epoch 154: average Loss: 3064.044434\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 155: 0/23 Loss: 10.260633\n",
      "2025-03-07 23:04: Train Epoch 155: 20/23 Loss: 536.257141\n",
      "2025-03-07 23:04: **********Train Epoch 155: averaged Loss: 133.397353\n",
      "2025-03-07 23:04: **********Val Epoch 155: average Loss: 3048.641602\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 156: 0/23 Loss: 3.603379\n",
      "2025-03-07 23:04: Train Epoch 156: 20/23 Loss: 554.286316\n",
      "2025-03-07 23:04: **********Train Epoch 156: averaged Loss: 136.629674\n",
      "2025-03-07 23:04: **********Val Epoch 156: average Loss: 3053.132446\n",
      "2025-03-07 23:04: Train Epoch 157: 0/23 Loss: 14.338448\n",
      "2025-03-07 23:04: Train Epoch 157: 20/23 Loss: 533.272949\n",
      "2025-03-07 23:04: **********Train Epoch 157: averaged Loss: 141.983153\n",
      "2025-03-07 23:04: **********Val Epoch 157: average Loss: 3040.412598\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 158: 0/23 Loss: 5.357705\n",
      "2025-03-07 23:04: Train Epoch 158: 20/23 Loss: 519.667480\n",
      "2025-03-07 23:04: **********Train Epoch 158: averaged Loss: 136.846015\n",
      "2025-03-07 23:04: **********Val Epoch 158: average Loss: 3047.251343\n",
      "2025-03-07 23:04: Train Epoch 159: 0/23 Loss: 9.229758\n",
      "2025-03-07 23:04: Train Epoch 159: 20/23 Loss: 520.162048\n",
      "2025-03-07 23:04: **********Train Epoch 159: averaged Loss: 140.110428\n",
      "2025-03-07 23:04: **********Val Epoch 159: average Loss: 3026.196899\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 160: 0/23 Loss: 2.444121\n",
      "2025-03-07 23:04: Train Epoch 160: 20/23 Loss: 549.935913\n",
      "2025-03-07 23:04: **********Train Epoch 160: averaged Loss: 134.848592\n",
      "2025-03-07 23:04: **********Val Epoch 160: average Loss: 3032.316772\n",
      "2025-03-07 23:04: Train Epoch 161: 0/23 Loss: 12.587016\n",
      "2025-03-07 23:04: Train Epoch 161: 20/23 Loss: 521.549377\n",
      "2025-03-07 23:04: **********Train Epoch 161: averaged Loss: 135.082519\n",
      "2025-03-07 23:04: **********Val Epoch 161: average Loss: 3023.594482\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 162: 0/23 Loss: 1.171188\n",
      "2025-03-07 23:04: Train Epoch 162: 20/23 Loss: 499.540070\n",
      "2025-03-07 23:04: **********Train Epoch 162: averaged Loss: 126.846772\n",
      "2025-03-07 23:04: **********Val Epoch 162: average Loss: 3002.815674\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 163: 0/23 Loss: 11.367404\n",
      "2025-03-07 23:04: Train Epoch 163: 20/23 Loss: 480.936798\n",
      "2025-03-07 23:04: **********Train Epoch 163: averaged Loss: 130.784854\n",
      "2025-03-07 23:04: **********Val Epoch 163: average Loss: 3004.662109\n",
      "2025-03-07 23:04: Train Epoch 164: 0/23 Loss: 1.584680\n",
      "2025-03-07 23:04: Train Epoch 164: 20/23 Loss: 487.430878\n",
      "2025-03-07 23:04: **********Train Epoch 164: averaged Loss: 123.906300\n",
      "2025-03-07 23:04: **********Val Epoch 164: average Loss: 2979.480957\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 165: 0/23 Loss: 11.394798\n",
      "2025-03-07 23:04: Train Epoch 165: 20/23 Loss: 482.722076\n",
      "2025-03-07 23:04: **********Train Epoch 165: averaged Loss: 127.531119\n",
      "2025-03-07 23:04: **********Val Epoch 165: average Loss: 2982.930664\n",
      "2025-03-07 23:04: Train Epoch 166: 0/23 Loss: 4.323964\n",
      "2025-03-07 23:04: Train Epoch 166: 20/23 Loss: 489.830078\n",
      "2025-03-07 23:04: **********Train Epoch 166: averaged Loss: 123.468691\n",
      "2025-03-07 23:04: **********Val Epoch 166: average Loss: 2972.337524\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 167: 0/23 Loss: 13.231097\n",
      "2025-03-07 23:04: Train Epoch 167: 20/23 Loss: 486.511688\n",
      "2025-03-07 23:04: **********Train Epoch 167: averaged Loss: 129.005344\n",
      "2025-03-07 23:04: **********Val Epoch 167: average Loss: 2968.850708\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 168: 0/23 Loss: 2.533375\n",
      "2025-03-07 23:04: Train Epoch 168: 20/23 Loss: 499.895081\n",
      "2025-03-07 23:04: **********Train Epoch 168: averaged Loss: 122.527453\n",
      "2025-03-07 23:04: **********Val Epoch 168: average Loss: 2960.567749\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 169: 0/23 Loss: 10.061329\n",
      "2025-03-07 23:04: Train Epoch 169: 20/23 Loss: 429.079956\n",
      "2025-03-07 23:04: **********Train Epoch 169: averaged Loss: 120.900522\n",
      "2025-03-07 23:04: **********Val Epoch 169: average Loss: 2958.075317\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 170: 0/23 Loss: 4.700006\n",
      "2025-03-07 23:04: Train Epoch 170: 20/23 Loss: 454.357666\n",
      "2025-03-07 23:04: **********Train Epoch 170: averaged Loss: 115.718414\n",
      "2025-03-07 23:04: **********Val Epoch 170: average Loss: 2950.004517\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 171: 0/23 Loss: 10.304831\n",
      "2025-03-07 23:04: Train Epoch 171: 20/23 Loss: 440.757446\n",
      "2025-03-07 23:04: **********Train Epoch 171: averaged Loss: 115.790696\n",
      "2025-03-07 23:04: **********Val Epoch 171: average Loss: 2931.670776\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 172: 0/23 Loss: 3.455611\n",
      "2025-03-07 23:04: Train Epoch 172: 20/23 Loss: 423.462769\n",
      "2025-03-07 23:04: **********Train Epoch 172: averaged Loss: 113.119135\n",
      "2025-03-07 23:04: **********Val Epoch 172: average Loss: 2928.688354\n",
      "2025-03-07 23:04: *********************************Current best model saved!\n",
      "2025-03-07 23:04: Train Epoch 173: 0/23 Loss: 10.368188\n",
      "2025-03-07 23:05: Train Epoch 173: 20/23 Loss: 420.941833\n",
      "2025-03-07 23:05: **********Train Epoch 173: averaged Loss: 111.845790\n",
      "2025-03-07 23:05: **********Val Epoch 173: average Loss: 2914.327148\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 174: 0/23 Loss: 1.517040\n",
      "2025-03-07 23:05: Train Epoch 174: 20/23 Loss: 407.142822\n",
      "2025-03-07 23:05: **********Train Epoch 174: averaged Loss: 109.206381\n",
      "2025-03-07 23:05: **********Val Epoch 174: average Loss: 2913.080322\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 175: 0/23 Loss: 3.281628\n",
      "2025-03-07 23:05: Train Epoch 175: 20/23 Loss: 414.083862\n",
      "2025-03-07 23:05: **********Train Epoch 175: averaged Loss: 106.966702\n",
      "2025-03-07 23:05: **********Val Epoch 175: average Loss: 2922.439331\n",
      "2025-03-07 23:05: Train Epoch 176: 0/23 Loss: 0.986120\n",
      "2025-03-07 23:05: Train Epoch 176: 20/23 Loss: 382.141785\n",
      "2025-03-07 23:05: **********Train Epoch 176: averaged Loss: 102.695091\n",
      "2025-03-07 23:05: **********Val Epoch 176: average Loss: 2904.181274\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 177: 0/23 Loss: 10.385068\n",
      "2025-03-07 23:05: Train Epoch 177: 20/23 Loss: 382.293915\n",
      "2025-03-07 23:05: **********Train Epoch 177: averaged Loss: 103.560429\n",
      "2025-03-07 23:05: **********Val Epoch 177: average Loss: 2897.860718\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 178: 0/23 Loss: 1.950777\n",
      "2025-03-07 23:05: Train Epoch 178: 20/23 Loss: 399.070068\n",
      "2025-03-07 23:05: **********Train Epoch 178: averaged Loss: 103.930088\n",
      "2025-03-07 23:05: **********Val Epoch 178: average Loss: 2861.050903\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 179: 0/23 Loss: 4.331184\n",
      "2025-03-07 23:05: Train Epoch 179: 20/23 Loss: 372.535980\n",
      "2025-03-07 23:05: **********Train Epoch 179: averaged Loss: 108.742442\n",
      "2025-03-07 23:05: **********Val Epoch 179: average Loss: 2874.774170\n",
      "2025-03-07 23:05: Train Epoch 180: 0/23 Loss: 7.192793\n",
      "2025-03-07 23:05: Train Epoch 180: 20/23 Loss: 430.129578\n",
      "2025-03-07 23:05: **********Train Epoch 180: averaged Loss: 116.304495\n",
      "2025-03-07 23:05: **********Val Epoch 180: average Loss: 2846.435059\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 181: 0/23 Loss: 14.968727\n",
      "2025-03-07 23:05: Train Epoch 181: 20/23 Loss: 344.934509\n",
      "2025-03-07 23:05: **********Train Epoch 181: averaged Loss: 108.448240\n",
      "2025-03-07 23:05: **********Val Epoch 181: average Loss: 2849.297729\n",
      "2025-03-07 23:05: Train Epoch 182: 0/23 Loss: 5.905954\n",
      "2025-03-07 23:05: Train Epoch 182: 20/23 Loss: 338.385834\n",
      "2025-03-07 23:05: **********Train Epoch 182: averaged Loss: 102.820457\n",
      "2025-03-07 23:05: **********Val Epoch 182: average Loss: 2844.544067\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 183: 0/23 Loss: 15.714668\n",
      "2025-03-07 23:05: Train Epoch 183: 20/23 Loss: 322.936005\n",
      "2025-03-07 23:05: **********Train Epoch 183: averaged Loss: 99.678047\n",
      "2025-03-07 23:05: **********Val Epoch 183: average Loss: 2821.613403\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 184: 0/23 Loss: 2.771594\n",
      "2025-03-07 23:05: Train Epoch 184: 20/23 Loss: 320.638123\n",
      "2025-03-07 23:05: **********Train Epoch 184: averaged Loss: 94.884908\n",
      "2025-03-07 23:05: **********Val Epoch 184: average Loss: 2824.190063\n",
      "2025-03-07 23:05: Train Epoch 185: 0/23 Loss: 10.645815\n",
      "2025-03-07 23:05: Train Epoch 185: 20/23 Loss: 294.991638\n",
      "2025-03-07 23:05: **********Train Epoch 185: averaged Loss: 107.107655\n",
      "2025-03-07 23:05: **********Val Epoch 185: average Loss: 2807.774902\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 186: 0/23 Loss: 0.833500\n",
      "2025-03-07 23:05: Train Epoch 186: 20/23 Loss: 429.449707\n",
      "2025-03-07 23:05: **********Train Epoch 186: averaged Loss: 115.545435\n",
      "2025-03-07 23:05: **********Val Epoch 186: average Loss: 2792.751831\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 187: 0/23 Loss: 9.449953\n",
      "2025-03-07 23:05: Train Epoch 187: 20/23 Loss: 312.320312\n",
      "2025-03-07 23:05: **********Train Epoch 187: averaged Loss: 105.620315\n",
      "2025-03-07 23:05: **********Val Epoch 187: average Loss: 2788.829468\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 188: 0/23 Loss: 6.183148\n",
      "2025-03-07 23:05: Train Epoch 188: 20/23 Loss: 293.826721\n",
      "2025-03-07 23:05: **********Train Epoch 188: averaged Loss: 93.809172\n",
      "2025-03-07 23:05: **********Val Epoch 188: average Loss: 2786.122559\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 189: 0/23 Loss: 12.411183\n",
      "2025-03-07 23:05: Train Epoch 189: 20/23 Loss: 297.795166\n",
      "2025-03-07 23:05: **********Train Epoch 189: averaged Loss: 92.010474\n",
      "2025-03-07 23:05: **********Val Epoch 189: average Loss: 2792.320190\n",
      "2025-03-07 23:05: Train Epoch 190: 0/23 Loss: 5.377158\n",
      "2025-03-07 23:05: Train Epoch 190: 20/23 Loss: 260.338501\n",
      "2025-03-07 23:05: **********Train Epoch 190: averaged Loss: 90.270537\n",
      "2025-03-07 23:05: **********Val Epoch 190: average Loss: 2795.893066\n",
      "2025-03-07 23:05: Train Epoch 191: 0/23 Loss: 3.962767\n",
      "2025-03-07 23:05: Train Epoch 191: 20/23 Loss: 282.504272\n",
      "2025-03-07 23:05: **********Train Epoch 191: averaged Loss: 86.614414\n",
      "2025-03-07 23:05: **********Val Epoch 191: average Loss: 2769.087646\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 192: 0/23 Loss: 14.836101\n",
      "2025-03-07 23:05: Train Epoch 192: 20/23 Loss: 255.006561\n",
      "2025-03-07 23:05: **********Train Epoch 192: averaged Loss: 91.417215\n",
      "2025-03-07 23:05: **********Val Epoch 192: average Loss: 2762.795166\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 193: 0/23 Loss: 2.622630\n",
      "2025-03-07 23:05: Train Epoch 193: 20/23 Loss: 259.334625\n",
      "2025-03-07 23:05: **********Train Epoch 193: averaged Loss: 85.755702\n",
      "2025-03-07 23:05: **********Val Epoch 193: average Loss: 2758.552490\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 194: 0/23 Loss: 8.542108\n",
      "2025-03-07 23:05: Train Epoch 194: 20/23 Loss: 238.599335\n",
      "2025-03-07 23:05: **********Train Epoch 194: averaged Loss: 88.480522\n",
      "2025-03-07 23:05: **********Val Epoch 194: average Loss: 2726.071655\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 195: 0/23 Loss: 7.934213\n",
      "2025-03-07 23:05: Train Epoch 195: 20/23 Loss: 231.271774\n",
      "2025-03-07 23:05: **********Train Epoch 195: averaged Loss: 83.404459\n",
      "2025-03-07 23:05: **********Val Epoch 195: average Loss: 2714.203186\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 196: 0/23 Loss: 10.772555\n",
      "2025-03-07 23:05: Train Epoch 196: 20/23 Loss: 224.514954\n",
      "2025-03-07 23:05: **********Train Epoch 196: averaged Loss: 82.061461\n",
      "2025-03-07 23:05: **********Val Epoch 196: average Loss: 2730.037842\n",
      "2025-03-07 23:05: Train Epoch 197: 0/23 Loss: 2.355001\n",
      "2025-03-07 23:05: Train Epoch 197: 20/23 Loss: 195.137421\n",
      "2025-03-07 23:05: **********Train Epoch 197: averaged Loss: 75.349479\n",
      "2025-03-07 23:05: **********Val Epoch 197: average Loss: 2713.922791\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 198: 0/23 Loss: 13.866138\n",
      "2025-03-07 23:05: Train Epoch 198: 20/23 Loss: 356.249237\n",
      "2025-03-07 23:05: **********Train Epoch 198: averaged Loss: 101.202075\n",
      "2025-03-07 23:05: **********Val Epoch 198: average Loss: 2701.259033\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 199: 0/23 Loss: 1.017467\n",
      "2025-03-07 23:05: Train Epoch 199: 20/23 Loss: 190.634338\n",
      "2025-03-07 23:05: **********Train Epoch 199: averaged Loss: 91.595360\n",
      "2025-03-07 23:05: **********Val Epoch 199: average Loss: 2678.843750\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 200: 0/23 Loss: 6.590337\n",
      "2025-03-07 23:05: Train Epoch 200: 20/23 Loss: 234.170761\n",
      "2025-03-07 23:05: **********Train Epoch 200: averaged Loss: 75.938832\n",
      "2025-03-07 23:05: **********Val Epoch 200: average Loss: 2689.685364\n",
      "2025-03-07 23:05: Train Epoch 201: 0/23 Loss: 3.772797\n",
      "2025-03-07 23:05: Train Epoch 201: 20/23 Loss: 258.597229\n",
      "2025-03-07 23:05: **********Train Epoch 201: averaged Loss: 76.466908\n",
      "2025-03-07 23:05: **********Val Epoch 201: average Loss: 2669.565247\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 202: 0/23 Loss: 1.086620\n",
      "2025-03-07 23:05: Train Epoch 202: 20/23 Loss: 186.268539\n",
      "2025-03-07 23:05: **********Train Epoch 202: averaged Loss: 86.048053\n",
      "2025-03-07 23:05: **********Val Epoch 202: average Loss: 2667.612793\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 203: 0/23 Loss: 4.797523\n",
      "2025-03-07 23:05: Train Epoch 203: 20/23 Loss: 211.808624\n",
      "2025-03-07 23:05: **********Train Epoch 203: averaged Loss: 75.264262\n",
      "2025-03-07 23:05: **********Val Epoch 203: average Loss: 2660.103394\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 204: 0/23 Loss: 8.671735\n",
      "2025-03-07 23:05: Train Epoch 204: 20/23 Loss: 190.966858\n",
      "2025-03-07 23:05: **********Train Epoch 204: averaged Loss: 71.765516\n",
      "2025-03-07 23:05: **********Val Epoch 204: average Loss: 2632.935059\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 205: 0/23 Loss: 2.218359\n",
      "2025-03-07 23:05: Train Epoch 205: 20/23 Loss: 172.468567\n",
      "2025-03-07 23:05: **********Train Epoch 205: averaged Loss: 81.065796\n",
      "2025-03-07 23:05: **********Val Epoch 205: average Loss: 2638.577026\n",
      "2025-03-07 23:05: Train Epoch 206: 0/23 Loss: 5.835601\n",
      "2025-03-07 23:05: Train Epoch 206: 20/23 Loss: 156.001312\n",
      "2025-03-07 23:05: **********Train Epoch 206: averaged Loss: 69.837192\n",
      "2025-03-07 23:05: **********Val Epoch 206: average Loss: 2621.938477\n",
      "2025-03-07 23:05: *********************************Current best model saved!\n",
      "2025-03-07 23:05: Train Epoch 207: 0/23 Loss: 11.494572\n",
      "2025-03-07 23:06: Train Epoch 207: 20/23 Loss: 148.987488\n",
      "2025-03-07 23:06: **********Train Epoch 207: averaged Loss: 76.827472\n",
      "2025-03-07 23:06: **********Val Epoch 207: average Loss: 2616.235718\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 208: 0/23 Loss: 5.017625\n",
      "2025-03-07 23:06: Train Epoch 208: 20/23 Loss: 176.014648\n",
      "2025-03-07 23:06: **********Train Epoch 208: averaged Loss: 75.337778\n",
      "2025-03-07 23:06: **********Val Epoch 208: average Loss: 2625.680664\n",
      "2025-03-07 23:06: Train Epoch 209: 0/23 Loss: 15.706240\n",
      "2025-03-07 23:06: Train Epoch 209: 20/23 Loss: 134.158813\n",
      "2025-03-07 23:06: **********Train Epoch 209: averaged Loss: 102.720525\n",
      "2025-03-07 23:06: **********Val Epoch 209: average Loss: 2607.658630\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 210: 0/23 Loss: 7.780082\n",
      "2025-03-07 23:06: Train Epoch 210: 20/23 Loss: 241.352539\n",
      "2025-03-07 23:06: **********Train Epoch 210: averaged Loss: 70.856355\n",
      "2025-03-07 23:06: **********Val Epoch 210: average Loss: 2632.565674\n",
      "2025-03-07 23:06: Train Epoch 211: 0/23 Loss: 8.608693\n",
      "2025-03-07 23:06: Train Epoch 211: 20/23 Loss: 140.222961\n",
      "2025-03-07 23:06: **********Train Epoch 211: averaged Loss: 77.236520\n",
      "2025-03-07 23:06: **********Val Epoch 211: average Loss: 2590.684143\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 212: 0/23 Loss: 8.393686\n",
      "2025-03-07 23:06: Train Epoch 212: 20/23 Loss: 156.886185\n",
      "2025-03-07 23:06: **********Train Epoch 212: averaged Loss: 74.283819\n",
      "2025-03-07 23:06: **********Val Epoch 212: average Loss: 2588.408020\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 213: 0/23 Loss: 9.156825\n",
      "2025-03-07 23:06: Train Epoch 213: 20/23 Loss: 156.384644\n",
      "2025-03-07 23:06: **********Train Epoch 213: averaged Loss: 64.708274\n",
      "2025-03-07 23:06: **********Val Epoch 213: average Loss: 2570.693604\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 214: 0/23 Loss: 8.572195\n",
      "2025-03-07 23:06: Train Epoch 214: 20/23 Loss: 133.958740\n",
      "2025-03-07 23:06: **********Train Epoch 214: averaged Loss: 59.171137\n",
      "2025-03-07 23:06: **********Val Epoch 214: average Loss: 2571.042603\n",
      "2025-03-07 23:06: Train Epoch 215: 0/23 Loss: 1.636465\n",
      "2025-03-07 23:06: Train Epoch 215: 20/23 Loss: 128.332336\n",
      "2025-03-07 23:06: **********Train Epoch 215: averaged Loss: 57.319143\n",
      "2025-03-07 23:06: **********Val Epoch 215: average Loss: 2571.631165\n",
      "2025-03-07 23:06: Train Epoch 216: 0/23 Loss: 3.570946\n",
      "2025-03-07 23:06: Train Epoch 216: 20/23 Loss: 148.983780\n",
      "2025-03-07 23:06: **********Train Epoch 216: averaged Loss: 68.086305\n",
      "2025-03-07 23:06: **********Val Epoch 216: average Loss: 2543.733032\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 217: 0/23 Loss: 10.395468\n",
      "2025-03-07 23:06: Train Epoch 217: 20/23 Loss: 108.230995\n",
      "2025-03-07 23:06: **********Train Epoch 217: averaged Loss: 84.437123\n",
      "2025-03-07 23:06: **********Val Epoch 217: average Loss: 2540.447144\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 218: 0/23 Loss: 10.850665\n",
      "2025-03-07 23:06: Train Epoch 218: 20/23 Loss: 153.463608\n",
      "2025-03-07 23:06: **********Train Epoch 218: averaged Loss: 61.031004\n",
      "2025-03-07 23:06: **********Val Epoch 218: average Loss: 2545.701050\n",
      "2025-03-07 23:06: Train Epoch 219: 0/23 Loss: 9.209787\n",
      "2025-03-07 23:06: Train Epoch 219: 20/23 Loss: 135.989609\n",
      "2025-03-07 23:06: **********Train Epoch 219: averaged Loss: 57.754614\n",
      "2025-03-07 23:06: **********Val Epoch 219: average Loss: 2526.411438\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 220: 0/23 Loss: 3.888597\n",
      "2025-03-07 23:06: Train Epoch 220: 20/23 Loss: 124.487564\n",
      "2025-03-07 23:06: **********Train Epoch 220: averaged Loss: 55.792839\n",
      "2025-03-07 23:06: **********Val Epoch 220: average Loss: 2512.838745\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 221: 0/23 Loss: 10.620564\n",
      "2025-03-07 23:06: Train Epoch 221: 20/23 Loss: 107.240936\n",
      "2025-03-07 23:06: **********Train Epoch 221: averaged Loss: 59.655608\n",
      "2025-03-07 23:06: **********Val Epoch 221: average Loss: 2506.504639\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 222: 0/23 Loss: 5.937682\n",
      "2025-03-07 23:06: Train Epoch 222: 20/23 Loss: 155.445038\n",
      "2025-03-07 23:06: **********Train Epoch 222: averaged Loss: 69.472740\n",
      "2025-03-07 23:06: **********Val Epoch 222: average Loss: 2509.670898\n",
      "2025-03-07 23:06: Train Epoch 223: 0/23 Loss: 14.318200\n",
      "2025-03-07 23:06: Train Epoch 223: 20/23 Loss: 100.461891\n",
      "2025-03-07 23:06: **********Train Epoch 223: averaged Loss: 79.948045\n",
      "2025-03-07 23:06: **********Val Epoch 223: average Loss: 2518.024536\n",
      "2025-03-07 23:06: Train Epoch 224: 0/23 Loss: 8.420997\n",
      "2025-03-07 23:06: Train Epoch 224: 20/23 Loss: 88.949486\n",
      "2025-03-07 23:06: **********Train Epoch 224: averaged Loss: 56.997510\n",
      "2025-03-07 23:06: **********Val Epoch 224: average Loss: 2506.155029\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 225: 0/23 Loss: 7.632327\n",
      "2025-03-07 23:06: Train Epoch 225: 20/23 Loss: 99.185738\n",
      "2025-03-07 23:06: **********Train Epoch 225: averaged Loss: 53.168729\n",
      "2025-03-07 23:06: **********Val Epoch 225: average Loss: 2497.540771\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 226: 0/23 Loss: 3.484286\n",
      "2025-03-07 23:06: Train Epoch 226: 20/23 Loss: 73.654022\n",
      "2025-03-07 23:06: **********Train Epoch 226: averaged Loss: 55.076126\n",
      "2025-03-07 23:06: **********Val Epoch 226: average Loss: 2489.091797\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 227: 0/23 Loss: 2.054340\n",
      "2025-03-07 23:06: Train Epoch 227: 20/23 Loss: 123.807640\n",
      "2025-03-07 23:06: **********Train Epoch 227: averaged Loss: 56.507870\n",
      "2025-03-07 23:06: **********Val Epoch 227: average Loss: 2480.158508\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 228: 0/23 Loss: 14.247872\n",
      "2025-03-07 23:06: Train Epoch 228: 20/23 Loss: 94.410606\n",
      "2025-03-07 23:06: **********Train Epoch 228: averaged Loss: 67.286628\n",
      "2025-03-07 23:06: **********Val Epoch 228: average Loss: 2478.255066\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 229: 0/23 Loss: 3.767373\n",
      "2025-03-07 23:06: Train Epoch 229: 20/23 Loss: 69.268288\n",
      "2025-03-07 23:06: **********Train Epoch 229: averaged Loss: 51.260515\n",
      "2025-03-07 23:06: **********Val Epoch 229: average Loss: 2471.884155\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 230: 0/23 Loss: 10.688817\n",
      "2025-03-07 23:06: Train Epoch 230: 20/23 Loss: 83.028023\n",
      "2025-03-07 23:06: **********Train Epoch 230: averaged Loss: 52.666611\n",
      "2025-03-07 23:06: **********Val Epoch 230: average Loss: 2460.756348\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 231: 0/23 Loss: 1.109597\n",
      "2025-03-07 23:06: Train Epoch 231: 20/23 Loss: 78.420731\n",
      "2025-03-07 23:06: **********Train Epoch 231: averaged Loss: 57.872062\n",
      "2025-03-07 23:06: **********Val Epoch 231: average Loss: 2455.539246\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 232: 0/23 Loss: 4.738437\n",
      "2025-03-07 23:06: Train Epoch 232: 20/23 Loss: 86.617813\n",
      "2025-03-07 23:06: **********Train Epoch 232: averaged Loss: 54.823897\n",
      "2025-03-07 23:06: **********Val Epoch 232: average Loss: 2442.549500\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 233: 0/23 Loss: 10.364198\n",
      "2025-03-07 23:06: Train Epoch 233: 20/23 Loss: 89.628517\n",
      "2025-03-07 23:06: **********Train Epoch 233: averaged Loss: 61.414478\n",
      "2025-03-07 23:06: **********Val Epoch 233: average Loss: 2447.665894\n",
      "2025-03-07 23:06: Train Epoch 234: 0/23 Loss: 8.867023\n",
      "2025-03-07 23:06: Train Epoch 234: 20/23 Loss: 68.307610\n",
      "2025-03-07 23:06: **********Train Epoch 234: averaged Loss: 50.975494\n",
      "2025-03-07 23:06: **********Val Epoch 234: average Loss: 2439.361389\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 235: 0/23 Loss: 7.066408\n",
      "2025-03-07 23:06: Train Epoch 235: 20/23 Loss: 68.246109\n",
      "2025-03-07 23:06: **********Train Epoch 235: averaged Loss: 50.093891\n",
      "2025-03-07 23:06: **********Val Epoch 235: average Loss: 2426.393677\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 236: 0/23 Loss: 0.925010\n",
      "2025-03-07 23:06: Train Epoch 236: 20/23 Loss: 75.757202\n",
      "2025-03-07 23:06: **********Train Epoch 236: averaged Loss: 53.978184\n",
      "2025-03-07 23:06: **********Val Epoch 236: average Loss: 2417.224426\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 237: 0/23 Loss: 5.456813\n",
      "2025-03-07 23:06: Train Epoch 237: 20/23 Loss: 86.097305\n",
      "2025-03-07 23:06: **********Train Epoch 237: averaged Loss: 51.850752\n",
      "2025-03-07 23:06: **********Val Epoch 237: average Loss: 2435.527100\n",
      "2025-03-07 23:06: Train Epoch 238: 0/23 Loss: 6.730165\n",
      "2025-03-07 23:06: Train Epoch 238: 20/23 Loss: 106.313980\n",
      "2025-03-07 23:06: **********Train Epoch 238: averaged Loss: 61.797448\n",
      "2025-03-07 23:06: **********Val Epoch 238: average Loss: 2409.974243\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 239: 0/23 Loss: 8.071152\n",
      "2025-03-07 23:06: Train Epoch 239: 20/23 Loss: 71.831856\n",
      "2025-03-07 23:06: **********Train Epoch 239: averaged Loss: 49.087823\n",
      "2025-03-07 23:06: **********Val Epoch 239: average Loss: 2428.925903\n",
      "2025-03-07 23:06: Train Epoch 240: 0/23 Loss: 12.506246\n",
      "2025-03-07 23:06: Train Epoch 240: 20/23 Loss: 89.321938\n",
      "2025-03-07 23:06: **********Train Epoch 240: averaged Loss: 51.209166\n",
      "2025-03-07 23:06: **********Val Epoch 240: average Loss: 2418.531433\n",
      "2025-03-07 23:06: Train Epoch 241: 0/23 Loss: 3.993597\n",
      "2025-03-07 23:06: Train Epoch 241: 20/23 Loss: 80.467667\n",
      "2025-03-07 23:06: **********Train Epoch 241: averaged Loss: 54.742473\n",
      "2025-03-07 23:06: **********Val Epoch 241: average Loss: 2399.513367\n",
      "2025-03-07 23:06: *********************************Current best model saved!\n",
      "2025-03-07 23:06: Train Epoch 242: 0/23 Loss: 3.932776\n",
      "2025-03-07 23:06: Train Epoch 242: 20/23 Loss: 74.631432\n",
      "2025-03-07 23:06: **********Train Epoch 242: averaged Loss: 47.223574\n",
      "2025-03-07 23:06: **********Val Epoch 242: average Loss: 2408.409058\n",
      "2025-03-07 23:06: Train Epoch 243: 0/23 Loss: 12.008454\n",
      "2025-03-07 23:07: Train Epoch 243: 20/23 Loss: 72.864716\n",
      "2025-03-07 23:07: **********Train Epoch 243: averaged Loss: 48.712785\n",
      "2025-03-07 23:07: **********Val Epoch 243: average Loss: 2414.186401\n",
      "2025-03-07 23:07: Train Epoch 244: 0/23 Loss: 9.311451\n",
      "2025-03-07 23:07: Train Epoch 244: 20/23 Loss: 80.159317\n",
      "2025-03-07 23:07: **********Train Epoch 244: averaged Loss: 50.750481\n",
      "2025-03-07 23:07: **********Val Epoch 244: average Loss: 2407.325012\n",
      "2025-03-07 23:07: Train Epoch 245: 0/23 Loss: 11.818789\n",
      "2025-03-07 23:07: Train Epoch 245: 20/23 Loss: 105.652969\n",
      "2025-03-07 23:07: **********Train Epoch 245: averaged Loss: 62.790684\n",
      "2025-03-07 23:07: **********Val Epoch 245: average Loss: 2424.466431\n",
      "2025-03-07 23:07: Train Epoch 246: 0/23 Loss: 6.342230\n",
      "2025-03-07 23:07: Train Epoch 246: 20/23 Loss: 69.653107\n",
      "2025-03-07 23:07: **********Train Epoch 246: averaged Loss: 47.964337\n",
      "2025-03-07 23:07: **********Val Epoch 246: average Loss: 2370.507446\n",
      "2025-03-07 23:07: *********************************Current best model saved!\n",
      "2025-03-07 23:07: Train Epoch 247: 0/23 Loss: 12.662598\n",
      "2025-03-07 23:07: Train Epoch 247: 20/23 Loss: 73.380539\n",
      "2025-03-07 23:07: **********Train Epoch 247: averaged Loss: 47.519717\n",
      "2025-03-07 23:07: **********Val Epoch 247: average Loss: 2378.564514\n",
      "2025-03-07 23:07: Train Epoch 248: 0/23 Loss: 1.468708\n",
      "2025-03-07 23:07: Train Epoch 248: 20/23 Loss: 67.534325\n",
      "2025-03-07 23:07: **********Train Epoch 248: averaged Loss: 44.268242\n",
      "2025-03-07 23:07: **********Val Epoch 248: average Loss: 2391.012878\n",
      "2025-03-07 23:07: Train Epoch 249: 0/23 Loss: 6.382815\n",
      "2025-03-07 23:07: Train Epoch 249: 20/23 Loss: 95.785339\n",
      "2025-03-07 23:07: **********Train Epoch 249: averaged Loss: 49.189913\n",
      "2025-03-07 23:07: **********Val Epoch 249: average Loss: 2374.174805\n",
      "2025-03-07 23:07: Train Epoch 250: 0/23 Loss: 2.713687\n",
      "2025-03-07 23:07: Train Epoch 250: 20/23 Loss: 103.454514\n",
      "2025-03-07 23:07: **********Train Epoch 250: averaged Loss: 56.813625\n",
      "2025-03-07 23:07: **********Val Epoch 250: average Loss: 2375.287598\n",
      "2025-03-07 23:07: Train Epoch 251: 0/23 Loss: 5.148107\n",
      "2025-03-07 23:07: Train Epoch 251: 20/23 Loss: 76.090942\n",
      "2025-03-07 23:07: **********Train Epoch 251: averaged Loss: 45.236379\n",
      "2025-03-07 23:07: **********Val Epoch 251: average Loss: 2373.660156\n",
      "2025-03-07 23:07: Train Epoch 252: 0/23 Loss: 13.983940\n",
      "2025-03-07 23:07: Train Epoch 252: 20/23 Loss: 67.963051\n",
      "2025-03-07 23:07: **********Train Epoch 252: averaged Loss: 46.338575\n",
      "2025-03-07 23:07: **********Val Epoch 252: average Loss: 2374.790039\n",
      "2025-03-07 23:07: Train Epoch 253: 0/23 Loss: 2.578210\n",
      "2025-03-07 23:07: Train Epoch 253: 20/23 Loss: 79.902191\n",
      "2025-03-07 23:07: **********Train Epoch 253: averaged Loss: 43.676090\n",
      "2025-03-07 23:07: **********Val Epoch 253: average Loss: 2362.296021\n",
      "2025-03-07 23:07: *********************************Current best model saved!\n",
      "2025-03-07 23:07: Train Epoch 254: 0/23 Loss: 9.686630\n",
      "2025-03-07 23:07: Train Epoch 254: 20/23 Loss: 99.652832\n",
      "2025-03-07 23:07: **********Train Epoch 254: averaged Loss: 45.490719\n",
      "2025-03-07 23:07: **********Val Epoch 254: average Loss: 2395.623413\n",
      "2025-03-07 23:07: Train Epoch 255: 0/23 Loss: 3.161812\n",
      "2025-03-07 23:07: Train Epoch 255: 20/23 Loss: 135.251450\n",
      "2025-03-07 23:07: **********Train Epoch 255: averaged Loss: 54.413593\n",
      "2025-03-07 23:07: **********Val Epoch 255: average Loss: 2341.058960\n",
      "2025-03-07 23:07: *********************************Current best model saved!\n",
      "2025-03-07 23:07: Train Epoch 256: 0/23 Loss: 9.007286\n",
      "2025-03-07 23:07: Train Epoch 256: 20/23 Loss: 136.574402\n",
      "2025-03-07 23:07: **********Train Epoch 256: averaged Loss: 65.175255\n",
      "2025-03-07 23:07: **********Val Epoch 256: average Loss: 2376.442139\n",
      "2025-03-07 23:07: Train Epoch 257: 0/23 Loss: 5.526713\n",
      "2025-03-07 23:07: Train Epoch 257: 20/23 Loss: 87.198486\n",
      "2025-03-07 23:07: **********Train Epoch 257: averaged Loss: 43.882274\n",
      "2025-03-07 23:07: **********Val Epoch 257: average Loss: 2379.421997\n",
      "2025-03-07 23:07: Train Epoch 258: 0/23 Loss: 4.556263\n",
      "2025-03-07 23:07: Train Epoch 258: 20/23 Loss: 91.644348\n",
      "2025-03-07 23:07: **********Train Epoch 258: averaged Loss: 44.495477\n",
      "2025-03-07 23:07: **********Val Epoch 258: average Loss: 2349.844604\n",
      "2025-03-07 23:07: Train Epoch 259: 0/23 Loss: 1.861355\n",
      "2025-03-07 23:07: Train Epoch 259: 20/23 Loss: 68.323601\n",
      "2025-03-07 23:07: **********Train Epoch 259: averaged Loss: 45.860378\n",
      "2025-03-07 23:07: **********Val Epoch 259: average Loss: 2373.053589\n",
      "2025-03-07 23:07: Train Epoch 260: 0/23 Loss: 1.277756\n",
      "2025-03-07 23:07: Train Epoch 260: 20/23 Loss: 88.160706\n",
      "2025-03-07 23:07: **********Train Epoch 260: averaged Loss: 47.919333\n",
      "2025-03-07 23:07: **********Val Epoch 260: average Loss: 2390.027344\n",
      "2025-03-07 23:07: Train Epoch 261: 0/23 Loss: 9.492311\n",
      "2025-03-07 23:07: Train Epoch 261: 20/23 Loss: 134.568222\n",
      "2025-03-07 23:07: **********Train Epoch 261: averaged Loss: 57.092955\n",
      "2025-03-07 23:07: **********Val Epoch 261: average Loss: 2334.233765\n",
      "2025-03-07 23:07: *********************************Current best model saved!\n",
      "2025-03-07 23:07: Train Epoch 262: 0/23 Loss: 3.203402\n",
      "2025-03-07 23:07: Train Epoch 262: 20/23 Loss: 86.530090\n",
      "2025-03-07 23:07: **********Train Epoch 262: averaged Loss: 43.713379\n",
      "2025-03-07 23:07: **********Val Epoch 262: average Loss: 2370.662231\n",
      "2025-03-07 23:07: Train Epoch 263: 0/23 Loss: 11.905649\n",
      "2025-03-07 23:07: Train Epoch 263: 20/23 Loss: 100.157913\n",
      "2025-03-07 23:07: **********Train Epoch 263: averaged Loss: 47.158220\n",
      "2025-03-07 23:07: **********Val Epoch 263: average Loss: 2362.428406\n",
      "2025-03-07 23:07: Train Epoch 264: 0/23 Loss: 3.566041\n",
      "2025-03-07 23:07: Train Epoch 264: 20/23 Loss: 93.277527\n",
      "2025-03-07 23:07: **********Train Epoch 264: averaged Loss: 43.370340\n",
      "2025-03-07 23:07: **********Val Epoch 264: average Loss: 2349.368286\n",
      "2025-03-07 23:07: Train Epoch 265: 0/23 Loss: 8.815482\n",
      "2025-03-07 23:07: Train Epoch 265: 20/23 Loss: 67.596680\n",
      "2025-03-07 23:07: **********Train Epoch 265: averaged Loss: 48.278981\n",
      "2025-03-07 23:07: **********Val Epoch 265: average Loss: 2333.954224\n",
      "2025-03-07 23:07: *********************************Current best model saved!\n",
      "2025-03-07 23:07: Train Epoch 266: 0/23 Loss: 11.169041\n",
      "2025-03-07 23:07: Train Epoch 266: 20/23 Loss: 77.369827\n",
      "2025-03-07 23:07: **********Train Epoch 266: averaged Loss: 43.796991\n",
      "2025-03-07 23:07: **********Val Epoch 266: average Loss: 2348.169067\n",
      "2025-03-07 23:07: Train Epoch 267: 0/23 Loss: 10.170657\n",
      "2025-03-07 23:07: Train Epoch 267: 20/23 Loss: 74.101601\n",
      "2025-03-07 23:07: **********Train Epoch 267: averaged Loss: 45.934859\n",
      "2025-03-07 23:07: **********Val Epoch 267: average Loss: 2347.153137\n",
      "2025-03-07 23:07: Train Epoch 268: 0/23 Loss: 9.963268\n",
      "2025-03-07 23:07: Train Epoch 268: 20/23 Loss: 113.791687\n",
      "2025-03-07 23:07: **********Train Epoch 268: averaged Loss: 46.798676\n",
      "2025-03-07 23:07: **********Val Epoch 268: average Loss: 2344.826538\n",
      "2025-03-07 23:07: Train Epoch 269: 0/23 Loss: 12.819508\n",
      "2025-03-07 23:07: Train Epoch 269: 20/23 Loss: 74.017471\n",
      "2025-03-07 23:07: **********Train Epoch 269: averaged Loss: 45.947942\n",
      "2025-03-07 23:07: **********Val Epoch 269: average Loss: 2357.472717\n",
      "2025-03-07 23:07: Train Epoch 270: 0/23 Loss: 7.804536\n",
      "2025-03-07 23:07: Train Epoch 270: 20/23 Loss: 89.269775\n",
      "2025-03-07 23:07: **********Train Epoch 270: averaged Loss: 50.844795\n",
      "2025-03-07 23:07: **********Val Epoch 270: average Loss: 2308.575073\n",
      "2025-03-07 23:07: *********************************Current best model saved!\n",
      "2025-03-07 23:07: Train Epoch 271: 0/23 Loss: 8.552797\n",
      "2025-03-07 23:07: Train Epoch 271: 20/23 Loss: 148.794754\n",
      "2025-03-07 23:07: **********Train Epoch 271: averaged Loss: 55.817725\n",
      "2025-03-07 23:07: **********Val Epoch 271: average Loss: 2325.073120\n",
      "2025-03-07 23:07: Train Epoch 272: 0/23 Loss: 7.998118\n",
      "2025-03-07 23:07: Train Epoch 272: 20/23 Loss: 87.009857\n",
      "2025-03-07 23:07: **********Train Epoch 272: averaged Loss: 44.955218\n",
      "2025-03-07 23:07: **********Val Epoch 272: average Loss: 2362.300110\n",
      "2025-03-07 23:07: Train Epoch 273: 0/23 Loss: 8.564878\n",
      "2025-03-07 23:07: Train Epoch 273: 20/23 Loss: 86.282303\n",
      "2025-03-07 23:07: **********Train Epoch 273: averaged Loss: 43.436691\n",
      "2025-03-07 23:07: **********Val Epoch 273: average Loss: 2331.652954\n",
      "2025-03-07 23:07: Train Epoch 274: 0/23 Loss: 6.724056\n",
      "2025-03-07 23:07: Train Epoch 274: 20/23 Loss: 85.392151\n",
      "2025-03-07 23:07: **********Train Epoch 274: averaged Loss: 41.457607\n",
      "2025-03-07 23:07: **********Val Epoch 274: average Loss: 2333.494080\n",
      "2025-03-07 23:07: Train Epoch 275: 0/23 Loss: 15.274145\n",
      "2025-03-07 23:07: Train Epoch 275: 20/23 Loss: 90.284821\n",
      "2025-03-07 23:07: **********Train Epoch 275: averaged Loss: 46.015739\n",
      "2025-03-07 23:07: **********Val Epoch 275: average Loss: 2338.259644\n",
      "2025-03-07 23:07: Train Epoch 276: 0/23 Loss: 8.166018\n",
      "2025-03-07 23:07: Train Epoch 276: 20/23 Loss: 99.959503\n",
      "2025-03-07 23:07: **********Train Epoch 276: averaged Loss: 46.573744\n",
      "2025-03-07 23:07: **********Val Epoch 276: average Loss: 2350.401489\n",
      "2025-03-07 23:07: Train Epoch 277: 0/23 Loss: 9.092148\n",
      "2025-03-07 23:07: Train Epoch 277: 20/23 Loss: 118.023628\n",
      "2025-03-07 23:07: **********Train Epoch 277: averaged Loss: 51.677855\n",
      "2025-03-07 23:07: **********Val Epoch 277: average Loss: 2323.848389\n",
      "2025-03-07 23:07: Train Epoch 278: 0/23 Loss: 7.330420\n",
      "2025-03-07 23:07: Train Epoch 278: 20/23 Loss: 88.466896\n",
      "2025-03-07 23:07: **********Train Epoch 278: averaged Loss: 46.005706\n",
      "2025-03-07 23:07: **********Val Epoch 278: average Loss: 2350.888184\n",
      "2025-03-07 23:07: Train Epoch 279: 0/23 Loss: 10.918694\n",
      "2025-03-07 23:08: Train Epoch 279: 20/23 Loss: 73.157669\n",
      "2025-03-07 23:08: **********Train Epoch 279: averaged Loss: 47.466934\n",
      "2025-03-07 23:08: **********Val Epoch 279: average Loss: 2323.296997\n",
      "2025-03-07 23:08: Train Epoch 280: 0/23 Loss: 7.487967\n",
      "2025-03-07 23:08: Train Epoch 280: 20/23 Loss: 79.256149\n",
      "2025-03-07 23:08: **********Train Epoch 280: averaged Loss: 43.583896\n",
      "2025-03-07 23:08: **********Val Epoch 280: average Loss: 2342.005859\n",
      "2025-03-07 23:08: Train Epoch 281: 0/23 Loss: 9.461782\n",
      "2025-03-07 23:08: Train Epoch 281: 20/23 Loss: 97.560257\n",
      "2025-03-07 23:08: **********Train Epoch 281: averaged Loss: 45.768364\n",
      "2025-03-07 23:08: **********Val Epoch 281: average Loss: 2328.381836\n",
      "2025-03-07 23:08: Train Epoch 282: 0/23 Loss: 2.633507\n",
      "2025-03-07 23:08: Train Epoch 282: 20/23 Loss: 90.305550\n",
      "2025-03-07 23:08: **********Train Epoch 282: averaged Loss: 42.415884\n",
      "2025-03-07 23:08: **********Val Epoch 282: average Loss: 2310.036682\n",
      "2025-03-07 23:08: Train Epoch 283: 0/23 Loss: 10.374409\n",
      "2025-03-07 23:08: Train Epoch 283: 20/23 Loss: 78.504547\n",
      "2025-03-07 23:08: **********Train Epoch 283: averaged Loss: 44.104495\n",
      "2025-03-07 23:08: **********Val Epoch 283: average Loss: 2332.296875\n",
      "2025-03-07 23:08: Train Epoch 284: 0/23 Loss: 2.896582\n",
      "2025-03-07 23:08: Train Epoch 284: 20/23 Loss: 79.744720\n",
      "2025-03-07 23:08: **********Train Epoch 284: averaged Loss: 41.219988\n",
      "2025-03-07 23:08: **********Val Epoch 284: average Loss: 2320.230225\n",
      "2025-03-07 23:08: Train Epoch 285: 0/23 Loss: 14.200064\n",
      "2025-03-07 23:08: Train Epoch 285: 20/23 Loss: 77.927055\n",
      "2025-03-07 23:08: **********Train Epoch 285: averaged Loss: 38.734391\n",
      "2025-03-07 23:08: **********Val Epoch 285: average Loss: 2319.328796\n",
      "2025-03-07 23:08: Train Epoch 286: 0/23 Loss: 10.740663\n",
      "2025-03-07 23:08: Train Epoch 286: 20/23 Loss: 88.430023\n",
      "2025-03-07 23:08: **********Train Epoch 286: averaged Loss: 41.394309\n",
      "2025-03-07 23:08: **********Val Epoch 286: average Loss: 2301.558899\n",
      "2025-03-07 23:08: *********************************Current best model saved!\n",
      "2025-03-07 23:08: Train Epoch 287: 0/23 Loss: 6.524951\n",
      "2025-03-07 23:08: Train Epoch 287: 20/23 Loss: 96.473419\n",
      "2025-03-07 23:08: **********Train Epoch 287: averaged Loss: 45.571530\n",
      "2025-03-07 23:08: **********Val Epoch 287: average Loss: 2328.517822\n",
      "2025-03-07 23:08: Train Epoch 288: 0/23 Loss: 7.458995\n",
      "2025-03-07 23:08: Train Epoch 288: 20/23 Loss: 83.144844\n",
      "2025-03-07 23:08: **********Train Epoch 288: averaged Loss: 45.928789\n",
      "2025-03-07 23:08: **********Val Epoch 288: average Loss: 2325.690796\n",
      "2025-03-07 23:08: Train Epoch 289: 0/23 Loss: 12.065892\n",
      "2025-03-07 23:08: Train Epoch 289: 20/23 Loss: 134.503098\n",
      "2025-03-07 23:08: **********Train Epoch 289: averaged Loss: 55.745349\n",
      "2025-03-07 23:08: **********Val Epoch 289: average Loss: 2333.390381\n",
      "2025-03-07 23:08: Train Epoch 290: 0/23 Loss: 6.122107\n",
      "2025-03-07 23:08: Train Epoch 290: 20/23 Loss: 91.013237\n",
      "2025-03-07 23:08: **********Train Epoch 290: averaged Loss: 43.078761\n",
      "2025-03-07 23:08: **********Val Epoch 290: average Loss: 2313.126221\n",
      "2025-03-07 23:08: Train Epoch 291: 0/23 Loss: 7.433628\n",
      "2025-03-07 23:08: Train Epoch 291: 20/23 Loss: 106.589203\n",
      "2025-03-07 23:08: **********Train Epoch 291: averaged Loss: 43.264322\n",
      "2025-03-07 23:08: **********Val Epoch 291: average Loss: 2315.423706\n",
      "2025-03-07 23:08: Train Epoch 292: 0/23 Loss: 4.330084\n",
      "2025-03-07 23:08: Train Epoch 292: 20/23 Loss: 76.160919\n",
      "2025-03-07 23:08: **********Train Epoch 292: averaged Loss: 39.120825\n",
      "2025-03-07 23:08: **********Val Epoch 292: average Loss: 2328.356689\n",
      "2025-03-07 23:08: Train Epoch 293: 0/23 Loss: 15.397354\n",
      "2025-03-07 23:08: Train Epoch 293: 20/23 Loss: 125.474464\n",
      "2025-03-07 23:08: **********Train Epoch 293: averaged Loss: 47.623157\n",
      "2025-03-07 23:08: **********Val Epoch 293: average Loss: 2325.496094\n",
      "2025-03-07 23:08: Train Epoch 294: 0/23 Loss: 1.132476\n",
      "2025-03-07 23:08: Train Epoch 294: 20/23 Loss: 79.509766\n",
      "2025-03-07 23:08: **********Train Epoch 294: averaged Loss: 40.132259\n",
      "2025-03-07 23:08: **********Val Epoch 294: average Loss: 2353.923950\n",
      "2025-03-07 23:08: Train Epoch 295: 0/23 Loss: 15.075011\n",
      "2025-03-07 23:08: Train Epoch 295: 20/23 Loss: 76.856110\n",
      "2025-03-07 23:08: **********Train Epoch 295: averaged Loss: 43.785494\n",
      "2025-03-07 23:08: **********Val Epoch 295: average Loss: 2337.689819\n",
      "2025-03-07 23:08: Train Epoch 296: 0/23 Loss: 7.548937\n",
      "2025-03-07 23:08: Train Epoch 296: 20/23 Loss: 70.906563\n",
      "2025-03-07 23:08: **********Train Epoch 296: averaged Loss: 41.076784\n",
      "2025-03-07 23:08: **********Val Epoch 296: average Loss: 2314.089661\n",
      "2025-03-07 23:08: Train Epoch 297: 0/23 Loss: 8.795225\n",
      "2025-03-07 23:08: Train Epoch 297: 20/23 Loss: 75.732620\n",
      "2025-03-07 23:08: **********Train Epoch 297: averaged Loss: 40.694010\n",
      "2025-03-07 23:08: **********Val Epoch 297: average Loss: 2313.805847\n",
      "2025-03-07 23:08: Train Epoch 298: 0/23 Loss: 4.640825\n",
      "2025-03-07 23:08: Train Epoch 298: 20/23 Loss: 69.427284\n",
      "2025-03-07 23:08: **********Train Epoch 298: averaged Loss: 39.068086\n",
      "2025-03-07 23:08: **********Val Epoch 298: average Loss: 2325.713989\n",
      "2025-03-07 23:08: Train Epoch 299: 0/23 Loss: 10.707060\n",
      "2025-03-07 23:08: Train Epoch 299: 20/23 Loss: 84.066002\n",
      "2025-03-07 23:08: **********Train Epoch 299: averaged Loss: 42.438089\n",
      "2025-03-07 23:08: **********Val Epoch 299: average Loss: 2323.241699\n",
      "2025-03-07 23:08: Train Epoch 300: 0/23 Loss: 0.896272\n",
      "2025-03-07 23:08: Train Epoch 300: 20/23 Loss: 71.827164\n",
      "2025-03-07 23:08: **********Train Epoch 300: averaged Loss: 41.373204\n",
      "2025-03-07 23:08: **********Val Epoch 300: average Loss: 2302.066162\n",
      "2025-03-07 23:08: Train Epoch 301: 0/23 Loss: 6.638651\n",
      "2025-03-07 23:08: Train Epoch 301: 20/23 Loss: 101.611435\n",
      "2025-03-07 23:08: **********Train Epoch 301: averaged Loss: 43.021074\n",
      "2025-03-07 23:08: **********Val Epoch 301: average Loss: 2290.165466\n",
      "2025-03-07 23:08: *********************************Current best model saved!\n",
      "2025-03-07 23:08: Train Epoch 302: 0/23 Loss: 6.079922\n",
      "2025-03-07 23:08: Train Epoch 302: 20/23 Loss: 65.525307\n",
      "2025-03-07 23:08: **********Train Epoch 302: averaged Loss: 42.524800\n",
      "2025-03-07 23:08: **********Val Epoch 302: average Loss: 2322.298096\n",
      "2025-03-07 23:08: Train Epoch 303: 0/23 Loss: 11.976311\n",
      "2025-03-07 23:08: Train Epoch 303: 20/23 Loss: 104.763199\n",
      "2025-03-07 23:08: **********Train Epoch 303: averaged Loss: 46.389712\n",
      "2025-03-07 23:08: **********Val Epoch 303: average Loss: 2334.354004\n",
      "2025-03-07 23:08: Train Epoch 304: 0/23 Loss: 2.584628\n",
      "2025-03-07 23:08: Train Epoch 304: 20/23 Loss: 64.927399\n",
      "2025-03-07 23:08: **********Train Epoch 304: averaged Loss: 41.955803\n",
      "2025-03-07 23:08: **********Val Epoch 304: average Loss: 2285.405762\n",
      "2025-03-07 23:08: *********************************Current best model saved!\n",
      "2025-03-07 23:08: Train Epoch 305: 0/23 Loss: 12.272543\n",
      "2025-03-07 23:08: Train Epoch 305: 20/23 Loss: 95.464279\n",
      "2025-03-07 23:08: **********Train Epoch 305: averaged Loss: 50.262960\n",
      "2025-03-07 23:08: **********Val Epoch 305: average Loss: 2325.123779\n",
      "2025-03-07 23:08: Train Epoch 306: 0/23 Loss: 3.929646\n",
      "2025-03-07 23:08: Train Epoch 306: 20/23 Loss: 71.409073\n",
      "2025-03-07 23:08: **********Train Epoch 306: averaged Loss: 40.950508\n",
      "2025-03-07 23:08: **********Val Epoch 306: average Loss: 2300.145447\n",
      "2025-03-07 23:08: Train Epoch 307: 0/23 Loss: 13.056753\n",
      "2025-03-07 23:08: Train Epoch 307: 20/23 Loss: 97.434685\n",
      "2025-03-07 23:08: **********Train Epoch 307: averaged Loss: 45.258264\n",
      "2025-03-07 23:08: **********Val Epoch 307: average Loss: 2337.630615\n",
      "2025-03-07 23:08: Train Epoch 308: 0/23 Loss: 6.924340\n",
      "2025-03-07 23:08: Train Epoch 308: 20/23 Loss: 75.416855\n",
      "2025-03-07 23:08: **********Train Epoch 308: averaged Loss: 39.956511\n",
      "2025-03-07 23:08: **********Val Epoch 308: average Loss: 2305.925659\n",
      "2025-03-07 23:08: Train Epoch 309: 0/23 Loss: 13.422550\n",
      "2025-03-07 23:08: Train Epoch 309: 20/23 Loss: 62.968834\n",
      "2025-03-07 23:08: **********Train Epoch 309: averaged Loss: 43.372995\n",
      "2025-03-07 23:08: **********Val Epoch 309: average Loss: 2300.401611\n",
      "2025-03-07 23:08: Train Epoch 310: 0/23 Loss: 4.349728\n",
      "2025-03-07 23:08: Train Epoch 310: 20/23 Loss: 66.546692\n",
      "2025-03-07 23:08: **********Train Epoch 310: averaged Loss: 36.325654\n",
      "2025-03-07 23:08: **********Val Epoch 310: average Loss: 2299.223572\n",
      "2025-03-07 23:08: Train Epoch 311: 0/23 Loss: 15.114643\n",
      "2025-03-07 23:08: Train Epoch 311: 20/23 Loss: 102.921997\n",
      "2025-03-07 23:08: **********Train Epoch 311: averaged Loss: 42.494346\n",
      "2025-03-07 23:08: **********Val Epoch 311: average Loss: 2305.137268\n",
      "2025-03-07 23:08: Train Epoch 312: 0/23 Loss: 7.974814\n",
      "2025-03-07 23:08: Train Epoch 312: 20/23 Loss: 63.454731\n",
      "2025-03-07 23:08: **********Train Epoch 312: averaged Loss: 38.904969\n",
      "2025-03-07 23:08: **********Val Epoch 312: average Loss: 2276.195068\n",
      "2025-03-07 23:08: *********************************Current best model saved!\n",
      "2025-03-07 23:08: Train Epoch 313: 0/23 Loss: 13.720033\n",
      "2025-03-07 23:08: Train Epoch 313: 20/23 Loss: 61.871033\n",
      "2025-03-07 23:08: **********Train Epoch 313: averaged Loss: 41.930665\n",
      "2025-03-07 23:08: **********Val Epoch 313: average Loss: 2283.102173\n",
      "2025-03-07 23:08: Train Epoch 314: 0/23 Loss: 3.459012\n",
      "2025-03-07 23:09: Train Epoch 314: 20/23 Loss: 60.503883\n",
      "2025-03-07 23:09: **********Train Epoch 314: averaged Loss: 38.106913\n",
      "2025-03-07 23:09: **********Val Epoch 314: average Loss: 2308.914307\n",
      "2025-03-07 23:09: Train Epoch 315: 0/23 Loss: 8.844280\n",
      "2025-03-07 23:09: Train Epoch 315: 20/23 Loss: 75.861427\n",
      "2025-03-07 23:09: **********Train Epoch 315: averaged Loss: 38.886638\n",
      "2025-03-07 23:09: **********Val Epoch 315: average Loss: 2306.283325\n",
      "2025-03-07 23:09: Train Epoch 316: 0/23 Loss: 4.255021\n",
      "2025-03-07 23:09: Train Epoch 316: 20/23 Loss: 77.728867\n",
      "2025-03-07 23:09: **********Train Epoch 316: averaged Loss: 38.730525\n",
      "2025-03-07 23:09: **********Val Epoch 316: average Loss: 2301.112671\n",
      "2025-03-07 23:09: Train Epoch 317: 0/23 Loss: 11.636490\n",
      "2025-03-07 23:09: Train Epoch 317: 20/23 Loss: 160.454712\n",
      "2025-03-07 23:09: **********Train Epoch 317: averaged Loss: 49.621788\n",
      "2025-03-07 23:09: **********Val Epoch 317: average Loss: 2309.073364\n",
      "2025-03-07 23:09: Train Epoch 318: 0/23 Loss: 1.375530\n",
      "2025-03-07 23:09: Train Epoch 318: 20/23 Loss: 68.756943\n",
      "2025-03-07 23:09: **********Train Epoch 318: averaged Loss: 36.936853\n",
      "2025-03-07 23:09: **********Val Epoch 318: average Loss: 2275.563721\n",
      "2025-03-07 23:09: *********************************Current best model saved!\n",
      "2025-03-07 23:09: Train Epoch 319: 0/23 Loss: 11.064873\n",
      "2025-03-07 23:09: Train Epoch 319: 20/23 Loss: 52.951801\n",
      "2025-03-07 23:09: **********Train Epoch 319: averaged Loss: 36.481018\n",
      "2025-03-07 23:09: **********Val Epoch 319: average Loss: 2282.291260\n",
      "2025-03-07 23:09: Train Epoch 320: 0/23 Loss: 0.915204\n",
      "2025-03-07 23:09: Train Epoch 320: 20/23 Loss: 59.763237\n",
      "2025-03-07 23:09: **********Train Epoch 320: averaged Loss: 36.255793\n",
      "2025-03-07 23:09: **********Val Epoch 320: average Loss: 2331.018677\n",
      "2025-03-07 23:09: Train Epoch 321: 0/23 Loss: 8.094698\n",
      "2025-03-07 23:09: Train Epoch 321: 20/23 Loss: 52.168579\n",
      "2025-03-07 23:09: **********Train Epoch 321: averaged Loss: 35.330332\n",
      "2025-03-07 23:09: **********Val Epoch 321: average Loss: 2325.227722\n",
      "2025-03-07 23:09: Train Epoch 322: 0/23 Loss: 2.850464\n",
      "2025-03-07 23:09: Train Epoch 322: 20/23 Loss: 52.136162\n",
      "2025-03-07 23:09: **********Train Epoch 322: averaged Loss: 36.080662\n",
      "2025-03-07 23:09: **********Val Epoch 322: average Loss: 2329.844727\n",
      "2025-03-07 23:09: Train Epoch 323: 0/23 Loss: 2.891791\n",
      "2025-03-07 23:09: Train Epoch 323: 20/23 Loss: 49.210560\n",
      "2025-03-07 23:09: **********Train Epoch 323: averaged Loss: 34.797170\n",
      "2025-03-07 23:09: **********Val Epoch 323: average Loss: 2313.771973\n",
      "2025-03-07 23:09: Train Epoch 324: 0/23 Loss: 0.776363\n",
      "2025-03-07 23:09: Train Epoch 324: 20/23 Loss: 51.470398\n",
      "2025-03-07 23:09: **********Train Epoch 324: averaged Loss: 36.101409\n",
      "2025-03-07 23:09: **********Val Epoch 324: average Loss: 2280.472900\n",
      "2025-03-07 23:09: Train Epoch 325: 0/23 Loss: 0.863903\n",
      "2025-03-07 23:09: Train Epoch 325: 20/23 Loss: 86.991905\n",
      "2025-03-07 23:09: **********Train Epoch 325: averaged Loss: 42.606498\n",
      "2025-03-07 23:09: **********Val Epoch 325: average Loss: 2268.215332\n",
      "2025-03-07 23:09: *********************************Current best model saved!\n",
      "2025-03-07 23:09: Train Epoch 326: 0/23 Loss: 5.279908\n",
      "2025-03-07 23:09: Train Epoch 326: 20/23 Loss: 57.320351\n",
      "2025-03-07 23:09: **********Train Epoch 326: averaged Loss: 39.356891\n",
      "2025-03-07 23:09: **********Val Epoch 326: average Loss: 2293.635376\n",
      "2025-03-07 23:09: Train Epoch 327: 0/23 Loss: 18.510262\n",
      "2025-03-07 23:09: Train Epoch 327: 20/23 Loss: 157.289612\n",
      "2025-03-07 23:09: **********Train Epoch 327: averaged Loss: 47.238125\n",
      "2025-03-07 23:09: **********Val Epoch 327: average Loss: 2348.083862\n",
      "2025-03-07 23:09: Train Epoch 328: 0/23 Loss: 7.910317\n",
      "2025-03-07 23:09: Train Epoch 328: 20/23 Loss: 78.274170\n",
      "2025-03-07 23:09: **********Train Epoch 328: averaged Loss: 41.851333\n",
      "2025-03-07 23:09: **********Val Epoch 328: average Loss: 2315.605164\n",
      "2025-03-07 23:09: Train Epoch 329: 0/23 Loss: 11.439817\n",
      "2025-03-07 23:09: Train Epoch 329: 20/23 Loss: 75.260406\n",
      "2025-03-07 23:09: **********Train Epoch 329: averaged Loss: 50.822431\n",
      "2025-03-07 23:09: **********Val Epoch 329: average Loss: 2254.786133\n",
      "2025-03-07 23:09: *********************************Current best model saved!\n",
      "2025-03-07 23:09: Train Epoch 330: 0/23 Loss: 5.263185\n",
      "2025-03-07 23:09: Train Epoch 330: 20/23 Loss: 112.016190\n",
      "2025-03-07 23:09: **********Train Epoch 330: averaged Loss: 44.388989\n",
      "2025-03-07 23:09: **********Val Epoch 330: average Loss: 2271.001587\n",
      "2025-03-07 23:09: Train Epoch 331: 0/23 Loss: 16.878143\n",
      "2025-03-07 23:09: Train Epoch 331: 20/23 Loss: 118.320084\n",
      "2025-03-07 23:09: **********Train Epoch 331: averaged Loss: 45.854622\n",
      "2025-03-07 23:09: **********Val Epoch 331: average Loss: 2312.787842\n",
      "2025-03-07 23:09: Train Epoch 332: 0/23 Loss: 5.989329\n",
      "2025-03-07 23:09: Train Epoch 332: 20/23 Loss: 91.881920\n",
      "2025-03-07 23:09: **********Train Epoch 332: averaged Loss: 42.375064\n",
      "2025-03-07 23:09: **********Val Epoch 332: average Loss: 2287.647827\n",
      "2025-03-07 23:09: Train Epoch 333: 0/23 Loss: 9.035042\n",
      "2025-03-07 23:09: Train Epoch 333: 20/23 Loss: 74.657806\n",
      "2025-03-07 23:09: **********Train Epoch 333: averaged Loss: 40.838447\n",
      "2025-03-07 23:09: **********Val Epoch 333: average Loss: 2272.873413\n",
      "2025-03-07 23:09: Train Epoch 334: 0/23 Loss: 6.780896\n",
      "2025-03-07 23:09: Train Epoch 334: 20/23 Loss: 72.205276\n",
      "2025-03-07 23:09: **********Train Epoch 334: averaged Loss: 35.203184\n",
      "2025-03-07 23:09: **********Val Epoch 334: average Loss: 2262.156189\n",
      "2025-03-07 23:09: Train Epoch 335: 0/23 Loss: 8.315996\n",
      "2025-03-07 23:09: Train Epoch 335: 20/23 Loss: 76.412933\n",
      "2025-03-07 23:09: **********Train Epoch 335: averaged Loss: 37.122934\n",
      "2025-03-07 23:09: **********Val Epoch 335: average Loss: 2310.589539\n",
      "2025-03-07 23:09: Train Epoch 336: 0/23 Loss: 4.573101\n",
      "2025-03-07 23:09: Train Epoch 336: 20/23 Loss: 79.014160\n",
      "2025-03-07 23:09: **********Train Epoch 336: averaged Loss: 36.089018\n",
      "2025-03-07 23:09: **********Val Epoch 336: average Loss: 2295.745300\n",
      "2025-03-07 23:09: Train Epoch 337: 0/23 Loss: 13.242612\n",
      "2025-03-07 23:09: Train Epoch 337: 20/23 Loss: 59.118279\n",
      "2025-03-07 23:09: **********Train Epoch 337: averaged Loss: 35.962034\n",
      "2025-03-07 23:09: **********Val Epoch 337: average Loss: 2260.986572\n",
      "2025-03-07 23:09: Train Epoch 338: 0/23 Loss: 1.666022\n",
      "2025-03-07 23:09: Train Epoch 338: 20/23 Loss: 91.091774\n",
      "2025-03-07 23:09: **********Train Epoch 338: averaged Loss: 35.204978\n",
      "2025-03-07 23:09: **********Val Epoch 338: average Loss: 2291.221252\n",
      "2025-03-07 23:09: Train Epoch 339: 0/23 Loss: 1.976753\n",
      "2025-03-07 23:09: Train Epoch 339: 20/23 Loss: 120.107330\n",
      "2025-03-07 23:09: **********Train Epoch 339: averaged Loss: 37.725187\n",
      "2025-03-07 23:09: **********Val Epoch 339: average Loss: 2315.544983\n",
      "2025-03-07 23:09: Train Epoch 340: 0/23 Loss: 0.732852\n",
      "2025-03-07 23:09: Train Epoch 340: 20/23 Loss: 55.769390\n",
      "2025-03-07 23:09: **********Train Epoch 340: averaged Loss: 34.812259\n",
      "2025-03-07 23:09: **********Val Epoch 340: average Loss: 2287.941772\n",
      "2025-03-07 23:09: Train Epoch 341: 0/23 Loss: 3.919317\n",
      "2025-03-07 23:09: Train Epoch 341: 20/23 Loss: 106.441841\n",
      "2025-03-07 23:09: **********Train Epoch 341: averaged Loss: 41.691719\n",
      "2025-03-07 23:09: **********Val Epoch 341: average Loss: 2282.600708\n",
      "2025-03-07 23:09: Train Epoch 342: 0/23 Loss: 3.886010\n",
      "2025-03-07 23:09: Train Epoch 342: 20/23 Loss: 102.090698\n",
      "2025-03-07 23:09: **********Train Epoch 342: averaged Loss: 37.922222\n",
      "2025-03-07 23:09: **********Val Epoch 342: average Loss: 2263.111938\n",
      "2025-03-07 23:09: Train Epoch 343: 0/23 Loss: 8.688779\n",
      "2025-03-07 23:09: Train Epoch 343: 20/23 Loss: 120.158440\n",
      "2025-03-07 23:09: **********Train Epoch 343: averaged Loss: 39.804184\n",
      "2025-03-07 23:09: **********Val Epoch 343: average Loss: 2260.181213\n",
      "2025-03-07 23:09: Train Epoch 344: 0/23 Loss: 7.245987\n",
      "2025-03-07 23:09: Train Epoch 344: 20/23 Loss: 81.770920\n",
      "2025-03-07 23:09: **********Train Epoch 344: averaged Loss: 38.234124\n",
      "2025-03-07 23:09: **********Val Epoch 344: average Loss: 2272.495239\n",
      "2025-03-07 23:09: Train Epoch 345: 0/23 Loss: 11.619455\n",
      "2025-03-07 23:09: Train Epoch 345: 20/23 Loss: 55.615364\n",
      "2025-03-07 23:09: **********Train Epoch 345: averaged Loss: 40.718511\n",
      "2025-03-07 23:09: **********Val Epoch 345: average Loss: 2296.050842\n",
      "2025-03-07 23:09: Train Epoch 346: 0/23 Loss: 3.512531\n",
      "2025-03-07 23:09: Train Epoch 346: 20/23 Loss: 71.133820\n",
      "2025-03-07 23:09: **********Train Epoch 346: averaged Loss: 36.998976\n",
      "2025-03-07 23:09: **********Val Epoch 346: average Loss: 2265.875061\n",
      "2025-03-07 23:09: Train Epoch 347: 0/23 Loss: 12.987524\n",
      "2025-03-07 23:09: Train Epoch 347: 20/23 Loss: 135.890717\n",
      "2025-03-07 23:09: **********Train Epoch 347: averaged Loss: 41.444509\n",
      "2025-03-07 23:09: **********Val Epoch 347: average Loss: 2322.738464\n",
      "2025-03-07 23:09: Train Epoch 348: 0/23 Loss: 1.118490\n",
      "2025-03-07 23:09: Train Epoch 348: 20/23 Loss: 72.398064\n",
      "2025-03-07 23:09: **********Train Epoch 348: averaged Loss: 36.887871\n",
      "2025-03-07 23:09: **********Val Epoch 348: average Loss: 2258.302551\n",
      "2025-03-07 23:09: Train Epoch 349: 0/23 Loss: 12.708860\n",
      "2025-03-07 23:10: Train Epoch 349: 20/23 Loss: 102.308968\n",
      "2025-03-07 23:10: **********Train Epoch 349: averaged Loss: 47.694577\n",
      "2025-03-07 23:10: **********Val Epoch 349: average Loss: 2263.972412\n",
      "2025-03-07 23:10: Train Epoch 350: 0/23 Loss: 4.732388\n",
      "2025-03-07 23:10: Train Epoch 350: 20/23 Loss: 67.565887\n",
      "2025-03-07 23:10: **********Train Epoch 350: averaged Loss: 36.337885\n",
      "2025-03-07 23:10: **********Val Epoch 350: average Loss: 2250.784546\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 351: 0/23 Loss: 13.159155\n",
      "2025-03-07 23:10: Train Epoch 351: 20/23 Loss: 91.021484\n",
      "2025-03-07 23:10: **********Train Epoch 351: averaged Loss: 38.943673\n",
      "2025-03-07 23:10: **********Val Epoch 351: average Loss: 2260.460266\n",
      "2025-03-07 23:10: Train Epoch 352: 0/23 Loss: 4.051302\n",
      "2025-03-07 23:10: Train Epoch 352: 20/23 Loss: 53.632797\n",
      "2025-03-07 23:10: **********Train Epoch 352: averaged Loss: 34.379876\n",
      "2025-03-07 23:10: **********Val Epoch 352: average Loss: 2251.991699\n",
      "2025-03-07 23:10: Train Epoch 353: 0/23 Loss: 9.068686\n",
      "2025-03-07 23:10: Train Epoch 353: 20/23 Loss: 47.571281\n",
      "2025-03-07 23:10: **********Train Epoch 353: averaged Loss: 35.671828\n",
      "2025-03-07 23:10: **********Val Epoch 353: average Loss: 2268.541138\n",
      "2025-03-07 23:10: Train Epoch 354: 0/23 Loss: 4.306114\n",
      "2025-03-07 23:10: Train Epoch 354: 20/23 Loss: 56.400749\n",
      "2025-03-07 23:10: **********Train Epoch 354: averaged Loss: 33.357694\n",
      "2025-03-07 23:10: **********Val Epoch 354: average Loss: 2266.625122\n",
      "2025-03-07 23:10: Train Epoch 355: 0/23 Loss: 11.559780\n",
      "2025-03-07 23:10: Train Epoch 355: 20/23 Loss: 61.563747\n",
      "2025-03-07 23:10: **********Train Epoch 355: averaged Loss: 34.715399\n",
      "2025-03-07 23:10: **********Val Epoch 355: average Loss: 2260.289490\n",
      "2025-03-07 23:10: Train Epoch 356: 0/23 Loss: 1.006120\n",
      "2025-03-07 23:10: Train Epoch 356: 20/23 Loss: 76.105843\n",
      "2025-03-07 23:10: **********Train Epoch 356: averaged Loss: 36.359724\n",
      "2025-03-07 23:10: **********Val Epoch 356: average Loss: 2259.003296\n",
      "2025-03-07 23:10: Train Epoch 357: 0/23 Loss: 4.784270\n",
      "2025-03-07 23:10: Train Epoch 357: 20/23 Loss: 55.162651\n",
      "2025-03-07 23:10: **********Train Epoch 357: averaged Loss: 34.829924\n",
      "2025-03-07 23:10: **********Val Epoch 357: average Loss: 2276.576538\n",
      "2025-03-07 23:10: Train Epoch 358: 0/23 Loss: 6.395547\n",
      "2025-03-07 23:10: Train Epoch 358: 20/23 Loss: 97.390549\n",
      "2025-03-07 23:10: **********Train Epoch 358: averaged Loss: 42.710715\n",
      "2025-03-07 23:10: **********Val Epoch 358: average Loss: 2259.454102\n",
      "2025-03-07 23:10: Train Epoch 359: 0/23 Loss: 6.946872\n",
      "2025-03-07 23:10: Train Epoch 359: 20/23 Loss: 84.617210\n",
      "2025-03-07 23:10: **********Train Epoch 359: averaged Loss: 37.931759\n",
      "2025-03-07 23:10: **********Val Epoch 359: average Loss: 2228.739807\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 360: 0/23 Loss: 8.997173\n",
      "2025-03-07 23:10: Train Epoch 360: 20/23 Loss: 72.366898\n",
      "2025-03-07 23:10: **********Train Epoch 360: averaged Loss: 45.671874\n",
      "2025-03-07 23:10: **********Val Epoch 360: average Loss: 2260.153809\n",
      "2025-03-07 23:10: Train Epoch 361: 0/23 Loss: 3.367100\n",
      "2025-03-07 23:10: Train Epoch 361: 20/23 Loss: 57.333973\n",
      "2025-03-07 23:10: **********Train Epoch 361: averaged Loss: 37.195321\n",
      "2025-03-07 23:10: **********Val Epoch 361: average Loss: 2227.594238\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 362: 0/23 Loss: 12.857363\n",
      "2025-03-07 23:10: Train Epoch 362: 20/23 Loss: 113.034775\n",
      "2025-03-07 23:10: **********Train Epoch 362: averaged Loss: 36.214841\n",
      "2025-03-07 23:10: **********Val Epoch 362: average Loss: 2257.186707\n",
      "2025-03-07 23:10: Train Epoch 363: 0/23 Loss: 7.547880\n",
      "2025-03-07 23:10: Train Epoch 363: 20/23 Loss: 61.293049\n",
      "2025-03-07 23:10: **********Train Epoch 363: averaged Loss: 31.581531\n",
      "2025-03-07 23:10: **********Val Epoch 363: average Loss: 2283.788269\n",
      "2025-03-07 23:10: Train Epoch 364: 0/23 Loss: 17.963779\n",
      "2025-03-07 23:10: Train Epoch 364: 20/23 Loss: 66.760666\n",
      "2025-03-07 23:10: **********Train Epoch 364: averaged Loss: 38.315616\n",
      "2025-03-07 23:10: **********Val Epoch 364: average Loss: 2297.975159\n",
      "2025-03-07 23:10: Train Epoch 365: 0/23 Loss: 8.750872\n",
      "2025-03-07 23:10: Train Epoch 365: 20/23 Loss: 118.057556\n",
      "2025-03-07 23:10: **********Train Epoch 365: averaged Loss: 35.277956\n",
      "2025-03-07 23:10: **********Val Epoch 365: average Loss: 2237.215820\n",
      "2025-03-07 23:10: Train Epoch 366: 0/23 Loss: 12.272614\n",
      "2025-03-07 23:10: Train Epoch 366: 20/23 Loss: 109.794319\n",
      "2025-03-07 23:10: **********Train Epoch 366: averaged Loss: 41.003126\n",
      "2025-03-07 23:10: **********Val Epoch 366: average Loss: 2237.017944\n",
      "2025-03-07 23:10: Train Epoch 367: 0/23 Loss: 3.762022\n",
      "2025-03-07 23:10: Train Epoch 367: 20/23 Loss: 56.404613\n",
      "2025-03-07 23:10: **********Train Epoch 367: averaged Loss: 58.289182\n",
      "2025-03-07 23:10: **********Val Epoch 367: average Loss: 2229.167725\n",
      "2025-03-07 23:10: Train Epoch 368: 0/23 Loss: 0.923002\n",
      "2025-03-07 23:10: Train Epoch 368: 20/23 Loss: 77.104744\n",
      "2025-03-07 23:10: **********Train Epoch 368: averaged Loss: 36.809861\n",
      "2025-03-07 23:10: **********Val Epoch 368: average Loss: 2257.951477\n",
      "2025-03-07 23:10: Train Epoch 369: 0/23 Loss: 12.816021\n",
      "2025-03-07 23:10: Train Epoch 369: 20/23 Loss: 90.885620\n",
      "2025-03-07 23:10: **********Train Epoch 369: averaged Loss: 35.481099\n",
      "2025-03-07 23:10: **********Val Epoch 369: average Loss: 2213.136108\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 370: 0/23 Loss: 2.677278\n",
      "2025-03-07 23:10: Train Epoch 370: 20/23 Loss: 72.610046\n",
      "2025-03-07 23:10: **********Train Epoch 370: averaged Loss: 34.029473\n",
      "2025-03-07 23:10: **********Val Epoch 370: average Loss: 2256.554321\n",
      "2025-03-07 23:10: Train Epoch 371: 0/23 Loss: 1.949597\n",
      "2025-03-07 23:10: Train Epoch 371: 20/23 Loss: 114.818031\n",
      "2025-03-07 23:10: **********Train Epoch 371: averaged Loss: 36.215661\n",
      "2025-03-07 23:10: **********Val Epoch 371: average Loss: 2223.768738\n",
      "2025-03-07 23:10: Train Epoch 372: 0/23 Loss: 6.736537\n",
      "2025-03-07 23:10: Train Epoch 372: 20/23 Loss: 104.678230\n",
      "2025-03-07 23:10: **********Train Epoch 372: averaged Loss: 41.958819\n",
      "2025-03-07 23:10: **********Val Epoch 372: average Loss: 2274.553284\n",
      "2025-03-07 23:10: Train Epoch 373: 0/23 Loss: 2.446434\n",
      "2025-03-07 23:10: Train Epoch 373: 20/23 Loss: 53.474049\n",
      "2025-03-07 23:10: **********Train Epoch 373: averaged Loss: 32.835538\n",
      "2025-03-07 23:10: **********Val Epoch 373: average Loss: 2236.714661\n",
      "2025-03-07 23:10: Train Epoch 374: 0/23 Loss: 12.379608\n",
      "2025-03-07 23:10: Train Epoch 374: 20/23 Loss: 56.027130\n",
      "2025-03-07 23:10: **********Train Epoch 374: averaged Loss: 35.026023\n",
      "2025-03-07 23:10: **********Val Epoch 374: average Loss: 2248.148438\n",
      "2025-03-07 23:10: Train Epoch 375: 0/23 Loss: 9.029206\n",
      "2025-03-07 23:10: Train Epoch 375: 20/23 Loss: 72.263031\n",
      "2025-03-07 23:10: **********Train Epoch 375: averaged Loss: 33.425117\n",
      "2025-03-07 23:10: **********Val Epoch 375: average Loss: 2231.385315\n",
      "2025-03-07 23:10: Train Epoch 376: 0/23 Loss: 14.512980\n",
      "2025-03-07 23:10: Train Epoch 376: 20/23 Loss: 62.846474\n",
      "2025-03-07 23:10: **********Train Epoch 376: averaged Loss: 35.909125\n",
      "2025-03-07 23:10: **********Val Epoch 376: average Loss: 2211.793823\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 377: 0/23 Loss: 4.081653\n",
      "2025-03-07 23:10: Train Epoch 377: 20/23 Loss: 96.327881\n",
      "2025-03-07 23:10: **********Train Epoch 377: averaged Loss: 40.506934\n",
      "2025-03-07 23:10: **********Val Epoch 377: average Loss: 2221.646118\n",
      "2025-03-07 23:10: Train Epoch 378: 0/23 Loss: 4.718507\n",
      "2025-03-07 23:10: Train Epoch 378: 20/23 Loss: 58.244244\n",
      "2025-03-07 23:10: **********Train Epoch 378: averaged Loss: 33.485267\n",
      "2025-03-07 23:10: **********Val Epoch 378: average Loss: 2210.515137\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 379: 0/23 Loss: 10.018638\n",
      "2025-03-07 23:10: Train Epoch 379: 20/23 Loss: 74.780518\n",
      "2025-03-07 23:10: **********Train Epoch 379: averaged Loss: 33.621323\n",
      "2025-03-07 23:10: **********Val Epoch 379: average Loss: 2233.491821\n",
      "2025-03-07 23:10: Train Epoch 380: 0/23 Loss: 1.602192\n",
      "2025-03-07 23:10: Train Epoch 380: 20/23 Loss: 71.621613\n",
      "2025-03-07 23:10: **********Train Epoch 380: averaged Loss: 30.611820\n",
      "2025-03-07 23:10: **********Val Epoch 380: average Loss: 2213.628174\n",
      "2025-03-07 23:10: Train Epoch 381: 0/23 Loss: 8.337543\n",
      "2025-03-07 23:10: Train Epoch 381: 20/23 Loss: 83.948418\n",
      "2025-03-07 23:10: **********Train Epoch 381: averaged Loss: 33.054631\n",
      "2025-03-07 23:10: **********Val Epoch 381: average Loss: 2232.856934\n",
      "2025-03-07 23:10: Train Epoch 382: 0/23 Loss: 4.038557\n",
      "2025-03-07 23:10: Train Epoch 382: 20/23 Loss: 50.306213\n",
      "2025-03-07 23:10: **********Train Epoch 382: averaged Loss: 32.720703\n",
      "2025-03-07 23:10: **********Val Epoch 382: average Loss: 2197.653564\n",
      "2025-03-07 23:10: *********************************Current best model saved!\n",
      "2025-03-07 23:10: Train Epoch 383: 0/23 Loss: 3.508782\n",
      "2025-03-07 23:10: Train Epoch 383: 20/23 Loss: 60.850056\n",
      "2025-03-07 23:10: **********Train Epoch 383: averaged Loss: 37.554361\n",
      "2025-03-07 23:10: **********Val Epoch 383: average Loss: 2237.493530\n",
      "2025-03-07 23:10: Train Epoch 384: 0/23 Loss: 6.420978\n",
      "2025-03-07 23:10: Train Epoch 384: 20/23 Loss: 74.257545\n",
      "2025-03-07 23:10: **********Train Epoch 384: averaged Loss: 33.153657\n",
      "2025-03-07 23:10: **********Val Epoch 384: average Loss: 2198.559937\n",
      "2025-03-07 23:10: Train Epoch 385: 0/23 Loss: 5.809402\n",
      "2025-03-07 23:11: Train Epoch 385: 20/23 Loss: 61.839596\n",
      "2025-03-07 23:11: **********Train Epoch 385: averaged Loss: 35.052665\n",
      "2025-03-07 23:11: **********Val Epoch 385: average Loss: 2223.015991\n",
      "2025-03-07 23:11: Train Epoch 386: 0/23 Loss: 6.920897\n",
      "2025-03-07 23:11: Train Epoch 386: 20/23 Loss: 95.941284\n",
      "2025-03-07 23:11: **********Train Epoch 386: averaged Loss: 43.827907\n",
      "2025-03-07 23:11: **********Val Epoch 386: average Loss: 2237.258423\n",
      "2025-03-07 23:11: Train Epoch 387: 0/23 Loss: 1.047136\n",
      "2025-03-07 23:11: Train Epoch 387: 20/23 Loss: 84.109146\n",
      "2025-03-07 23:11: **********Train Epoch 387: averaged Loss: 34.033855\n",
      "2025-03-07 23:11: **********Val Epoch 387: average Loss: 2213.080444\n",
      "2025-03-07 23:11: Train Epoch 388: 0/23 Loss: 6.535962\n",
      "2025-03-07 23:11: Train Epoch 388: 20/23 Loss: 77.808334\n",
      "2025-03-07 23:11: **********Train Epoch 388: averaged Loss: 32.883971\n",
      "2025-03-07 23:11: **********Val Epoch 388: average Loss: 2238.635742\n",
      "2025-03-07 23:11: Train Epoch 389: 0/23 Loss: 3.354567\n",
      "2025-03-07 23:11: Train Epoch 389: 20/23 Loss: 52.049797\n",
      "2025-03-07 23:11: **********Train Epoch 389: averaged Loss: 29.579674\n",
      "2025-03-07 23:11: **********Val Epoch 389: average Loss: 2216.841187\n",
      "2025-03-07 23:11: Train Epoch 390: 0/23 Loss: 7.618933\n",
      "2025-03-07 23:11: Train Epoch 390: 20/23 Loss: 97.763725\n",
      "2025-03-07 23:11: **********Train Epoch 390: averaged Loss: 33.413886\n",
      "2025-03-07 23:11: **********Val Epoch 390: average Loss: 2195.773438\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 391: 0/23 Loss: 6.813580\n",
      "2025-03-07 23:11: Train Epoch 391: 20/23 Loss: 59.777008\n",
      "2025-03-07 23:11: **********Train Epoch 391: averaged Loss: 35.215681\n",
      "2025-03-07 23:11: **********Val Epoch 391: average Loss: 2189.738159\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 392: 0/23 Loss: 14.485399\n",
      "2025-03-07 23:11: Train Epoch 392: 20/23 Loss: 87.673279\n",
      "2025-03-07 23:11: **********Train Epoch 392: averaged Loss: 42.939976\n",
      "2025-03-07 23:11: **********Val Epoch 392: average Loss: 2235.364990\n",
      "2025-03-07 23:11: Train Epoch 393: 0/23 Loss: 1.369238\n",
      "2025-03-07 23:11: Train Epoch 393: 20/23 Loss: 57.953697\n",
      "2025-03-07 23:11: **********Train Epoch 393: averaged Loss: 34.994020\n",
      "2025-03-07 23:11: **********Val Epoch 393: average Loss: 2198.463501\n",
      "2025-03-07 23:11: Train Epoch 394: 0/23 Loss: 11.670931\n",
      "2025-03-07 23:11: Train Epoch 394: 20/23 Loss: 53.231628\n",
      "2025-03-07 23:11: **********Train Epoch 394: averaged Loss: 31.757679\n",
      "2025-03-07 23:11: **********Val Epoch 394: average Loss: 2185.395630\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 395: 0/23 Loss: 2.618582\n",
      "2025-03-07 23:11: Train Epoch 395: 20/23 Loss: 60.035263\n",
      "2025-03-07 23:11: **********Train Epoch 395: averaged Loss: 29.615089\n",
      "2025-03-07 23:11: **********Val Epoch 395: average Loss: 2195.112427\n",
      "2025-03-07 23:11: Train Epoch 396: 0/23 Loss: 8.083412\n",
      "2025-03-07 23:11: Train Epoch 396: 20/23 Loss: 63.614292\n",
      "2025-03-07 23:11: **********Train Epoch 396: averaged Loss: 38.690377\n",
      "2025-03-07 23:11: **********Val Epoch 396: average Loss: 2189.259277\n",
      "2025-03-07 23:11: Train Epoch 397: 0/23 Loss: 6.073661\n",
      "2025-03-07 23:11: Train Epoch 397: 20/23 Loss: 62.284393\n",
      "2025-03-07 23:11: **********Train Epoch 397: averaged Loss: 42.024048\n",
      "2025-03-07 23:11: **********Val Epoch 397: average Loss: 2217.347534\n",
      "2025-03-07 23:11: Train Epoch 398: 0/23 Loss: 0.939727\n",
      "2025-03-07 23:11: Train Epoch 398: 20/23 Loss: 89.283073\n",
      "2025-03-07 23:11: **********Train Epoch 398: averaged Loss: 33.108467\n",
      "2025-03-07 23:11: **********Val Epoch 398: average Loss: 2226.127930\n",
      "2025-03-07 23:11: Train Epoch 399: 0/23 Loss: 13.518817\n",
      "2025-03-07 23:11: Train Epoch 399: 20/23 Loss: 66.495438\n",
      "2025-03-07 23:11: **********Train Epoch 399: averaged Loss: 31.776072\n",
      "2025-03-07 23:11: **********Val Epoch 399: average Loss: 2201.886963\n",
      "2025-03-07 23:11: Train Epoch 400: 0/23 Loss: 4.725695\n",
      "2025-03-07 23:11: Train Epoch 400: 20/23 Loss: 60.479172\n",
      "2025-03-07 23:11: **********Train Epoch 400: averaged Loss: 31.437924\n",
      "2025-03-07 23:11: **********Val Epoch 400: average Loss: 2183.224304\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 401: 0/23 Loss: 13.879074\n",
      "2025-03-07 23:11: Train Epoch 401: 20/23 Loss: 57.680874\n",
      "2025-03-07 23:11: **********Train Epoch 401: averaged Loss: 35.207388\n",
      "2025-03-07 23:11: **********Val Epoch 401: average Loss: 2218.635620\n",
      "2025-03-07 23:11: Train Epoch 402: 0/23 Loss: 3.853055\n",
      "2025-03-07 23:11: Train Epoch 402: 20/23 Loss: 60.492397\n",
      "2025-03-07 23:11: **********Train Epoch 402: averaged Loss: 31.689884\n",
      "2025-03-07 23:11: **********Val Epoch 402: average Loss: 2245.558716\n",
      "2025-03-07 23:11: Train Epoch 403: 0/23 Loss: 10.573230\n",
      "2025-03-07 23:11: Train Epoch 403: 20/23 Loss: 64.776642\n",
      "2025-03-07 23:11: **********Train Epoch 403: averaged Loss: 32.402472\n",
      "2025-03-07 23:11: **********Val Epoch 403: average Loss: 2210.555420\n",
      "2025-03-07 23:11: Train Epoch 404: 0/23 Loss: 2.175805\n",
      "2025-03-07 23:11: Train Epoch 404: 20/23 Loss: 103.457436\n",
      "2025-03-07 23:11: **********Train Epoch 404: averaged Loss: 31.663927\n",
      "2025-03-07 23:11: **********Val Epoch 404: average Loss: 2219.929993\n",
      "2025-03-07 23:11: Train Epoch 405: 0/23 Loss: 8.194076\n",
      "2025-03-07 23:11: Train Epoch 405: 20/23 Loss: 54.202415\n",
      "2025-03-07 23:11: **********Train Epoch 405: averaged Loss: 33.321930\n",
      "2025-03-07 23:11: **********Val Epoch 405: average Loss: 2190.514343\n",
      "2025-03-07 23:11: Train Epoch 406: 0/23 Loss: 0.646273\n",
      "2025-03-07 23:11: Train Epoch 406: 20/23 Loss: 98.011108\n",
      "2025-03-07 23:11: **********Train Epoch 406: averaged Loss: 40.010064\n",
      "2025-03-07 23:11: **********Val Epoch 406: average Loss: 2189.936646\n",
      "2025-03-07 23:11: Train Epoch 407: 0/23 Loss: 10.705745\n",
      "2025-03-07 23:11: Train Epoch 407: 20/23 Loss: 92.952484\n",
      "2025-03-07 23:11: **********Train Epoch 407: averaged Loss: 35.715581\n",
      "2025-03-07 23:11: **********Val Epoch 407: average Loss: 2177.369080\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 408: 0/23 Loss: 5.203710\n",
      "2025-03-07 23:11: Train Epoch 408: 20/23 Loss: 96.759003\n",
      "2025-03-07 23:11: **********Train Epoch 408: averaged Loss: 34.595973\n",
      "2025-03-07 23:11: **********Val Epoch 408: average Loss: 2173.948547\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 409: 0/23 Loss: 7.792381\n",
      "2025-03-07 23:11: Train Epoch 409: 20/23 Loss: 61.643295\n",
      "2025-03-07 23:11: **********Train Epoch 409: averaged Loss: 37.362430\n",
      "2025-03-07 23:11: **********Val Epoch 409: average Loss: 2195.318542\n",
      "2025-03-07 23:11: Train Epoch 410: 0/23 Loss: 10.855644\n",
      "2025-03-07 23:11: Train Epoch 410: 20/23 Loss: 69.398743\n",
      "2025-03-07 23:11: **********Train Epoch 410: averaged Loss: 43.293673\n",
      "2025-03-07 23:11: **********Val Epoch 410: average Loss: 2180.265015\n",
      "2025-03-07 23:11: Train Epoch 411: 0/23 Loss: 6.639961\n",
      "2025-03-07 23:11: Train Epoch 411: 20/23 Loss: 78.295486\n",
      "2025-03-07 23:11: **********Train Epoch 411: averaged Loss: 36.850931\n",
      "2025-03-07 23:11: **********Val Epoch 411: average Loss: 2203.174805\n",
      "2025-03-07 23:11: Train Epoch 412: 0/23 Loss: 16.417591\n",
      "2025-03-07 23:11: Train Epoch 412: 20/23 Loss: 65.609879\n",
      "2025-03-07 23:11: **********Train Epoch 412: averaged Loss: 32.690380\n",
      "2025-03-07 23:11: **********Val Epoch 412: average Loss: 2215.450867\n",
      "2025-03-07 23:11: Train Epoch 413: 0/23 Loss: 3.302296\n",
      "2025-03-07 23:11: Train Epoch 413: 20/23 Loss: 48.594517\n",
      "2025-03-07 23:11: **********Train Epoch 413: averaged Loss: 30.721532\n",
      "2025-03-07 23:11: **********Val Epoch 413: average Loss: 2187.985291\n",
      "2025-03-07 23:11: Train Epoch 414: 0/23 Loss: 8.962954\n",
      "2025-03-07 23:11: Train Epoch 414: 20/23 Loss: 67.035202\n",
      "2025-03-07 23:11: **********Train Epoch 414: averaged Loss: 33.982442\n",
      "2025-03-07 23:11: **********Val Epoch 414: average Loss: 2172.235352\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 415: 0/23 Loss: 1.435105\n",
      "2025-03-07 23:11: Train Epoch 415: 20/23 Loss: 62.518814\n",
      "2025-03-07 23:11: **********Train Epoch 415: averaged Loss: 32.862807\n",
      "2025-03-07 23:11: **********Val Epoch 415: average Loss: 2143.970825\n",
      "2025-03-07 23:11: *********************************Current best model saved!\n",
      "2025-03-07 23:11: Train Epoch 416: 0/23 Loss: 8.288122\n",
      "2025-03-07 23:11: Train Epoch 416: 20/23 Loss: 88.276993\n",
      "2025-03-07 23:11: **********Train Epoch 416: averaged Loss: 35.153748\n",
      "2025-03-07 23:11: **********Val Epoch 416: average Loss: 2190.289307\n",
      "2025-03-07 23:11: Train Epoch 417: 0/23 Loss: 4.177366\n",
      "2025-03-07 23:11: Train Epoch 417: 20/23 Loss: 58.249760\n",
      "2025-03-07 23:11: **********Train Epoch 417: averaged Loss: 35.631023\n",
      "2025-03-07 23:11: **********Val Epoch 417: average Loss: 2166.655518\n",
      "2025-03-07 23:11: Train Epoch 418: 0/23 Loss: 15.657707\n",
      "2025-03-07 23:11: Train Epoch 418: 20/23 Loss: 87.838287\n",
      "2025-03-07 23:11: **********Train Epoch 418: averaged Loss: 44.391746\n",
      "2025-03-07 23:11: **********Val Epoch 418: average Loss: 2174.055176\n",
      "2025-03-07 23:11: Train Epoch 419: 0/23 Loss: 4.932358\n",
      "2025-03-07 23:11: Train Epoch 419: 20/23 Loss: 72.927574\n",
      "2025-03-07 23:11: **********Train Epoch 419: averaged Loss: 34.888001\n",
      "2025-03-07 23:11: **********Val Epoch 419: average Loss: 2170.505066\n",
      "2025-03-07 23:11: Train Epoch 420: 0/23 Loss: 9.718069\n",
      "2025-03-07 23:12: Train Epoch 420: 20/23 Loss: 91.967270\n",
      "2025-03-07 23:12: **********Train Epoch 420: averaged Loss: 32.396137\n",
      "2025-03-07 23:12: **********Val Epoch 420: average Loss: 2171.477295\n",
      "2025-03-07 23:12: Train Epoch 421: 0/23 Loss: 6.273287\n",
      "2025-03-07 23:12: Train Epoch 421: 20/23 Loss: 64.202805\n",
      "2025-03-07 23:12: **********Train Epoch 421: averaged Loss: 31.740102\n",
      "2025-03-07 23:12: **********Val Epoch 421: average Loss: 2167.838013\n",
      "2025-03-07 23:12: Train Epoch 422: 0/23 Loss: 16.074486\n",
      "2025-03-07 23:12: Train Epoch 422: 20/23 Loss: 68.257767\n",
      "2025-03-07 23:12: **********Train Epoch 422: averaged Loss: 32.286662\n",
      "2025-03-07 23:12: **********Val Epoch 422: average Loss: 2169.420166\n",
      "2025-03-07 23:12: Train Epoch 423: 0/23 Loss: 6.086061\n",
      "2025-03-07 23:12: Train Epoch 423: 20/23 Loss: 77.487579\n",
      "2025-03-07 23:12: **********Train Epoch 423: averaged Loss: 30.659195\n",
      "2025-03-07 23:12: **********Val Epoch 423: average Loss: 2188.622864\n",
      "2025-03-07 23:12: Train Epoch 424: 0/23 Loss: 7.131962\n",
      "2025-03-07 23:12: Train Epoch 424: 20/23 Loss: 73.390099\n",
      "2025-03-07 23:12: **********Train Epoch 424: averaged Loss: 32.889771\n",
      "2025-03-07 23:12: **********Val Epoch 424: average Loss: 2192.587830\n",
      "2025-03-07 23:12: Train Epoch 425: 0/23 Loss: 4.486225\n",
      "2025-03-07 23:12: Train Epoch 425: 20/23 Loss: 111.655334\n",
      "2025-03-07 23:12: **********Train Epoch 425: averaged Loss: 43.970989\n",
      "2025-03-07 23:12: **********Val Epoch 425: average Loss: 2193.822021\n",
      "2025-03-07 23:12: Train Epoch 426: 0/23 Loss: 3.728526\n",
      "2025-03-07 23:12: Train Epoch 426: 20/23 Loss: 64.965050\n",
      "2025-03-07 23:12: **********Train Epoch 426: averaged Loss: 33.408126\n",
      "2025-03-07 23:12: **********Val Epoch 426: average Loss: 2148.242798\n",
      "2025-03-07 23:12: Train Epoch 427: 0/23 Loss: 12.758802\n",
      "2025-03-07 23:12: Train Epoch 427: 20/23 Loss: 46.896957\n",
      "2025-03-07 23:12: **********Train Epoch 427: averaged Loss: 30.933971\n",
      "2025-03-07 23:12: **********Val Epoch 427: average Loss: 2199.778198\n",
      "2025-03-07 23:12: Train Epoch 428: 0/23 Loss: 1.064012\n",
      "2025-03-07 23:12: Train Epoch 428: 20/23 Loss: 50.427616\n",
      "2025-03-07 23:12: **********Train Epoch 428: averaged Loss: 28.697943\n",
      "2025-03-07 23:12: **********Val Epoch 428: average Loss: 2182.793213\n",
      "2025-03-07 23:12: Train Epoch 429: 0/23 Loss: 6.460385\n",
      "2025-03-07 23:12: Train Epoch 429: 20/23 Loss: 47.469826\n",
      "2025-03-07 23:12: **********Train Epoch 429: averaged Loss: 29.413001\n",
      "2025-03-07 23:12: **********Val Epoch 429: average Loss: 2167.548462\n",
      "2025-03-07 23:12: Train Epoch 430: 0/23 Loss: 1.609361\n",
      "2025-03-07 23:12: Train Epoch 430: 20/23 Loss: 54.202335\n",
      "2025-03-07 23:12: **********Train Epoch 430: averaged Loss: 30.094935\n",
      "2025-03-07 23:12: **********Val Epoch 430: average Loss: 2166.249512\n",
      "2025-03-07 23:12: Train Epoch 431: 0/23 Loss: 10.243067\n",
      "2025-03-07 23:12: Train Epoch 431: 20/23 Loss: 71.091728\n",
      "2025-03-07 23:12: **********Train Epoch 431: averaged Loss: 38.031907\n",
      "2025-03-07 23:12: **********Val Epoch 431: average Loss: 2120.403320\n",
      "2025-03-07 23:12: *********************************Current best model saved!\n",
      "2025-03-07 23:12: Train Epoch 432: 0/23 Loss: 12.477880\n",
      "2025-03-07 23:12: Train Epoch 432: 20/23 Loss: 62.967354\n",
      "2025-03-07 23:12: **********Train Epoch 432: averaged Loss: 30.091563\n",
      "2025-03-07 23:12: **********Val Epoch 432: average Loss: 2185.112183\n",
      "2025-03-07 23:12: Train Epoch 433: 0/23 Loss: 14.137855\n",
      "2025-03-07 23:12: Train Epoch 433: 20/23 Loss: 52.732147\n",
      "2025-03-07 23:12: **********Train Epoch 433: averaged Loss: 29.393333\n",
      "2025-03-07 23:12: **********Val Epoch 433: average Loss: 2150.609741\n",
      "2025-03-07 23:12: Train Epoch 434: 0/23 Loss: 2.916261\n",
      "2025-03-07 23:12: Train Epoch 434: 20/23 Loss: 57.605324\n",
      "2025-03-07 23:12: **********Train Epoch 434: averaged Loss: 28.586298\n",
      "2025-03-07 23:12: **********Val Epoch 434: average Loss: 2177.693604\n",
      "2025-03-07 23:12: Train Epoch 435: 0/23 Loss: 6.809841\n",
      "2025-03-07 23:12: Train Epoch 435: 20/23 Loss: 60.725777\n",
      "2025-03-07 23:12: **********Train Epoch 435: averaged Loss: 27.827664\n",
      "2025-03-07 23:12: **********Val Epoch 435: average Loss: 2120.626160\n",
      "2025-03-07 23:12: Train Epoch 436: 0/23 Loss: 3.079414\n",
      "2025-03-07 23:12: Train Epoch 436: 20/23 Loss: 65.069572\n",
      "2025-03-07 23:12: **********Train Epoch 436: averaged Loss: 26.899054\n",
      "2025-03-07 23:12: **********Val Epoch 436: average Loss: 2170.499268\n",
      "2025-03-07 23:12: Train Epoch 437: 0/23 Loss: 4.648446\n",
      "2025-03-07 23:12: Train Epoch 437: 20/23 Loss: 78.359848\n",
      "2025-03-07 23:12: **********Train Epoch 437: averaged Loss: 29.892230\n",
      "2025-03-07 23:12: **********Val Epoch 437: average Loss: 2180.072754\n",
      "2025-03-07 23:12: Train Epoch 438: 0/23 Loss: 11.960136\n",
      "2025-03-07 23:12: Train Epoch 438: 20/23 Loss: 71.723869\n",
      "2025-03-07 23:12: **********Train Epoch 438: averaged Loss: 29.063193\n",
      "2025-03-07 23:12: **********Val Epoch 438: average Loss: 2158.309998\n",
      "2025-03-07 23:12: Train Epoch 439: 0/23 Loss: 2.410692\n",
      "2025-03-07 23:12: Train Epoch 439: 20/23 Loss: 104.456848\n",
      "2025-03-07 23:12: **********Train Epoch 439: averaged Loss: 29.993972\n",
      "2025-03-07 23:12: **********Val Epoch 439: average Loss: 2135.983643\n",
      "2025-03-07 23:12: Train Epoch 440: 0/23 Loss: 1.169284\n",
      "2025-03-07 23:12: Train Epoch 440: 20/23 Loss: 55.229301\n",
      "2025-03-07 23:12: **********Train Epoch 440: averaged Loss: 29.536979\n",
      "2025-03-07 23:12: **********Val Epoch 440: average Loss: 2144.167297\n",
      "2025-03-07 23:12: Train Epoch 441: 0/23 Loss: 4.094422\n",
      "2025-03-07 23:12: Train Epoch 441: 20/23 Loss: 112.761383\n",
      "2025-03-07 23:12: **********Train Epoch 441: averaged Loss: 38.510383\n",
      "2025-03-07 23:12: **********Val Epoch 441: average Loss: 2105.023804\n",
      "2025-03-07 23:12: *********************************Current best model saved!\n",
      "2025-03-07 23:12: Train Epoch 442: 0/23 Loss: 4.391576\n",
      "2025-03-07 23:12: Train Epoch 442: 20/23 Loss: 78.280647\n",
      "2025-03-07 23:12: **********Train Epoch 442: averaged Loss: 30.730084\n",
      "2025-03-07 23:12: **********Val Epoch 442: average Loss: 2172.909363\n",
      "2025-03-07 23:12: Train Epoch 443: 0/23 Loss: 9.505198\n",
      "2025-03-07 23:12: Train Epoch 443: 20/23 Loss: 105.964172\n",
      "2025-03-07 23:12: **********Train Epoch 443: averaged Loss: 32.242418\n",
      "2025-03-07 23:12: **********Val Epoch 443: average Loss: 2134.095947\n",
      "2025-03-07 23:12: Train Epoch 444: 0/23 Loss: 5.043996\n",
      "2025-03-07 23:12: Train Epoch 444: 20/23 Loss: 77.077835\n",
      "2025-03-07 23:12: **********Train Epoch 444: averaged Loss: 33.746877\n",
      "2025-03-07 23:12: **********Val Epoch 444: average Loss: 2162.503052\n",
      "2025-03-07 23:12: Train Epoch 445: 20/23 Loss: 67.976746\n",
      "2025-03-07 23:12: **********Train Epoch 445: averaged Loss: 42.372614\n",
      "2025-03-07 23:12: **********Val Epoch 445: average Loss: 2146.970032\n",
      "2025-03-07 23:12: Train Epoch 446: 0/23 Loss: 6.289795\n",
      "2025-03-07 23:12: Train Epoch 446: 20/23 Loss: 63.924438\n",
      "2025-03-07 23:12: **********Train Epoch 446: averaged Loss: 32.542317\n",
      "2025-03-07 23:12: **********Val Epoch 446: average Loss: 2126.011963\n",
      "2025-03-07 23:12: Train Epoch 447: 0/23 Loss: 11.868879\n",
      "2025-03-07 23:12: Train Epoch 447: 20/23 Loss: 64.016861\n",
      "2025-03-07 23:12: **********Train Epoch 447: averaged Loss: 29.742952\n",
      "2025-03-07 23:12: **********Val Epoch 447: average Loss: 2150.740479\n",
      "2025-03-07 23:12: Train Epoch 448: 0/23 Loss: 6.072982\n",
      "2025-03-07 23:12: Train Epoch 448: 20/23 Loss: 111.570374\n",
      "2025-03-07 23:12: **********Train Epoch 448: averaged Loss: 39.271247\n",
      "2025-03-07 23:12: **********Val Epoch 448: average Loss: 2150.298462\n",
      "2025-03-07 23:12: Train Epoch 449: 0/23 Loss: 12.544048\n",
      "2025-03-07 23:12: Train Epoch 449: 20/23 Loss: 144.334442\n",
      "2025-03-07 23:12: **********Train Epoch 449: averaged Loss: 60.482693\n",
      "2025-03-07 23:12: **********Val Epoch 449: average Loss: 2155.454712\n",
      "2025-03-07 23:12: Train Epoch 450: 0/23 Loss: 2.607630\n",
      "2025-03-07 23:12: Train Epoch 450: 20/23 Loss: 88.210342\n",
      "2025-03-07 23:12: **********Train Epoch 450: averaged Loss: 33.077060\n",
      "2025-03-07 23:12: **********Val Epoch 450: average Loss: 2116.768188\n",
      "2025-03-07 23:12: Train Epoch 451: 0/23 Loss: 7.564818\n",
      "2025-03-07 23:12: Train Epoch 451: 20/23 Loss: 59.654953\n",
      "2025-03-07 23:12: **********Train Epoch 451: averaged Loss: 31.096961\n",
      "2025-03-07 23:12: **********Val Epoch 451: average Loss: 2148.884033\n",
      "2025-03-07 23:12: Train Epoch 452: 0/23 Loss: 5.756327\n",
      "2025-03-07 23:12: Train Epoch 452: 20/23 Loss: 57.613441\n",
      "2025-03-07 23:12: **********Train Epoch 452: averaged Loss: 32.909133\n",
      "2025-03-07 23:12: **********Val Epoch 452: average Loss: 2108.294189\n",
      "2025-03-07 23:12: Train Epoch 453: 0/23 Loss: 13.403313\n",
      "2025-03-07 23:12: Train Epoch 453: 20/23 Loss: 61.812416\n",
      "2025-03-07 23:12: **********Train Epoch 453: averaged Loss: 35.706414\n",
      "2025-03-07 23:12: **********Val Epoch 453: average Loss: 2135.656799\n",
      "2025-03-07 23:12: Train Epoch 454: 0/23 Loss: 3.055153\n",
      "2025-03-07 23:12: Train Epoch 454: 20/23 Loss: 54.930538\n",
      "2025-03-07 23:12: **********Train Epoch 454: averaged Loss: 27.482135\n",
      "2025-03-07 23:12: **********Val Epoch 454: average Loss: 2120.148193\n",
      "2025-03-07 23:12: Train Epoch 455: 0/23 Loss: 16.683853\n",
      "2025-03-07 23:12: Train Epoch 455: 20/23 Loss: 64.698502\n",
      "2025-03-07 23:12: **********Train Epoch 455: averaged Loss: 28.682780\n",
      "2025-03-07 23:12: **********Val Epoch 455: average Loss: 2150.293457\n",
      "2025-03-07 23:12: Train Epoch 456: 0/23 Loss: 6.214491\n",
      "2025-03-07 23:13: Train Epoch 456: 20/23 Loss: 64.023239\n",
      "2025-03-07 23:13: **********Train Epoch 456: averaged Loss: 30.714853\n",
      "2025-03-07 23:13: **********Val Epoch 456: average Loss: 2154.196777\n",
      "2025-03-07 23:13: Train Epoch 457: 0/23 Loss: 12.677583\n",
      "2025-03-07 23:13: Train Epoch 457: 20/23 Loss: 77.977661\n",
      "2025-03-07 23:13: **********Train Epoch 457: averaged Loss: 33.808975\n",
      "2025-03-07 23:13: **********Val Epoch 457: average Loss: 2108.135132\n",
      "2025-03-07 23:13: Train Epoch 458: 0/23 Loss: 5.843059\n",
      "2025-03-07 23:13: Train Epoch 458: 20/23 Loss: 69.789589\n",
      "2025-03-07 23:13: **********Train Epoch 458: averaged Loss: 27.169439\n",
      "2025-03-07 23:13: **********Val Epoch 458: average Loss: 2165.680481\n",
      "2025-03-07 23:13: Train Epoch 459: 0/23 Loss: 11.953188\n",
      "2025-03-07 23:13: Train Epoch 459: 20/23 Loss: 56.244114\n",
      "2025-03-07 23:13: **********Train Epoch 459: averaged Loss: 29.440934\n",
      "2025-03-07 23:13: **********Val Epoch 459: average Loss: 2164.751953\n",
      "2025-03-07 23:13: Train Epoch 460: 0/23 Loss: 2.886245\n",
      "2025-03-07 23:13: Train Epoch 460: 20/23 Loss: 70.383499\n",
      "2025-03-07 23:13: **********Train Epoch 460: averaged Loss: 31.416152\n",
      "2025-03-07 23:13: **********Val Epoch 460: average Loss: 2193.201355\n",
      "2025-03-07 23:13: Train Epoch 461: 0/23 Loss: 3.831433\n",
      "2025-03-07 23:13: Train Epoch 461: 20/23 Loss: 57.791603\n",
      "2025-03-07 23:13: **********Train Epoch 461: averaged Loss: 27.468006\n",
      "2025-03-07 23:13: **********Val Epoch 461: average Loss: 2121.176147\n",
      "2025-03-07 23:13: Train Epoch 462: 0/23 Loss: 11.434070\n",
      "2025-03-07 23:13: Train Epoch 462: 20/23 Loss: 64.050499\n",
      "2025-03-07 23:13: **********Train Epoch 462: averaged Loss: 33.258337\n",
      "2025-03-07 23:13: **********Val Epoch 462: average Loss: 2146.875122\n",
      "2025-03-07 23:13: Train Epoch 463: 0/23 Loss: 2.646800\n",
      "2025-03-07 23:13: Train Epoch 463: 20/23 Loss: 65.245407\n",
      "2025-03-07 23:13: **********Train Epoch 463: averaged Loss: 29.308703\n",
      "2025-03-07 23:13: **********Val Epoch 463: average Loss: 2124.857300\n",
      "2025-03-07 23:13: Train Epoch 464: 0/23 Loss: 11.528046\n",
      "2025-03-07 23:13: Train Epoch 464: 20/23 Loss: 86.027107\n",
      "2025-03-07 23:13: **********Train Epoch 464: averaged Loss: 32.198435\n",
      "2025-03-07 23:13: **********Val Epoch 464: average Loss: 2111.164673\n",
      "2025-03-07 23:13: Train Epoch 465: 0/23 Loss: 3.750489\n",
      "2025-03-07 23:13: Train Epoch 465: 20/23 Loss: 98.586678\n",
      "2025-03-07 23:13: **********Train Epoch 465: averaged Loss: 37.630301\n",
      "2025-03-07 23:13: **********Val Epoch 465: average Loss: 2109.515808\n",
      "2025-03-07 23:13: Train Epoch 466: 0/23 Loss: 4.856840\n",
      "2025-03-07 23:13: Train Epoch 466: 20/23 Loss: 79.747940\n",
      "2025-03-07 23:13: **********Train Epoch 466: averaged Loss: 43.251931\n",
      "2025-03-07 23:13: **********Val Epoch 466: average Loss: 2112.660950\n",
      "2025-03-07 23:13: Train Epoch 467: 0/23 Loss: 6.019592\n",
      "2025-03-07 23:13: Train Epoch 467: 20/23 Loss: 76.154587\n",
      "2025-03-07 23:13: **********Train Epoch 467: averaged Loss: 32.995540\n",
      "2025-03-07 23:13: **********Val Epoch 467: average Loss: 2115.001465\n",
      "2025-03-07 23:13: Train Epoch 468: 0/23 Loss: 11.978288\n",
      "2025-03-07 23:13: Train Epoch 468: 20/23 Loss: 71.243225\n",
      "2025-03-07 23:13: **********Train Epoch 468: averaged Loss: 29.166283\n",
      "2025-03-07 23:13: **********Val Epoch 468: average Loss: 2119.874023\n",
      "2025-03-07 23:13: Train Epoch 469: 0/23 Loss: 4.841691\n",
      "2025-03-07 23:13: Train Epoch 469: 20/23 Loss: 71.320007\n",
      "2025-03-07 23:13: **********Train Epoch 469: averaged Loss: 28.771127\n",
      "2025-03-07 23:13: **********Val Epoch 469: average Loss: 2135.413818\n",
      "2025-03-07 23:13: Train Epoch 470: 0/23 Loss: 7.169590\n",
      "2025-03-07 23:13: Train Epoch 470: 20/23 Loss: 76.312958\n",
      "2025-03-07 23:13: **********Train Epoch 470: averaged Loss: 40.190769\n",
      "2025-03-07 23:13: **********Val Epoch 470: average Loss: 2124.416992\n",
      "2025-03-07 23:13: Train Epoch 471: 0/23 Loss: 8.025912\n",
      "2025-03-07 23:13: Train Epoch 471: 20/23 Loss: 64.344810\n",
      "2025-03-07 23:13: **********Train Epoch 471: averaged Loss: 29.496274\n",
      "2025-03-07 23:13: **********Val Epoch 471: average Loss: 2100.566406\n",
      "2025-03-07 23:13: *********************************Current best model saved!\n",
      "2025-03-07 23:13: Train Epoch 472: 0/23 Loss: 10.526841\n",
      "2025-03-07 23:13: Train Epoch 472: 20/23 Loss: 60.790794\n",
      "2025-03-07 23:13: **********Train Epoch 472: averaged Loss: 26.344935\n",
      "2025-03-07 23:13: **********Val Epoch 472: average Loss: 2122.795044\n",
      "2025-03-07 23:13: Train Epoch 473: 0/23 Loss: 0.934509\n",
      "2025-03-07 23:13: Train Epoch 473: 20/23 Loss: 55.494888\n",
      "2025-03-07 23:13: **********Train Epoch 473: averaged Loss: 26.106538\n",
      "2025-03-07 23:13: **********Val Epoch 473: average Loss: 2149.852234\n",
      "2025-03-07 23:13: Train Epoch 474: 0/23 Loss: 5.097872\n",
      "2025-03-07 23:13: Train Epoch 474: 20/23 Loss: 70.791603\n",
      "2025-03-07 23:13: **********Train Epoch 474: averaged Loss: 28.348390\n",
      "2025-03-07 23:13: **********Val Epoch 474: average Loss: 2106.900146\n",
      "2025-03-07 23:13: Train Epoch 475: 0/23 Loss: 3.194577\n",
      "2025-03-07 23:13: Train Epoch 475: 20/23 Loss: 65.789398\n",
      "2025-03-07 23:13: **********Train Epoch 475: averaged Loss: 29.630937\n",
      "2025-03-07 23:13: **********Val Epoch 475: average Loss: 2071.099243\n",
      "2025-03-07 23:13: *********************************Current best model saved!\n",
      "2025-03-07 23:13: Train Epoch 476: 0/23 Loss: 5.119787\n",
      "2025-03-07 23:13: Train Epoch 476: 20/23 Loss: 54.523254\n",
      "2025-03-07 23:13: **********Train Epoch 476: averaged Loss: 23.244342\n",
      "2025-03-07 23:13: **********Val Epoch 476: average Loss: 2128.095459\n",
      "2025-03-07 23:13: Train Epoch 477: 0/23 Loss: 2.322562\n",
      "2025-03-07 23:13: Train Epoch 477: 20/23 Loss: 53.704716\n",
      "2025-03-07 23:13: **********Train Epoch 477: averaged Loss: 25.460238\n",
      "2025-03-07 23:13: **********Val Epoch 477: average Loss: 2107.928101\n",
      "2025-03-07 23:13: Train Epoch 478: 0/23 Loss: 2.281265\n",
      "2025-03-07 23:13: Train Epoch 478: 20/23 Loss: 88.858551\n",
      "2025-03-07 23:13: **********Train Epoch 478: averaged Loss: 32.922435\n",
      "2025-03-07 23:13: **********Val Epoch 478: average Loss: 2081.808472\n",
      "2025-03-07 23:13: Train Epoch 479: 0/23 Loss: 4.691560\n",
      "2025-03-07 23:13: Train Epoch 479: 20/23 Loss: 70.161247\n",
      "2025-03-07 23:13: **********Train Epoch 479: averaged Loss: 30.287691\n",
      "2025-03-07 23:13: **********Val Epoch 479: average Loss: 2134.980164\n",
      "2025-03-07 23:13: Train Epoch 480: 0/23 Loss: 12.292009\n",
      "2025-03-07 23:13: Train Epoch 480: 20/23 Loss: 68.579262\n",
      "2025-03-07 23:13: **********Train Epoch 480: averaged Loss: 30.397738\n",
      "2025-03-07 23:13: **********Val Epoch 480: average Loss: 2076.438354\n",
      "2025-03-07 23:13: Train Epoch 481: 0/23 Loss: 1.363238\n",
      "2025-03-07 23:13: Train Epoch 481: 20/23 Loss: 63.366287\n",
      "2025-03-07 23:13: **********Train Epoch 481: averaged Loss: 31.610148\n",
      "2025-03-07 23:13: **********Val Epoch 481: average Loss: 2088.661377\n",
      "2025-03-07 23:13: Train Epoch 482: 0/23 Loss: 12.779696\n",
      "2025-03-07 23:13: Train Epoch 482: 20/23 Loss: 97.802124\n",
      "2025-03-07 23:13: **********Train Epoch 482: averaged Loss: 37.293607\n",
      "2025-03-07 23:13: **********Val Epoch 482: average Loss: 2108.487854\n",
      "2025-03-07 23:13: Train Epoch 483: 0/23 Loss: 4.530369\n",
      "2025-03-07 23:13: Train Epoch 483: 20/23 Loss: 67.513695\n",
      "2025-03-07 23:13: **********Train Epoch 483: averaged Loss: 31.487154\n",
      "2025-03-07 23:13: **********Val Epoch 483: average Loss: 2107.055176\n",
      "2025-03-07 23:13: Train Epoch 484: 0/23 Loss: 15.227618\n",
      "2025-03-07 23:13: Train Epoch 484: 20/23 Loss: 88.034325\n",
      "2025-03-07 23:13: **********Train Epoch 484: averaged Loss: 29.753784\n",
      "2025-03-07 23:13: **********Val Epoch 484: average Loss: 2098.533936\n",
      "2025-03-07 23:13: Train Epoch 485: 0/23 Loss: 9.714935\n",
      "2025-03-07 23:13: Train Epoch 485: 20/23 Loss: 56.007893\n",
      "2025-03-07 23:13: **********Train Epoch 485: averaged Loss: 26.294895\n",
      "2025-03-07 23:13: **********Val Epoch 485: average Loss: 2087.843140\n",
      "2025-03-07 23:13: Train Epoch 486: 0/23 Loss: 10.990957\n",
      "2025-03-07 23:13: Train Epoch 486: 20/23 Loss: 71.474640\n",
      "2025-03-07 23:13: **********Train Epoch 486: averaged Loss: 33.466595\n",
      "2025-03-07 23:13: **********Val Epoch 486: average Loss: 2088.202637\n",
      "2025-03-07 23:13: Train Epoch 487: 0/23 Loss: 1.433344\n",
      "2025-03-07 23:13: Train Epoch 487: 20/23 Loss: 92.554916\n",
      "2025-03-07 23:13: **********Train Epoch 487: averaged Loss: 26.561161\n",
      "2025-03-07 23:13: **********Val Epoch 487: average Loss: 2073.374390\n",
      "2025-03-07 23:13: Train Epoch 488: 0/23 Loss: 5.631249\n",
      "2025-03-07 23:13: Train Epoch 488: 20/23 Loss: 59.780910\n",
      "2025-03-07 23:13: **********Train Epoch 488: averaged Loss: 25.661288\n",
      "2025-03-07 23:13: **********Val Epoch 488: average Loss: 2086.314819\n",
      "2025-03-07 23:13: Train Epoch 489: 0/23 Loss: 2.933509\n",
      "2025-03-07 23:13: Train Epoch 489: 20/23 Loss: 78.111580\n",
      "2025-03-07 23:13: **********Train Epoch 489: averaged Loss: 30.929863\n",
      "2025-03-07 23:13: **********Val Epoch 489: average Loss: 2085.227966\n",
      "2025-03-07 23:13: Train Epoch 490: 0/23 Loss: 0.997693\n",
      "2025-03-07 23:13: Train Epoch 490: 20/23 Loss: 62.239578\n",
      "2025-03-07 23:13: **********Train Epoch 490: averaged Loss: 26.954083\n",
      "2025-03-07 23:13: **********Val Epoch 490: average Loss: 2091.599731\n",
      "2025-03-07 23:13: Train Epoch 491: 0/23 Loss: 5.381192\n",
      "2025-03-07 23:14: Train Epoch 491: 20/23 Loss: 102.101997\n",
      "2025-03-07 23:14: **********Train Epoch 491: averaged Loss: 32.230154\n",
      "2025-03-07 23:14: **********Val Epoch 491: average Loss: 2078.773071\n",
      "2025-03-07 23:14: Train Epoch 492: 0/23 Loss: 4.463895\n",
      "2025-03-07 23:14: Train Epoch 492: 20/23 Loss: 61.850361\n",
      "2025-03-07 23:14: **********Train Epoch 492: averaged Loss: 28.679394\n",
      "2025-03-07 23:14: **********Val Epoch 492: average Loss: 2070.944458\n",
      "2025-03-07 23:14: *********************************Current best model saved!\n",
      "2025-03-07 23:14: Train Epoch 493: 0/23 Loss: 10.811446\n",
      "2025-03-07 23:14: Train Epoch 493: 20/23 Loss: 129.132446\n",
      "2025-03-07 23:14: **********Train Epoch 493: averaged Loss: 35.508243\n",
      "2025-03-07 23:14: **********Val Epoch 493: average Loss: 2121.226562\n",
      "2025-03-07 23:14: Train Epoch 494: 0/23 Loss: 4.693677\n",
      "2025-03-07 23:14: Train Epoch 494: 20/23 Loss: 112.293526\n",
      "2025-03-07 23:14: **********Train Epoch 494: averaged Loss: 33.429987\n",
      "2025-03-07 23:14: **********Val Epoch 494: average Loss: 2058.487366\n",
      "2025-03-07 23:14: *********************************Current best model saved!\n",
      "2025-03-07 23:14: Train Epoch 495: 0/23 Loss: 17.974632\n",
      "2025-03-07 23:14: Train Epoch 495: 20/23 Loss: 113.468750\n",
      "2025-03-07 23:14: **********Train Epoch 495: averaged Loss: 36.765822\n",
      "2025-03-07 23:14: **********Val Epoch 495: average Loss: 2111.989990\n",
      "2025-03-07 23:14: Train Epoch 496: 0/23 Loss: 5.906640\n",
      "2025-03-07 23:14: Train Epoch 496: 20/23 Loss: 76.997330\n",
      "2025-03-07 23:14: **********Train Epoch 496: averaged Loss: 28.031016\n",
      "2025-03-07 23:14: **********Val Epoch 496: average Loss: 2089.666748\n",
      "2025-03-07 23:14: Train Epoch 497: 0/23 Loss: 12.382128\n",
      "2025-03-07 23:14: Train Epoch 497: 20/23 Loss: 87.538277\n",
      "2025-03-07 23:14: **********Train Epoch 497: averaged Loss: 29.748660\n",
      "2025-03-07 23:14: **********Val Epoch 497: average Loss: 2110.096924\n",
      "2025-03-07 23:14: Train Epoch 498: 0/23 Loss: 3.532134\n",
      "2025-03-07 23:14: Train Epoch 498: 20/23 Loss: 62.931110\n",
      "2025-03-07 23:14: **********Train Epoch 498: averaged Loss: 29.878213\n",
      "2025-03-07 23:14: **********Val Epoch 498: average Loss: 2078.417175\n",
      "2025-03-07 23:14: Train Epoch 499: 0/23 Loss: 12.704370\n",
      "2025-03-07 23:14: Train Epoch 499: 20/23 Loss: 69.419952\n",
      "2025-03-07 23:14: **********Train Epoch 499: averaged Loss: 33.416170\n",
      "2025-03-07 23:14: **********Val Epoch 499: average Loss: 2089.777710\n",
      "2025-03-07 23:14: Train Epoch 500: 0/23 Loss: 5.512794\n",
      "2025-03-07 23:14: Train Epoch 500: 20/23 Loss: 61.099838\n",
      "2025-03-07 23:14: **********Train Epoch 500: averaged Loss: 25.181250\n",
      "2025-03-07 23:14: **********Val Epoch 500: average Loss: 2093.204834\n",
      "2025-03-07 23:14: Train Epoch 501: 0/23 Loss: 8.610035\n",
      "2025-03-07 23:14: Train Epoch 501: 20/23 Loss: 60.608284\n",
      "2025-03-07 23:14: **********Train Epoch 501: averaged Loss: 27.384640\n",
      "2025-03-07 23:14: **********Val Epoch 501: average Loss: 2071.323364\n",
      "2025-03-07 23:14: Train Epoch 502: 0/23 Loss: 2.612224\n",
      "2025-03-07 23:14: Train Epoch 502: 20/23 Loss: 58.796577\n",
      "2025-03-07 23:14: **********Train Epoch 502: averaged Loss: 28.153025\n",
      "2025-03-07 23:14: **********Val Epoch 502: average Loss: 2085.403809\n",
      "2025-03-07 23:14: Train Epoch 503: 0/23 Loss: 1.405140\n",
      "2025-03-07 23:14: Train Epoch 503: 20/23 Loss: 60.088257\n",
      "2025-03-07 23:14: **********Train Epoch 503: averaged Loss: 26.043085\n",
      "2025-03-07 23:14: **********Val Epoch 503: average Loss: 2068.648193\n",
      "2025-03-07 23:14: Train Epoch 504: 0/23 Loss: 7.895955\n",
      "2025-03-07 23:14: Train Epoch 504: 20/23 Loss: 57.754513\n",
      "2025-03-07 23:14: **********Train Epoch 504: averaged Loss: 28.235228\n",
      "2025-03-07 23:14: **********Val Epoch 504: average Loss: 2073.244019\n",
      "2025-03-07 23:14: Train Epoch 505: 0/23 Loss: 8.013698\n",
      "2025-03-07 23:14: Train Epoch 505: 20/23 Loss: 83.273956\n",
      "2025-03-07 23:14: **********Train Epoch 505: averaged Loss: 35.473669\n",
      "2025-03-07 23:14: **********Val Epoch 505: average Loss: 2066.905640\n",
      "2025-03-07 23:14: Train Epoch 506: 0/23 Loss: 10.569466\n",
      "2025-03-07 23:14: Train Epoch 506: 20/23 Loss: 104.652702\n",
      "2025-03-07 23:14: **********Train Epoch 506: averaged Loss: 36.271024\n",
      "2025-03-07 23:14: **********Val Epoch 506: average Loss: 2058.127319\n",
      "2025-03-07 23:14: *********************************Current best model saved!\n",
      "2025-03-07 23:14: Train Epoch 507: 0/23 Loss: 1.063635\n",
      "2025-03-07 23:14: Train Epoch 507: 20/23 Loss: 72.142654\n",
      "2025-03-07 23:14: **********Train Epoch 507: averaged Loss: 30.658354\n",
      "2025-03-07 23:14: **********Val Epoch 507: average Loss: 2065.365601\n",
      "2025-03-07 23:14: Train Epoch 508: 0/23 Loss: 10.702833\n",
      "2025-03-07 23:14: Train Epoch 508: 20/23 Loss: 76.424629\n",
      "2025-03-07 23:14: **********Train Epoch 508: averaged Loss: 27.784610\n",
      "2025-03-07 23:14: **********Val Epoch 508: average Loss: 2097.358521\n",
      "2025-03-07 23:14: Train Epoch 509: 0/23 Loss: 8.981615\n",
      "2025-03-07 23:14: Train Epoch 509: 20/23 Loss: 59.761230\n",
      "2025-03-07 23:14: **********Train Epoch 509: averaged Loss: 26.812980\n",
      "2025-03-07 23:14: **********Val Epoch 509: average Loss: 2040.018982\n",
      "2025-03-07 23:14: *********************************Current best model saved!\n",
      "2025-03-07 23:14: Train Epoch 510: 0/23 Loss: 11.663542\n",
      "2025-03-07 23:14: Train Epoch 510: 20/23 Loss: 80.418556\n",
      "2025-03-07 23:14: **********Train Epoch 510: averaged Loss: 30.263134\n",
      "2025-03-07 23:14: **********Val Epoch 510: average Loss: 2076.638916\n",
      "2025-03-07 23:14: Train Epoch 511: 0/23 Loss: 7.077954\n",
      "2025-03-07 23:14: Train Epoch 511: 20/23 Loss: 58.735607\n",
      "2025-03-07 23:14: **********Train Epoch 511: averaged Loss: 24.452728\n",
      "2025-03-07 23:14: **********Val Epoch 511: average Loss: 2058.758240\n",
      "2025-03-07 23:14: Train Epoch 512: 0/23 Loss: 7.747991\n",
      "2025-03-07 23:14: Train Epoch 512: 20/23 Loss: 55.829197\n",
      "2025-03-07 23:14: **********Train Epoch 512: averaged Loss: 24.380345\n",
      "2025-03-07 23:14: **********Val Epoch 512: average Loss: 2085.001770\n",
      "2025-03-07 23:14: Train Epoch 513: 0/23 Loss: 3.940033\n",
      "2025-03-07 23:14: Train Epoch 513: 20/23 Loss: 72.779663\n",
      "2025-03-07 23:14: **********Train Epoch 513: averaged Loss: 24.682361\n",
      "2025-03-07 23:14: **********Val Epoch 513: average Loss: 2056.091003\n",
      "2025-03-07 23:14: Train Epoch 514: 0/23 Loss: 5.686293\n",
      "2025-03-07 23:14: Train Epoch 514: 20/23 Loss: 65.471939\n",
      "2025-03-07 23:14: **********Train Epoch 514: averaged Loss: 27.406283\n",
      "2025-03-07 23:14: **********Val Epoch 514: average Loss: 2093.211304\n",
      "2025-03-07 23:14: Train Epoch 515: 0/23 Loss: 7.530365\n",
      "2025-03-07 23:14: Train Epoch 515: 20/23 Loss: 79.037323\n",
      "2025-03-07 23:14: **********Train Epoch 515: averaged Loss: 26.509166\n",
      "2025-03-07 23:14: **********Val Epoch 515: average Loss: 2094.805908\n",
      "2025-03-07 23:14: Train Epoch 516: 0/23 Loss: 5.103012\n",
      "2025-03-07 23:14: Train Epoch 516: 20/23 Loss: 72.231430\n",
      "2025-03-07 23:14: **********Train Epoch 516: averaged Loss: 29.484974\n",
      "2025-03-07 23:14: **********Val Epoch 516: average Loss: 2068.609802\n",
      "2025-03-07 23:14: Train Epoch 517: 0/23 Loss: 4.939594\n",
      "2025-03-07 23:14: Train Epoch 517: 20/23 Loss: 98.547623\n",
      "2025-03-07 23:14: **********Train Epoch 517: averaged Loss: 26.024010\n",
      "2025-03-07 23:14: **********Val Epoch 517: average Loss: 2080.235352\n",
      "2025-03-07 23:14: Train Epoch 518: 0/23 Loss: 5.440056\n",
      "2025-03-07 23:14: Train Epoch 518: 20/23 Loss: 74.829697\n",
      "2025-03-07 23:14: **********Train Epoch 518: averaged Loss: 25.666802\n",
      "2025-03-07 23:14: **********Val Epoch 518: average Loss: 2031.332886\n",
      "2025-03-07 23:14: *********************************Current best model saved!\n",
      "2025-03-07 23:14: Train Epoch 519: 0/23 Loss: 2.268401\n",
      "2025-03-07 23:14: Train Epoch 519: 20/23 Loss: 134.592438\n",
      "2025-03-07 23:14: **********Train Epoch 519: averaged Loss: 36.602287\n",
      "2025-03-07 23:14: **********Val Epoch 519: average Loss: 2049.638550\n",
      "2025-03-07 23:14: Train Epoch 520: 0/23 Loss: 6.968698\n",
      "2025-03-07 23:14: Train Epoch 520: 20/23 Loss: 118.881416\n",
      "2025-03-07 23:14: **********Train Epoch 520: averaged Loss: 29.537265\n",
      "2025-03-07 23:14: **********Val Epoch 520: average Loss: 2046.704102\n",
      "2025-03-07 23:14: Train Epoch 521: 0/23 Loss: 7.997339\n",
      "2025-03-07 23:14: Train Epoch 521: 20/23 Loss: 101.270332\n",
      "2025-03-07 23:14: **********Train Epoch 521: averaged Loss: 26.131911\n",
      "2025-03-07 23:14: **********Val Epoch 521: average Loss: 2103.897827\n",
      "2025-03-07 23:14: Train Epoch 522: 0/23 Loss: 3.877245\n",
      "2025-03-07 23:14: Train Epoch 522: 20/23 Loss: 59.048218\n",
      "2025-03-07 23:14: **********Train Epoch 522: averaged Loss: 23.556601\n",
      "2025-03-07 23:14: **********Val Epoch 522: average Loss: 2061.786255\n",
      "2025-03-07 23:14: Train Epoch 523: 0/23 Loss: 12.631277\n",
      "2025-03-07 23:14: Train Epoch 523: 20/23 Loss: 72.096603\n",
      "2025-03-07 23:14: **********Train Epoch 523: averaged Loss: 32.487049\n",
      "2025-03-07 23:14: **********Val Epoch 523: average Loss: 2048.950806\n",
      "2025-03-07 23:14: Train Epoch 524: 0/23 Loss: 6.180286\n",
      "2025-03-07 23:14: Train Epoch 524: 20/23 Loss: 66.678062\n",
      "2025-03-07 23:14: **********Train Epoch 524: averaged Loss: 24.140847\n",
      "2025-03-07 23:14: **********Val Epoch 524: average Loss: 2004.487732\n",
      "2025-03-07 23:14: *********************************Current best model saved!\n",
      "2025-03-07 23:14: Train Epoch 525: 0/23 Loss: 11.933926\n",
      "2025-03-07 23:14: Train Epoch 525: 20/23 Loss: 52.307541\n",
      "2025-03-07 23:14: **********Train Epoch 525: averaged Loss: 23.374085\n",
      "2025-03-07 23:14: **********Val Epoch 525: average Loss: 2045.052490\n",
      "2025-03-07 23:14: Train Epoch 526: 0/23 Loss: 6.922956\n",
      "2025-03-07 23:15: Train Epoch 526: 20/23 Loss: 85.948853\n",
      "2025-03-07 23:15: **********Train Epoch 526: averaged Loss: 23.945918\n",
      "2025-03-07 23:15: **********Val Epoch 526: average Loss: 2026.091187\n",
      "2025-03-07 23:15: Train Epoch 527: 0/23 Loss: 9.214682\n",
      "2025-03-07 23:15: Train Epoch 527: 20/23 Loss: 63.105988\n",
      "2025-03-07 23:15: **********Train Epoch 527: averaged Loss: 26.421337\n",
      "2025-03-07 23:15: **********Val Epoch 527: average Loss: 2084.556274\n",
      "2025-03-07 23:15: Train Epoch 528: 0/23 Loss: 2.817234\n",
      "2025-03-07 23:15: Train Epoch 528: 20/23 Loss: 64.227402\n",
      "2025-03-07 23:15: **********Train Epoch 528: averaged Loss: 22.917034\n",
      "2025-03-07 23:15: **********Val Epoch 528: average Loss: 2054.156555\n",
      "2025-03-07 23:15: Train Epoch 529: 0/23 Loss: 8.072430\n",
      "2025-03-07 23:15: Train Epoch 529: 20/23 Loss: 77.958160\n",
      "2025-03-07 23:15: **********Train Epoch 529: averaged Loss: 25.629183\n",
      "2025-03-07 23:15: **********Val Epoch 529: average Loss: 2055.454712\n",
      "2025-03-07 23:15: Train Epoch 530: 0/23 Loss: 4.389797\n",
      "2025-03-07 23:15: Train Epoch 530: 20/23 Loss: 55.656189\n",
      "2025-03-07 23:15: **********Train Epoch 530: averaged Loss: 23.691860\n",
      "2025-03-07 23:15: **********Val Epoch 530: average Loss: 2037.469788\n",
      "2025-03-07 23:15: Train Epoch 531: 0/23 Loss: 14.320348\n",
      "2025-03-07 23:15: Train Epoch 531: 20/23 Loss: 61.299282\n",
      "2025-03-07 23:15: **********Train Epoch 531: averaged Loss: 25.936134\n",
      "2025-03-07 23:15: **********Val Epoch 531: average Loss: 2016.839478\n",
      "2025-03-07 23:15: Train Epoch 532: 0/23 Loss: 4.337015\n",
      "2025-03-07 23:15: Train Epoch 532: 20/23 Loss: 62.834995\n",
      "2025-03-07 23:15: **********Train Epoch 532: averaged Loss: 23.093943\n",
      "2025-03-07 23:15: **********Val Epoch 532: average Loss: 1988.607300\n",
      "2025-03-07 23:15: *********************************Current best model saved!\n",
      "2025-03-07 23:15: Train Epoch 533: 0/23 Loss: 10.062494\n",
      "2025-03-07 23:15: Train Epoch 533: 20/23 Loss: 66.022949\n",
      "2025-03-07 23:15: **********Train Epoch 533: averaged Loss: 24.634838\n",
      "2025-03-07 23:15: **********Val Epoch 533: average Loss: 2027.589294\n",
      "2025-03-07 23:15: Train Epoch 534: 0/23 Loss: 0.781851\n",
      "2025-03-07 23:15: Train Epoch 534: 20/23 Loss: 67.734863\n",
      "2025-03-07 23:15: **********Train Epoch 534: averaged Loss: 22.840330\n",
      "2025-03-07 23:15: **********Val Epoch 534: average Loss: 2018.475098\n",
      "2025-03-07 23:15: Train Epoch 535: 0/23 Loss: 8.752048\n",
      "2025-03-07 23:15: Train Epoch 535: 20/23 Loss: 94.203514\n",
      "2025-03-07 23:15: **********Train Epoch 535: averaged Loss: 28.575103\n",
      "2025-03-07 23:15: **********Val Epoch 535: average Loss: 2017.792847\n",
      "2025-03-07 23:15: Train Epoch 536: 0/23 Loss: 5.584568\n",
      "2025-03-07 23:15: Train Epoch 536: 20/23 Loss: 76.326241\n",
      "2025-03-07 23:15: **********Train Epoch 536: averaged Loss: 29.750601\n",
      "2025-03-07 23:15: **********Val Epoch 536: average Loss: 1995.555908\n",
      "2025-03-07 23:15: Train Epoch 537: 0/23 Loss: 14.802578\n",
      "2025-03-07 23:15: Train Epoch 537: 20/23 Loss: 103.341202\n",
      "2025-03-07 23:15: **********Train Epoch 537: averaged Loss: 31.508013\n",
      "2025-03-07 23:15: **********Val Epoch 537: average Loss: 2060.272095\n",
      "2025-03-07 23:15: Train Epoch 538: 0/23 Loss: 2.859145\n",
      "2025-03-07 23:15: Train Epoch 538: 20/23 Loss: 67.361389\n",
      "2025-03-07 23:15: **********Train Epoch 538: averaged Loss: 25.863543\n",
      "2025-03-07 23:15: **********Val Epoch 538: average Loss: 1969.925903\n",
      "2025-03-07 23:15: *********************************Current best model saved!\n",
      "2025-03-07 23:15: Train Epoch 539: 0/23 Loss: 10.799648\n",
      "2025-03-07 23:15: Train Epoch 539: 20/23 Loss: 53.172943\n",
      "2025-03-07 23:15: **********Train Epoch 539: averaged Loss: 32.296952\n",
      "2025-03-07 23:15: **********Val Epoch 539: average Loss: 2040.060364\n",
      "2025-03-07 23:15: Train Epoch 540: 0/23 Loss: 6.683305\n",
      "2025-03-07 23:15: Train Epoch 540: 20/23 Loss: 68.584480\n",
      "2025-03-07 23:15: **********Train Epoch 540: averaged Loss: 23.896847\n",
      "2025-03-07 23:15: **********Val Epoch 540: average Loss: 2016.936890\n",
      "2025-03-07 23:15: Train Epoch 541: 0/23 Loss: 14.308843\n",
      "2025-03-07 23:15: Train Epoch 541: 20/23 Loss: 71.091064\n",
      "2025-03-07 23:15: **********Train Epoch 541: averaged Loss: 28.460716\n",
      "2025-03-07 23:15: **********Val Epoch 541: average Loss: 2040.509155\n",
      "2025-03-07 23:15: Train Epoch 542: 0/23 Loss: 10.567820\n",
      "2025-03-07 23:15: Train Epoch 542: 20/23 Loss: 59.802124\n",
      "2025-03-07 23:15: **********Train Epoch 542: averaged Loss: 23.152226\n",
      "2025-03-07 23:15: **********Val Epoch 542: average Loss: 2016.696777\n",
      "2025-03-07 23:15: Train Epoch 543: 0/23 Loss: 5.614979\n",
      "2025-03-07 23:15: Train Epoch 543: 20/23 Loss: 54.086441\n",
      "2025-03-07 23:15: **********Train Epoch 543: averaged Loss: 23.467598\n",
      "2025-03-07 23:15: **********Val Epoch 543: average Loss: 2005.790283\n",
      "2025-03-07 23:15: Train Epoch 544: 0/23 Loss: 11.910410\n",
      "2025-03-07 23:15: Train Epoch 544: 20/23 Loss: 60.593933\n",
      "2025-03-07 23:15: **********Train Epoch 544: averaged Loss: 23.896145\n",
      "2025-03-07 23:15: **********Val Epoch 544: average Loss: 2003.275574\n",
      "2025-03-07 23:15: Train Epoch 545: 0/23 Loss: 8.424794\n",
      "2025-03-07 23:15: Train Epoch 545: 20/23 Loss: 70.101089\n",
      "2025-03-07 23:15: **********Train Epoch 545: averaged Loss: 28.665994\n",
      "2025-03-07 23:15: **********Val Epoch 545: average Loss: 1994.103394\n",
      "2025-03-07 23:15: Train Epoch 546: 0/23 Loss: 9.319217\n",
      "2025-03-07 23:15: Train Epoch 546: 20/23 Loss: 55.121315\n",
      "2025-03-07 23:15: **********Train Epoch 546: averaged Loss: 21.534648\n",
      "2025-03-07 23:15: **********Val Epoch 546: average Loss: 1987.276123\n",
      "2025-03-07 23:15: Train Epoch 547: 0/23 Loss: 17.888065\n",
      "2025-03-07 23:15: Train Epoch 547: 20/23 Loss: 60.686764\n",
      "2025-03-07 23:15: **********Train Epoch 547: averaged Loss: 23.052483\n",
      "2025-03-07 23:15: **********Val Epoch 547: average Loss: 1958.365051\n",
      "2025-03-07 23:15: *********************************Current best model saved!\n",
      "2025-03-07 23:15: Train Epoch 548: 0/23 Loss: 1.247044\n",
      "2025-03-07 23:15: Train Epoch 548: 20/23 Loss: 98.465851\n",
      "2025-03-07 23:15: **********Train Epoch 548: averaged Loss: 24.840184\n",
      "2025-03-07 23:15: **********Val Epoch 548: average Loss: 2023.236328\n",
      "2025-03-07 23:15: Train Epoch 549: 0/23 Loss: 5.728395\n",
      "2025-03-07 23:15: Train Epoch 549: 20/23 Loss: 54.737640\n",
      "2025-03-07 23:15: **********Train Epoch 549: averaged Loss: 19.404294\n",
      "2025-03-07 23:15: **********Val Epoch 549: average Loss: 1959.044250\n",
      "2025-03-07 23:15: Train Epoch 550: 0/23 Loss: 6.194153\n",
      "2025-03-07 23:15: Train Epoch 550: 20/23 Loss: 74.502205\n",
      "2025-03-07 23:15: **********Train Epoch 550: averaged Loss: 20.879572\n",
      "2025-03-07 23:15: **********Val Epoch 550: average Loss: 1964.636963\n",
      "2025-03-07 23:15: Train Epoch 551: 0/23 Loss: 3.977318\n",
      "2025-03-07 23:15: Train Epoch 551: 20/23 Loss: 52.873024\n",
      "2025-03-07 23:15: **********Train Epoch 551: averaged Loss: 19.972394\n",
      "2025-03-07 23:15: **********Val Epoch 551: average Loss: 2004.978516\n",
      "2025-03-07 23:15: Train Epoch 552: 0/23 Loss: 3.318828\n",
      "2025-03-07 23:15: Train Epoch 552: 20/23 Loss: 57.767334\n",
      "2025-03-07 23:15: **********Train Epoch 552: averaged Loss: 17.988276\n",
      "2025-03-07 23:15: **********Val Epoch 552: average Loss: 1965.086792\n",
      "2025-03-07 23:15: Train Epoch 553: 0/23 Loss: 7.148578\n",
      "2025-03-07 23:15: Train Epoch 553: 20/23 Loss: 54.263496\n",
      "2025-03-07 23:15: **********Train Epoch 553: averaged Loss: 19.058929\n",
      "2025-03-07 23:15: **********Val Epoch 553: average Loss: 1982.061401\n",
      "2025-03-07 23:15: Train Epoch 554: 0/23 Loss: 8.810128\n",
      "2025-03-07 23:15: Train Epoch 554: 20/23 Loss: 83.012390\n",
      "2025-03-07 23:15: **********Train Epoch 554: averaged Loss: 22.879140\n",
      "2025-03-07 23:15: **********Val Epoch 554: average Loss: 1981.427124\n",
      "2025-03-07 23:15: Train Epoch 555: 0/23 Loss: 2.409562\n",
      "2025-03-07 23:15: Train Epoch 555: 20/23 Loss: 68.710930\n",
      "2025-03-07 23:15: **********Train Epoch 555: averaged Loss: 25.665265\n",
      "2025-03-07 23:15: **********Val Epoch 555: average Loss: 1966.591858\n",
      "2025-03-07 23:15: Train Epoch 556: 0/23 Loss: 5.332997\n",
      "2025-03-07 23:15: Train Epoch 556: 20/23 Loss: 94.404831\n",
      "2025-03-07 23:15: **********Train Epoch 556: averaged Loss: 23.695111\n",
      "2025-03-07 23:15: **********Val Epoch 556: average Loss: 2003.860107\n",
      "2025-03-07 23:15: Train Epoch 557: 0/23 Loss: 4.560169\n",
      "2025-03-07 23:15: Train Epoch 557: 20/23 Loss: 56.228306\n",
      "2025-03-07 23:15: **********Train Epoch 557: averaged Loss: 19.784420\n",
      "2025-03-07 23:15: **********Val Epoch 557: average Loss: 1950.187622\n",
      "2025-03-07 23:15: *********************************Current best model saved!\n",
      "2025-03-07 23:15: Train Epoch 558: 0/23 Loss: 1.634295\n",
      "2025-03-07 23:15: Train Epoch 558: 20/23 Loss: 104.442886\n",
      "2025-03-07 23:15: **********Train Epoch 558: averaged Loss: 25.242218\n",
      "2025-03-07 23:15: **********Val Epoch 558: average Loss: 1973.414429\n",
      "2025-03-07 23:15: Train Epoch 559: 0/23 Loss: 7.562210\n",
      "2025-03-07 23:15: Train Epoch 559: 20/23 Loss: 104.001999\n",
      "2025-03-07 23:15: **********Train Epoch 559: averaged Loss: 26.734276\n",
      "2025-03-07 23:15: **********Val Epoch 559: average Loss: 1964.114868\n",
      "2025-03-07 23:15: Train Epoch 560: 0/23 Loss: 13.451242\n",
      "2025-03-07 23:15: Train Epoch 560: 20/23 Loss: 77.617325\n",
      "2025-03-07 23:15: **********Train Epoch 560: averaged Loss: 31.758650\n",
      "2025-03-07 23:15: **********Val Epoch 560: average Loss: 1978.818604\n",
      "2025-03-07 23:15: Train Epoch 561: 0/23 Loss: 9.786617\n",
      "2025-03-07 23:16: Train Epoch 561: 20/23 Loss: 59.945007\n",
      "2025-03-07 23:16: **********Train Epoch 561: averaged Loss: 23.198331\n",
      "2025-03-07 23:16: **********Val Epoch 561: average Loss: 1930.932068\n",
      "2025-03-07 23:16: *********************************Current best model saved!\n",
      "2025-03-07 23:16: Train Epoch 562: 0/23 Loss: 15.552881\n",
      "2025-03-07 23:16: Train Epoch 562: 20/23 Loss: 66.337021\n",
      "2025-03-07 23:16: **********Train Epoch 562: averaged Loss: 25.135505\n",
      "2025-03-07 23:16: **********Val Epoch 562: average Loss: 1960.064209\n",
      "2025-03-07 23:16: Train Epoch 563: 0/23 Loss: 3.748698\n",
      "2025-03-07 23:16: Train Epoch 563: 20/23 Loss: 75.717003\n",
      "2025-03-07 23:16: **********Train Epoch 563: averaged Loss: 25.530262\n",
      "2025-03-07 23:16: **********Val Epoch 563: average Loss: 1926.726379\n",
      "2025-03-07 23:16: *********************************Current best model saved!\n",
      "2025-03-07 23:16: Train Epoch 564: 0/23 Loss: 14.079267\n",
      "2025-03-07 23:16: Train Epoch 564: 20/23 Loss: 85.174515\n",
      "2025-03-07 23:16: **********Train Epoch 564: averaged Loss: 34.255825\n",
      "2025-03-07 23:16: **********Val Epoch 564: average Loss: 1991.958130\n",
      "2025-03-07 23:16: Train Epoch 565: 0/23 Loss: 2.668741\n",
      "2025-03-07 23:16: Train Epoch 565: 20/23 Loss: 70.572083\n",
      "2025-03-07 23:16: **********Train Epoch 565: averaged Loss: 27.083829\n",
      "2025-03-07 23:16: **********Val Epoch 565: average Loss: 1920.972046\n",
      "2025-03-07 23:16: *********************************Current best model saved!\n",
      "2025-03-07 23:16: Train Epoch 566: 0/23 Loss: 11.033122\n",
      "2025-03-07 23:16: Train Epoch 566: 20/23 Loss: 100.760345\n",
      "2025-03-07 23:16: **********Train Epoch 566: averaged Loss: 31.176017\n",
      "2025-03-07 23:16: **********Val Epoch 566: average Loss: 1977.840088\n",
      "2025-03-07 23:16: Train Epoch 567: 0/23 Loss: 4.633850\n",
      "2025-03-07 23:16: Train Epoch 567: 20/23 Loss: 81.250336\n",
      "2025-03-07 23:16: **********Train Epoch 567: averaged Loss: 29.314687\n",
      "2025-03-07 23:16: **********Val Epoch 567: average Loss: 1939.577698\n",
      "2025-03-07 23:16: Train Epoch 568: 0/23 Loss: 7.218040\n",
      "2025-03-07 23:16: Train Epoch 568: 20/23 Loss: 76.154915\n",
      "2025-03-07 23:16: **********Train Epoch 568: averaged Loss: 28.570207\n",
      "2025-03-07 23:16: **********Val Epoch 568: average Loss: 1966.599060\n",
      "2025-03-07 23:16: Train Epoch 569: 0/23 Loss: 9.602977\n",
      "2025-03-07 23:16: Train Epoch 569: 20/23 Loss: 65.228333\n",
      "2025-03-07 23:16: **********Train Epoch 569: averaged Loss: 25.967834\n",
      "2025-03-07 23:16: **********Val Epoch 569: average Loss: 1940.802307\n",
      "2025-03-07 23:16: Train Epoch 570: 0/23 Loss: 7.189521\n",
      "2025-03-07 23:16: Train Epoch 570: 20/23 Loss: 76.884666\n",
      "2025-03-07 23:16: **********Train Epoch 570: averaged Loss: 22.686856\n",
      "2025-03-07 23:16: **********Val Epoch 570: average Loss: 1965.514038\n",
      "2025-03-07 23:16: Train Epoch 571: 0/23 Loss: 3.654577\n",
      "2025-03-07 23:16: Train Epoch 571: 20/23 Loss: 66.951149\n",
      "2025-03-07 23:16: **********Train Epoch 571: averaged Loss: 21.503622\n",
      "2025-03-07 23:16: **********Val Epoch 571: average Loss: 1979.176514\n",
      "2025-03-07 23:16: Train Epoch 572: 0/23 Loss: 14.696891\n",
      "2025-03-07 23:16: Train Epoch 572: 20/23 Loss: 100.834198\n",
      "2025-03-07 23:16: **********Train Epoch 572: averaged Loss: 25.793605\n",
      "2025-03-07 23:16: **********Val Epoch 572: average Loss: 1984.469238\n",
      "2025-03-07 23:16: Train Epoch 573: 0/23 Loss: 5.873637\n",
      "2025-03-07 23:16: Train Epoch 573: 20/23 Loss: 87.730309\n",
      "2025-03-07 23:16: **********Train Epoch 573: averaged Loss: 23.191431\n",
      "2025-03-07 23:16: **********Val Epoch 573: average Loss: 1961.358215\n",
      "2025-03-07 23:16: Train Epoch 574: 0/23 Loss: 10.070055\n",
      "2025-03-07 23:16: Train Epoch 574: 20/23 Loss: 77.158630\n",
      "2025-03-07 23:16: **********Train Epoch 574: averaged Loss: 23.192635\n",
      "2025-03-07 23:16: **********Val Epoch 574: average Loss: 1924.603638\n",
      "2025-03-07 23:16: Train Epoch 575: 0/23 Loss: 7.243101\n",
      "2025-03-07 23:16: Train Epoch 575: 20/23 Loss: 69.932709\n",
      "2025-03-07 23:16: **********Train Epoch 575: averaged Loss: 22.203737\n",
      "2025-03-07 23:16: **********Val Epoch 575: average Loss: 1975.073364\n",
      "2025-03-07 23:16: Train Epoch 576: 0/23 Loss: 9.887484\n",
      "2025-03-07 23:16: Train Epoch 576: 20/23 Loss: 79.729385\n",
      "2025-03-07 23:16: **********Train Epoch 576: averaged Loss: 27.481790\n",
      "2025-03-07 23:16: **********Val Epoch 576: average Loss: 1986.427002\n",
      "2025-03-07 23:16: Train Epoch 577: 0/23 Loss: 3.984039\n",
      "2025-03-07 23:16: Train Epoch 577: 20/23 Loss: 78.029411\n",
      "2025-03-07 23:16: **********Train Epoch 577: averaged Loss: 27.406565\n",
      "2025-03-07 23:16: **********Val Epoch 577: average Loss: 1934.763428\n",
      "2025-03-07 23:16: Train Epoch 578: 0/23 Loss: 12.353689\n",
      "2025-03-07 23:16: Train Epoch 578: 20/23 Loss: 82.069092\n",
      "2025-03-07 23:16: **********Train Epoch 578: averaged Loss: 29.289778\n",
      "2025-03-07 23:16: **********Val Epoch 578: average Loss: 1973.372620\n",
      "2025-03-07 23:16: Train Epoch 579: 0/23 Loss: 7.402558\n",
      "2025-03-07 23:16: Train Epoch 579: 20/23 Loss: 60.001728\n",
      "2025-03-07 23:16: **********Train Epoch 579: averaged Loss: 27.481745\n",
      "2025-03-07 23:16: **********Val Epoch 579: average Loss: 1946.712036\n",
      "2025-03-07 23:16: Train Epoch 580: 0/23 Loss: 12.087440\n",
      "2025-03-07 23:16: Train Epoch 580: 20/23 Loss: 73.883057\n",
      "2025-03-07 23:16: **********Train Epoch 580: averaged Loss: 24.875945\n",
      "2025-03-07 23:16: **********Val Epoch 580: average Loss: 1946.044312\n",
      "2025-03-07 23:16: Train Epoch 581: 0/23 Loss: 8.819942\n",
      "2025-03-07 23:16: Train Epoch 581: 20/23 Loss: 73.162552\n",
      "2025-03-07 23:16: **********Train Epoch 581: averaged Loss: 23.254981\n",
      "2025-03-07 23:16: **********Val Epoch 581: average Loss: 1960.789307\n",
      "2025-03-07 23:16: Train Epoch 582: 0/23 Loss: 12.921980\n",
      "2025-03-07 23:16: Train Epoch 582: 20/23 Loss: 56.963943\n",
      "2025-03-07 23:16: **********Train Epoch 582: averaged Loss: 27.797679\n",
      "2025-03-07 23:16: **********Val Epoch 582: average Loss: 1939.758911\n",
      "2025-03-07 23:16: Train Epoch 583: 0/23 Loss: 7.016313\n",
      "2025-03-07 23:16: Train Epoch 583: 20/23 Loss: 67.914642\n",
      "2025-03-07 23:16: **********Train Epoch 583: averaged Loss: 21.842200\n",
      "2025-03-07 23:16: **********Val Epoch 583: average Loss: 1927.637329\n",
      "2025-03-07 23:16: Train Epoch 584: 0/23 Loss: 13.190093\n",
      "2025-03-07 23:16: Train Epoch 584: 20/23 Loss: 77.102066\n",
      "2025-03-07 23:16: **********Train Epoch 584: averaged Loss: 25.991034\n",
      "2025-03-07 23:16: **********Val Epoch 584: average Loss: 1955.389526\n",
      "2025-03-07 23:16: Train Epoch 585: 0/23 Loss: 5.566570\n",
      "2025-03-07 23:16: Train Epoch 585: 20/23 Loss: 64.585510\n",
      "2025-03-07 23:16: **********Train Epoch 585: averaged Loss: 21.758694\n",
      "2025-03-07 23:16: **********Val Epoch 585: average Loss: 1912.312866\n",
      "2025-03-07 23:16: *********************************Current best model saved!\n",
      "2025-03-07 23:16: Train Epoch 586: 0/23 Loss: 7.278531\n",
      "2025-03-07 23:16: Train Epoch 586: 20/23 Loss: 47.741570\n",
      "2025-03-07 23:16: **********Train Epoch 586: averaged Loss: 25.792121\n",
      "2025-03-07 23:16: **********Val Epoch 586: average Loss: 1970.428040\n",
      "2025-03-07 23:16: Train Epoch 587: 0/23 Loss: 1.393856\n",
      "2025-03-07 23:16: Train Epoch 587: 20/23 Loss: 66.130753\n",
      "2025-03-07 23:16: **********Train Epoch 587: averaged Loss: 20.579505\n",
      "2025-03-07 23:16: **********Val Epoch 587: average Loss: 1902.959290\n",
      "2025-03-07 23:16: *********************************Current best model saved!\n",
      "2025-03-07 23:16: Train Epoch 588: 0/23 Loss: 5.689718\n",
      "2025-03-07 23:16: Train Epoch 588: 20/23 Loss: 45.235641\n",
      "2025-03-07 23:16: **********Train Epoch 588: averaged Loss: 20.323628\n",
      "2025-03-07 23:16: **********Val Epoch 588: average Loss: 1956.740417\n",
      "2025-03-07 23:16: Train Epoch 589: 0/23 Loss: 6.276495\n",
      "2025-03-07 23:16: Train Epoch 589: 20/23 Loss: 75.558548\n",
      "2025-03-07 23:16: **********Train Epoch 589: averaged Loss: 18.023525\n",
      "2025-03-07 23:16: **********Val Epoch 589: average Loss: 1932.481384\n",
      "2025-03-07 23:16: Train Epoch 590: 0/23 Loss: 3.707099\n",
      "2025-03-07 23:16: Train Epoch 590: 20/23 Loss: 54.132812\n",
      "2025-03-07 23:16: **********Train Epoch 590: averaged Loss: 20.288188\n",
      "2025-03-07 23:16: **********Val Epoch 590: average Loss: 1925.875793\n",
      "2025-03-07 23:16: Train Epoch 591: 0/23 Loss: 7.211803\n",
      "2025-03-07 23:16: Train Epoch 591: 20/23 Loss: 100.460358\n",
      "2025-03-07 23:16: **********Train Epoch 591: averaged Loss: 26.275454\n",
      "2025-03-07 23:16: **********Val Epoch 591: average Loss: 1926.161865\n",
      "2025-03-07 23:16: Train Epoch 592: 0/23 Loss: 1.069694\n",
      "2025-03-07 23:16: Train Epoch 592: 20/23 Loss: 68.270607\n",
      "2025-03-07 23:16: **********Train Epoch 592: averaged Loss: 21.247202\n",
      "2025-03-07 23:16: **********Val Epoch 592: average Loss: 1930.384766\n",
      "2025-03-07 23:16: Train Epoch 593: 0/23 Loss: 10.210112\n",
      "2025-03-07 23:16: Train Epoch 593: 20/23 Loss: 67.775909\n",
      "2025-03-07 23:16: **********Train Epoch 593: averaged Loss: 23.432312\n",
      "2025-03-07 23:16: **********Val Epoch 593: average Loss: 1880.816711\n",
      "2025-03-07 23:16: *********************************Current best model saved!\n",
      "2025-03-07 23:16: Train Epoch 594: 0/23 Loss: 4.813991\n",
      "2025-03-07 23:16: Train Epoch 594: 20/23 Loss: 64.208755\n",
      "2025-03-07 23:16: **********Train Epoch 594: averaged Loss: 20.858519\n",
      "2025-03-07 23:16: **********Val Epoch 594: average Loss: 1923.290161\n",
      "2025-03-07 23:16: Train Epoch 595: 0/23 Loss: 11.366438\n",
      "2025-03-07 23:16: Train Epoch 595: 20/23 Loss: 74.046822\n",
      "2025-03-07 23:16: **********Train Epoch 595: averaged Loss: 24.623670\n",
      "2025-03-07 23:16: **********Val Epoch 595: average Loss: 1930.947937\n",
      "2025-03-07 23:16: Train Epoch 596: 0/23 Loss: 1.994392\n",
      "2025-03-07 23:16: Train Epoch 596: 20/23 Loss: 92.043167\n",
      "2025-03-07 23:16: **********Train Epoch 596: averaged Loss: 32.720148\n",
      "2025-03-07 23:16: **********Val Epoch 596: average Loss: 1942.161133\n",
      "2025-03-07 23:16: Train Epoch 597: 0/23 Loss: 20.990177\n",
      "2025-03-07 23:17: Train Epoch 597: 20/23 Loss: 90.672302\n",
      "2025-03-07 23:17: **********Train Epoch 597: averaged Loss: 39.562969\n",
      "2025-03-07 23:17: **********Val Epoch 597: average Loss: 1912.121155\n",
      "2025-03-07 23:17: Train Epoch 598: 0/23 Loss: 6.376866\n",
      "2025-03-07 23:17: Train Epoch 598: 20/23 Loss: 70.340553\n",
      "2025-03-07 23:17: **********Train Epoch 598: averaged Loss: 27.571692\n",
      "2025-03-07 23:17: **********Val Epoch 598: average Loss: 1898.512939\n",
      "2025-03-07 23:17: Train Epoch 599: 0/23 Loss: 6.595366\n",
      "2025-03-07 23:17: Train Epoch 599: 20/23 Loss: 52.815495\n",
      "2025-03-07 23:17: **********Train Epoch 599: averaged Loss: 21.964518\n",
      "2025-03-07 23:17: **********Val Epoch 599: average Loss: 1905.160034\n",
      "2025-03-07 23:17: Train Epoch 600: 0/23 Loss: 8.394272\n",
      "2025-03-07 23:17: Train Epoch 600: 20/23 Loss: 52.668770\n",
      "2025-03-07 23:17: **********Train Epoch 600: averaged Loss: 23.080800\n",
      "2025-03-07 23:17: **********Val Epoch 600: average Loss: 1891.237305\n",
      "2025-03-07 23:17: Train Epoch 601: 0/23 Loss: 11.342676\n",
      "2025-03-07 23:17: Train Epoch 601: 20/23 Loss: 74.827713\n",
      "2025-03-07 23:17: **********Train Epoch 601: averaged Loss: 24.519894\n",
      "2025-03-07 23:17: **********Val Epoch 601: average Loss: 1933.399170\n",
      "2025-03-07 23:17: Train Epoch 602: 0/23 Loss: 5.641481\n",
      "2025-03-07 23:17: Train Epoch 602: 20/23 Loss: 85.123894\n",
      "2025-03-07 23:17: **********Train Epoch 602: averaged Loss: 28.323046\n",
      "2025-03-07 23:17: **********Val Epoch 602: average Loss: 1921.921570\n",
      "2025-03-07 23:17: Train Epoch 603: 0/23 Loss: 13.874648\n",
      "2025-03-07 23:17: Train Epoch 603: 20/23 Loss: 51.624310\n",
      "2025-03-07 23:17: **********Train Epoch 603: averaged Loss: 23.563114\n",
      "2025-03-07 23:17: **********Val Epoch 603: average Loss: 1943.121094\n",
      "2025-03-07 23:17: Train Epoch 604: 0/23 Loss: 0.908621\n",
      "2025-03-07 23:17: Train Epoch 604: 20/23 Loss: 63.681171\n",
      "2025-03-07 23:17: **********Train Epoch 604: averaged Loss: 19.848879\n",
      "2025-03-07 23:17: **********Val Epoch 604: average Loss: 1956.894043\n",
      "2025-03-07 23:17: Train Epoch 605: 0/23 Loss: 13.659443\n",
      "2025-03-07 23:17: Train Epoch 605: 20/23 Loss: 68.621063\n",
      "2025-03-07 23:17: **********Train Epoch 605: averaged Loss: 21.319890\n",
      "2025-03-07 23:17: **********Val Epoch 605: average Loss: 1920.175232\n",
      "2025-03-07 23:17: Train Epoch 606: 0/23 Loss: 1.025288\n",
      "2025-03-07 23:17: Train Epoch 606: 20/23 Loss: 51.469196\n",
      "2025-03-07 23:17: **********Train Epoch 606: averaged Loss: 19.953749\n",
      "2025-03-07 23:17: **********Val Epoch 606: average Loss: 1929.643677\n",
      "2025-03-07 23:17: Train Epoch 607: 0/23 Loss: 11.386913\n",
      "2025-03-07 23:17: Train Epoch 607: 20/23 Loss: 48.528214\n",
      "2025-03-07 23:17: **********Train Epoch 607: averaged Loss: 21.303513\n",
      "2025-03-07 23:17: **********Val Epoch 607: average Loss: 1935.608276\n",
      "2025-03-07 23:17: Train Epoch 608: 0/23 Loss: 3.529737\n",
      "2025-03-07 23:17: Train Epoch 608: 20/23 Loss: 71.212700\n",
      "2025-03-07 23:17: **********Train Epoch 608: averaged Loss: 20.015059\n",
      "2025-03-07 23:17: **********Val Epoch 608: average Loss: 1949.507019\n",
      "2025-03-07 23:17: Train Epoch 609: 0/23 Loss: 15.150847\n",
      "2025-03-07 23:17: Train Epoch 609: 20/23 Loss: 52.579235\n",
      "2025-03-07 23:17: **********Train Epoch 609: averaged Loss: 20.069040\n",
      "2025-03-07 23:17: **********Val Epoch 609: average Loss: 1941.210693\n",
      "2025-03-07 23:17: Train Epoch 610: 0/23 Loss: 6.349159\n",
      "2025-03-07 23:17: Train Epoch 610: 20/23 Loss: 67.514168\n",
      "2025-03-07 23:17: **********Train Epoch 610: averaged Loss: 19.874850\n",
      "2025-03-07 23:17: **********Val Epoch 610: average Loss: 1920.334106\n",
      "2025-03-07 23:17: Train Epoch 611: 0/23 Loss: 0.990271\n",
      "2025-03-07 23:17: Train Epoch 611: 20/23 Loss: 45.668175\n",
      "2025-03-07 23:17: **********Train Epoch 611: averaged Loss: 22.236255\n",
      "2025-03-07 23:17: **********Val Epoch 611: average Loss: 1880.592407\n",
      "2025-03-07 23:17: *********************************Current best model saved!\n",
      "2025-03-07 23:17: Train Epoch 612: 0/23 Loss: 6.000324\n",
      "2025-03-07 23:17: Train Epoch 612: 20/23 Loss: 75.213783\n",
      "2025-03-07 23:17: **********Train Epoch 612: averaged Loss: 21.477472\n",
      "2025-03-07 23:17: **********Val Epoch 612: average Loss: 1890.432983\n",
      "2025-03-07 23:17: Train Epoch 613: 0/23 Loss: 7.714007\n",
      "2025-03-07 23:17: Train Epoch 613: 20/23 Loss: 51.040588\n",
      "2025-03-07 23:17: **********Train Epoch 613: averaged Loss: 22.551329\n",
      "2025-03-07 23:17: **********Val Epoch 613: average Loss: 1908.667725\n",
      "2025-03-07 23:17: Train Epoch 614: 0/23 Loss: 10.362060\n",
      "2025-03-07 23:17: Train Epoch 614: 20/23 Loss: 77.260162\n",
      "2025-03-07 23:17: **********Train Epoch 614: averaged Loss: 20.746411\n",
      "2025-03-07 23:17: **********Val Epoch 614: average Loss: 1896.228699\n",
      "2025-03-07 23:17: Train Epoch 615: 0/23 Loss: 4.112833\n",
      "2025-03-07 23:17: Train Epoch 615: 20/23 Loss: 52.241493\n",
      "2025-03-07 23:17: **********Train Epoch 615: averaged Loss: 17.761820\n",
      "2025-03-07 23:17: **********Val Epoch 615: average Loss: 1885.976868\n",
      "2025-03-07 23:17: Train Epoch 616: 0/23 Loss: 7.579409\n",
      "2025-03-07 23:17: Train Epoch 616: 20/23 Loss: 68.474701\n",
      "2025-03-07 23:17: **********Train Epoch 616: averaged Loss: 17.536017\n",
      "2025-03-07 23:17: **********Val Epoch 616: average Loss: 1928.219360\n",
      "2025-03-07 23:17: Train Epoch 617: 0/23 Loss: 2.685709\n",
      "2025-03-07 23:17: Train Epoch 617: 20/23 Loss: 53.076599\n",
      "2025-03-07 23:17: **********Train Epoch 617: averaged Loss: 18.397848\n",
      "2025-03-07 23:17: **********Val Epoch 617: average Loss: 1924.805176\n",
      "2025-03-07 23:17: Train Epoch 618: 0/23 Loss: 0.987513\n",
      "2025-03-07 23:17: Train Epoch 618: 20/23 Loss: 85.894119\n",
      "2025-03-07 23:17: **********Train Epoch 618: averaged Loss: 23.635185\n",
      "2025-03-07 23:17: **********Val Epoch 618: average Loss: 1927.031616\n",
      "2025-03-07 23:17: Train Epoch 619: 0/23 Loss: 4.236564\n",
      "2025-03-07 23:17: Train Epoch 619: 20/23 Loss: 61.727547\n",
      "2025-03-07 23:17: **********Train Epoch 619: averaged Loss: 26.529825\n",
      "2025-03-07 23:17: **********Val Epoch 619: average Loss: 1891.459290\n",
      "2025-03-07 23:17: Train Epoch 620: 0/23 Loss: 9.742123\n",
      "2025-03-07 23:17: Train Epoch 620: 20/23 Loss: 45.679932\n",
      "2025-03-07 23:17: **********Train Epoch 620: averaged Loss: 24.152922\n",
      "2025-03-07 23:17: **********Val Epoch 620: average Loss: 1893.125549\n",
      "2025-03-07 23:17: Train Epoch 621: 0/23 Loss: 7.632132\n",
      "2025-03-07 23:17: Train Epoch 621: 20/23 Loss: 60.676445\n",
      "2025-03-07 23:17: **********Train Epoch 621: averaged Loss: 20.813325\n",
      "2025-03-07 23:17: **********Val Epoch 621: average Loss: 1886.410400\n",
      "2025-03-07 23:17: Train Epoch 622: 0/23 Loss: 8.001097\n",
      "2025-03-07 23:17: Train Epoch 622: 20/23 Loss: 52.619625\n",
      "2025-03-07 23:17: **********Train Epoch 622: averaged Loss: 22.055281\n",
      "2025-03-07 23:17: **********Val Epoch 622: average Loss: 1911.999634\n",
      "2025-03-07 23:17: Train Epoch 623: 0/23 Loss: 5.007482\n",
      "2025-03-07 23:17: Train Epoch 623: 20/23 Loss: 43.636749\n",
      "2025-03-07 23:17: **********Train Epoch 623: averaged Loss: 17.522065\n",
      "2025-03-07 23:17: **********Val Epoch 623: average Loss: 1883.592651\n",
      "2025-03-07 23:17: Train Epoch 624: 0/23 Loss: 8.778788\n",
      "2025-03-07 23:17: Train Epoch 624: 20/23 Loss: 76.427231\n",
      "2025-03-07 23:17: **********Train Epoch 624: averaged Loss: 19.992344\n",
      "2025-03-07 23:17: **********Val Epoch 624: average Loss: 1925.464233\n",
      "2025-03-07 23:17: Train Epoch 625: 0/23 Loss: 7.040321\n",
      "2025-03-07 23:17: Train Epoch 625: 20/23 Loss: 69.826279\n",
      "2025-03-07 23:17: **********Train Epoch 625: averaged Loss: 21.229548\n",
      "2025-03-07 23:17: **********Val Epoch 625: average Loss: 1883.712341\n",
      "2025-03-07 23:17: Train Epoch 626: 0/23 Loss: 4.412379\n",
      "2025-03-07 23:17: Train Epoch 626: 20/23 Loss: 55.515171\n",
      "2025-03-07 23:17: **********Train Epoch 626: averaged Loss: 17.568661\n",
      "2025-03-07 23:17: **********Val Epoch 626: average Loss: 1878.985962\n",
      "2025-03-07 23:17: *********************************Current best model saved!\n",
      "2025-03-07 23:17: Train Epoch 627: 0/23 Loss: 1.390388\n",
      "2025-03-07 23:17: Train Epoch 627: 20/23 Loss: 47.667782\n",
      "2025-03-07 23:17: **********Train Epoch 627: averaged Loss: 19.054306\n",
      "2025-03-07 23:17: **********Val Epoch 627: average Loss: 1909.079102\n",
      "2025-03-07 23:17: Train Epoch 628: 0/23 Loss: 13.961115\n",
      "2025-03-07 23:17: Train Epoch 628: 20/23 Loss: 73.086159\n",
      "2025-03-07 23:17: **********Train Epoch 628: averaged Loss: 29.117226\n",
      "2025-03-07 23:17: **********Val Epoch 628: average Loss: 1891.124878\n",
      "2025-03-07 23:17: Train Epoch 629: 0/23 Loss: 10.072678\n",
      "2025-03-07 23:17: Train Epoch 629: 20/23 Loss: 61.461624\n",
      "2025-03-07 23:17: **********Train Epoch 629: averaged Loss: 26.397280\n",
      "2025-03-07 23:17: **********Val Epoch 629: average Loss: 1877.559814\n",
      "2025-03-07 23:17: *********************************Current best model saved!\n",
      "2025-03-07 23:17: Train Epoch 630: 0/23 Loss: 9.199709\n",
      "2025-03-07 23:17: Train Epoch 630: 20/23 Loss: 47.367989\n",
      "2025-03-07 23:17: **********Train Epoch 630: averaged Loss: 24.781232\n",
      "2025-03-07 23:17: **********Val Epoch 630: average Loss: 1902.992310\n",
      "2025-03-07 23:17: Train Epoch 631: 0/23 Loss: 3.721946\n",
      "2025-03-07 23:17: Train Epoch 631: 20/23 Loss: 54.038269\n",
      "2025-03-07 23:17: **********Train Epoch 631: averaged Loss: 21.120974\n",
      "2025-03-07 23:17: **********Val Epoch 631: average Loss: 1887.300354\n",
      "2025-03-07 23:17: Train Epoch 632: 0/23 Loss: 15.494473\n",
      "2025-03-07 23:18: Train Epoch 632: 20/23 Loss: 59.787056\n",
      "2025-03-07 23:18: **********Train Epoch 632: averaged Loss: 23.493681\n",
      "2025-03-07 23:18: **********Val Epoch 632: average Loss: 1927.145020\n",
      "2025-03-07 23:18: Train Epoch 633: 0/23 Loss: 7.692987\n",
      "2025-03-07 23:18: Train Epoch 633: 20/23 Loss: 90.708511\n",
      "2025-03-07 23:18: **********Train Epoch 633: averaged Loss: 22.292715\n",
      "2025-03-07 23:18: **********Val Epoch 633: average Loss: 1915.859131\n",
      "2025-03-07 23:18: Train Epoch 634: 0/23 Loss: 14.641047\n",
      "2025-03-07 23:18: Train Epoch 634: 20/23 Loss: 72.999252\n",
      "2025-03-07 23:18: **********Train Epoch 634: averaged Loss: 21.789606\n",
      "2025-03-07 23:18: **********Val Epoch 634: average Loss: 1902.942749\n",
      "2025-03-07 23:18: Train Epoch 635: 0/23 Loss: 5.109413\n",
      "2025-03-07 23:18: Train Epoch 635: 20/23 Loss: 48.870872\n",
      "2025-03-07 23:18: **********Train Epoch 635: averaged Loss: 17.470822\n",
      "2025-03-07 23:18: **********Val Epoch 635: average Loss: 1887.524963\n",
      "2025-03-07 23:18: Train Epoch 636: 0/23 Loss: 12.508696\n",
      "2025-03-07 23:18: Train Epoch 636: 20/23 Loss: 40.536781\n",
      "2025-03-07 23:18: **********Train Epoch 636: averaged Loss: 17.322632\n",
      "2025-03-07 23:18: **********Val Epoch 636: average Loss: 1856.613098\n",
      "2025-03-07 23:18: *********************************Current best model saved!\n",
      "2025-03-07 23:18: Train Epoch 637: 0/23 Loss: 5.942401\n",
      "2025-03-07 23:18: Train Epoch 637: 20/23 Loss: 56.039589\n",
      "2025-03-07 23:18: **********Train Epoch 637: averaged Loss: 20.002684\n",
      "2025-03-07 23:18: **********Val Epoch 637: average Loss: 1852.656982\n",
      "2025-03-07 23:18: *********************************Current best model saved!\n",
      "2025-03-07 23:18: Train Epoch 638: 0/23 Loss: 9.982163\n",
      "2025-03-07 23:18: Train Epoch 638: 20/23 Loss: 47.199345\n",
      "2025-03-07 23:18: **********Train Epoch 638: averaged Loss: 26.440163\n",
      "2025-03-07 23:18: **********Val Epoch 638: average Loss: 1896.349548\n",
      "2025-03-07 23:18: Train Epoch 639: 0/23 Loss: 2.602399\n",
      "2025-03-07 23:18: Train Epoch 639: 20/23 Loss: 71.206390\n",
      "2025-03-07 23:18: **********Train Epoch 639: averaged Loss: 25.020084\n",
      "2025-03-07 23:18: **********Val Epoch 639: average Loss: 1850.174133\n",
      "2025-03-07 23:18: *********************************Current best model saved!\n",
      "2025-03-07 23:18: Train Epoch 640: 0/23 Loss: 13.848110\n",
      "2025-03-07 23:18: Train Epoch 640: 20/23 Loss: 60.385498\n",
      "2025-03-07 23:18: **********Train Epoch 640: averaged Loss: 27.771213\n",
      "2025-03-07 23:18: **********Val Epoch 640: average Loss: 1923.357544\n",
      "2025-03-07 23:18: Train Epoch 641: 0/23 Loss: 5.484258\n",
      "2025-03-07 23:18: Train Epoch 641: 20/23 Loss: 60.013443\n",
      "2025-03-07 23:18: **********Train Epoch 641: averaged Loss: 21.017812\n",
      "2025-03-07 23:18: **********Val Epoch 641: average Loss: 1907.983765\n",
      "2025-03-07 23:18: Train Epoch 642: 0/23 Loss: 6.388656\n",
      "2025-03-07 23:18: Train Epoch 642: 20/23 Loss: 47.803692\n",
      "2025-03-07 23:18: **********Train Epoch 642: averaged Loss: 21.444257\n",
      "2025-03-07 23:18: **********Val Epoch 642: average Loss: 1918.258362\n",
      "2025-03-07 23:18: Train Epoch 643: 0/23 Loss: 0.961071\n",
      "2025-03-07 23:18: Train Epoch 643: 20/23 Loss: 61.775070\n",
      "2025-03-07 23:18: **********Train Epoch 643: averaged Loss: 21.534557\n",
      "2025-03-07 23:18: **********Val Epoch 643: average Loss: 1885.598633\n",
      "2025-03-07 23:18: Train Epoch 644: 0/23 Loss: 11.313722\n",
      "2025-03-07 23:18: Train Epoch 644: 20/23 Loss: 80.484680\n",
      "2025-03-07 23:18: **********Train Epoch 644: averaged Loss: 28.987314\n",
      "2025-03-07 23:18: **********Val Epoch 644: average Loss: 1896.420654\n",
      "2025-03-07 23:18: Train Epoch 645: 0/23 Loss: 3.302699\n",
      "2025-03-07 23:18: Train Epoch 645: 20/23 Loss: 57.297325\n",
      "2025-03-07 23:18: **********Train Epoch 645: averaged Loss: 25.546568\n",
      "2025-03-07 23:18: **********Val Epoch 645: average Loss: 1802.794403\n",
      "2025-03-07 23:18: *********************************Current best model saved!\n",
      "2025-03-07 23:18: Train Epoch 646: 0/23 Loss: 8.904131\n",
      "2025-03-07 23:18: Train Epoch 646: 20/23 Loss: 73.196518\n",
      "2025-03-07 23:18: **********Train Epoch 646: averaged Loss: 21.904449\n",
      "2025-03-07 23:18: **********Val Epoch 646: average Loss: 1918.714600\n",
      "2025-03-07 23:18: Train Epoch 647: 0/23 Loss: 2.966663\n",
      "2025-03-07 23:18: Train Epoch 647: 20/23 Loss: 54.608845\n",
      "2025-03-07 23:18: **********Train Epoch 647: averaged Loss: 19.790413\n",
      "2025-03-07 23:18: **********Val Epoch 647: average Loss: 1873.032227\n",
      "2025-03-07 23:18: Train Epoch 648: 0/23 Loss: 0.925020\n",
      "2025-03-07 23:18: Train Epoch 648: 20/23 Loss: 52.621635\n",
      "2025-03-07 23:18: **********Train Epoch 648: averaged Loss: 17.836111\n",
      "2025-03-07 23:18: **********Val Epoch 648: average Loss: 1885.541199\n",
      "2025-03-07 23:18: Train Epoch 649: 0/23 Loss: 2.194207\n",
      "2025-03-07 23:18: Train Epoch 649: 20/23 Loss: 46.029327\n",
      "2025-03-07 23:18: **********Train Epoch 649: averaged Loss: 17.076003\n",
      "2025-03-07 23:18: **********Val Epoch 649: average Loss: 1895.006531\n",
      "2025-03-07 23:18: Train Epoch 650: 0/23 Loss: 0.971587\n",
      "2025-03-07 23:18: Train Epoch 650: 20/23 Loss: 39.584080\n",
      "2025-03-07 23:18: **********Train Epoch 650: averaged Loss: 16.290801\n",
      "2025-03-07 23:18: **********Val Epoch 650: average Loss: 1891.196350\n",
      "2025-03-07 23:18: Train Epoch 651: 0/23 Loss: 3.013731\n",
      "2025-03-07 23:18: Train Epoch 651: 20/23 Loss: 43.545437\n",
      "2025-03-07 23:18: **********Train Epoch 651: averaged Loss: 18.336624\n",
      "2025-03-07 23:18: **********Val Epoch 651: average Loss: 1865.448120\n",
      "2025-03-07 23:18: Train Epoch 652: 0/23 Loss: 8.815439\n",
      "2025-03-07 23:18: Train Epoch 652: 20/23 Loss: 58.128063\n",
      "2025-03-07 23:18: **********Train Epoch 652: averaged Loss: 22.775106\n",
      "2025-03-07 23:18: **********Val Epoch 652: average Loss: 1878.038391\n",
      "2025-03-07 23:18: Train Epoch 653: 0/23 Loss: 3.656710\n",
      "2025-03-07 23:18: Train Epoch 653: 20/23 Loss: 64.481224\n",
      "2025-03-07 23:18: **********Train Epoch 653: averaged Loss: 22.847994\n",
      "2025-03-07 23:18: **********Val Epoch 653: average Loss: 1872.963501\n",
      "2025-03-07 23:18: Train Epoch 654: 0/23 Loss: 3.534212\n",
      "2025-03-07 23:18: Train Epoch 654: 20/23 Loss: 52.355938\n",
      "2025-03-07 23:18: **********Train Epoch 654: averaged Loss: 24.467619\n",
      "2025-03-07 23:18: **********Val Epoch 654: average Loss: 1912.277466\n",
      "2025-03-07 23:18: Train Epoch 655: 0/23 Loss: 1.130441\n",
      "2025-03-07 23:18: Train Epoch 655: 20/23 Loss: 65.488647\n",
      "2025-03-07 23:18: **********Train Epoch 655: averaged Loss: 20.790831\n",
      "2025-03-07 23:18: **********Val Epoch 655: average Loss: 1846.607910\n",
      "2025-03-07 23:18: Train Epoch 656: 0/23 Loss: 9.272753\n",
      "2025-03-07 23:18: Train Epoch 656: 20/23 Loss: 49.131680\n",
      "2025-03-07 23:18: **********Train Epoch 656: averaged Loss: 18.860945\n",
      "2025-03-07 23:18: **********Val Epoch 656: average Loss: 1893.067261\n",
      "2025-03-07 23:18: Train Epoch 657: 0/23 Loss: 5.396992\n",
      "2025-03-07 23:18: Train Epoch 657: 20/23 Loss: 61.462296\n",
      "2025-03-07 23:18: **********Train Epoch 657: averaged Loss: 19.213733\n",
      "2025-03-07 23:18: **********Val Epoch 657: average Loss: 1863.065674\n",
      "2025-03-07 23:18: Train Epoch 658: 0/23 Loss: 13.318616\n",
      "2025-03-07 23:18: Train Epoch 658: 20/23 Loss: 42.710781\n",
      "2025-03-07 23:18: **********Train Epoch 658: averaged Loss: 21.036278\n",
      "2025-03-07 23:18: **********Val Epoch 658: average Loss: 1906.388550\n",
      "2025-03-07 23:18: Train Epoch 659: 0/23 Loss: 9.519550\n",
      "2025-03-07 23:18: Train Epoch 659: 20/23 Loss: 75.631622\n",
      "2025-03-07 23:18: **********Train Epoch 659: averaged Loss: 21.740226\n",
      "2025-03-07 23:18: **********Val Epoch 659: average Loss: 1863.745850\n",
      "2025-03-07 23:18: Train Epoch 660: 0/23 Loss: 10.681889\n",
      "2025-03-07 23:18: Train Epoch 660: 20/23 Loss: 44.493454\n",
      "2025-03-07 23:18: **********Train Epoch 660: averaged Loss: 20.019313\n",
      "2025-03-07 23:18: **********Val Epoch 660: average Loss: 1878.538391\n",
      "2025-03-07 23:18: Train Epoch 661: 0/23 Loss: 3.842673\n",
      "2025-03-07 23:18: Train Epoch 661: 20/23 Loss: 56.835186\n",
      "2025-03-07 23:18: **********Train Epoch 661: averaged Loss: 22.483922\n",
      "2025-03-07 23:18: **********Val Epoch 661: average Loss: 1875.839844\n",
      "2025-03-07 23:18: Train Epoch 662: 0/23 Loss: 12.885358\n",
      "2025-03-07 23:18: Train Epoch 662: 20/23 Loss: 60.589584\n",
      "2025-03-07 23:18: **********Train Epoch 662: averaged Loss: 24.320021\n",
      "2025-03-07 23:18: **********Val Epoch 662: average Loss: 1872.950073\n",
      "2025-03-07 23:18: Train Epoch 663: 0/23 Loss: 1.166295\n",
      "2025-03-07 23:18: Train Epoch 663: 20/23 Loss: 74.443192\n",
      "2025-03-07 23:18: **********Train Epoch 663: averaged Loss: 23.295908\n",
      "2025-03-07 23:18: **********Val Epoch 663: average Loss: 1852.917969\n",
      "2025-03-07 23:18: Train Epoch 664: 0/23 Loss: 11.885067\n",
      "2025-03-07 23:18: Train Epoch 664: 20/23 Loss: 71.240494\n",
      "2025-03-07 23:18: **********Train Epoch 664: averaged Loss: 22.573261\n",
      "2025-03-07 23:18: **********Val Epoch 664: average Loss: 1888.909912\n",
      "2025-03-07 23:18: Train Epoch 665: 0/23 Loss: 6.375381\n",
      "2025-03-07 23:18: Train Epoch 665: 20/23 Loss: 64.314804\n",
      "2025-03-07 23:18: **********Train Epoch 665: averaged Loss: 22.755374\n",
      "2025-03-07 23:18: **********Val Epoch 665: average Loss: 1877.567627\n",
      "2025-03-07 23:18: Train Epoch 666: 0/23 Loss: 7.716676\n",
      "2025-03-07 23:18: Train Epoch 666: 20/23 Loss: 50.528717\n",
      "2025-03-07 23:18: **********Train Epoch 666: averaged Loss: 22.475028\n",
      "2025-03-07 23:18: **********Val Epoch 666: average Loss: 1839.665100\n",
      "2025-03-07 23:18: Train Epoch 667: 0/23 Loss: 13.032199\n",
      "2025-03-07 23:18: Train Epoch 667: 20/23 Loss: 47.848289\n",
      "2025-03-07 23:18: **********Train Epoch 667: averaged Loss: 18.856142\n",
      "2025-03-07 23:18: **********Val Epoch 667: average Loss: 1871.534302\n",
      "2025-03-07 23:18: Train Epoch 668: 0/23 Loss: 9.205047\n",
      "2025-03-07 23:19: Train Epoch 668: 20/23 Loss: 40.643990\n",
      "2025-03-07 23:19: **********Train Epoch 668: averaged Loss: 17.893225\n",
      "2025-03-07 23:19: **********Val Epoch 668: average Loss: 1873.771912\n",
      "2025-03-07 23:19: Train Epoch 669: 0/23 Loss: 2.192423\n",
      "2025-03-07 23:19: Train Epoch 669: 20/23 Loss: 61.140274\n",
      "2025-03-07 23:19: **********Train Epoch 669: averaged Loss: 17.147516\n",
      "2025-03-07 23:19: **********Val Epoch 669: average Loss: 1852.723969\n",
      "2025-03-07 23:19: Train Epoch 670: 0/23 Loss: 8.700033\n",
      "2025-03-07 23:19: Train Epoch 670: 20/23 Loss: 43.464302\n",
      "2025-03-07 23:19: **********Train Epoch 670: averaged Loss: 19.499261\n",
      "2025-03-07 23:19: **********Val Epoch 670: average Loss: 1884.982788\n",
      "2025-03-07 23:19: Train Epoch 671: 0/23 Loss: 9.495338\n",
      "2025-03-07 23:19: Train Epoch 671: 20/23 Loss: 84.431320\n",
      "2025-03-07 23:19: **********Train Epoch 671: averaged Loss: 19.899025\n",
      "2025-03-07 23:19: **********Val Epoch 671: average Loss: 1883.280518\n",
      "2025-03-07 23:19: Train Epoch 672: 0/23 Loss: 9.038280\n",
      "2025-03-07 23:19: Train Epoch 672: 20/23 Loss: 39.739700\n",
      "2025-03-07 23:19: **********Train Epoch 672: averaged Loss: 17.147054\n",
      "2025-03-07 23:19: **********Val Epoch 672: average Loss: 1896.941040\n",
      "2025-03-07 23:19: Train Epoch 673: 0/23 Loss: 5.698304\n",
      "2025-03-07 23:19: Train Epoch 673: 20/23 Loss: 56.263702\n",
      "2025-03-07 23:19: **********Train Epoch 673: averaged Loss: 17.886419\n",
      "2025-03-07 23:19: **********Val Epoch 673: average Loss: 1893.874268\n",
      "2025-03-07 23:19: Train Epoch 674: 0/23 Loss: 15.365314\n",
      "2025-03-07 23:19: Train Epoch 674: 20/23 Loss: 65.524490\n",
      "2025-03-07 23:19: **********Train Epoch 674: averaged Loss: 25.947401\n",
      "2025-03-07 23:19: **********Val Epoch 674: average Loss: 1883.858704\n",
      "2025-03-07 23:19: Train Epoch 675: 0/23 Loss: 3.866276\n",
      "2025-03-07 23:19: Train Epoch 675: 20/23 Loss: 55.420769\n",
      "2025-03-07 23:19: **********Train Epoch 675: averaged Loss: 23.257775\n",
      "2025-03-07 23:19: **********Val Epoch 675: average Loss: 1833.335083\n",
      "2025-03-07 23:19: Train Epoch 676: 0/23 Loss: 13.296560\n",
      "2025-03-07 23:19: Train Epoch 676: 20/23 Loss: 47.188194\n",
      "2025-03-07 23:19: **********Train Epoch 676: averaged Loss: 26.305607\n",
      "2025-03-07 23:19: **********Val Epoch 676: average Loss: 1891.171326\n",
      "2025-03-07 23:19: Train Epoch 677: 0/23 Loss: 2.391570\n",
      "2025-03-07 23:19: Train Epoch 677: 20/23 Loss: 46.625038\n",
      "2025-03-07 23:19: **********Train Epoch 677: averaged Loss: 19.723669\n",
      "2025-03-07 23:19: **********Val Epoch 677: average Loss: 1845.240784\n",
      "2025-03-07 23:19: Train Epoch 678: 0/23 Loss: 14.584667\n",
      "2025-03-07 23:19: Train Epoch 678: 20/23 Loss: 59.278442\n",
      "2025-03-07 23:19: **********Train Epoch 678: averaged Loss: 22.577492\n",
      "2025-03-07 23:19: **********Val Epoch 678: average Loss: 1904.368713\n",
      "2025-03-07 23:19: Train Epoch 679: 0/23 Loss: 4.481622\n",
      "2025-03-07 23:19: Train Epoch 679: 20/23 Loss: 56.181526\n",
      "2025-03-07 23:19: **********Train Epoch 679: averaged Loss: 20.190163\n",
      "2025-03-07 23:19: **********Val Epoch 679: average Loss: 1835.616302\n",
      "2025-03-07 23:19: Train Epoch 680: 0/23 Loss: 14.125953\n",
      "2025-03-07 23:19: Train Epoch 680: 20/23 Loss: 52.570992\n",
      "2025-03-07 23:19: **********Train Epoch 680: averaged Loss: 23.666663\n",
      "2025-03-07 23:19: **********Val Epoch 680: average Loss: 1857.365479\n",
      "2025-03-07 23:19: Train Epoch 681: 0/23 Loss: 7.779242\n",
      "2025-03-07 23:19: Train Epoch 681: 20/23 Loss: 48.071350\n",
      "2025-03-07 23:19: **********Train Epoch 681: averaged Loss: 21.458907\n",
      "2025-03-07 23:19: **********Val Epoch 681: average Loss: 1855.573608\n",
      "2025-03-07 23:19: Train Epoch 682: 0/23 Loss: 11.929847\n",
      "2025-03-07 23:19: Train Epoch 682: 20/23 Loss: 39.788918\n",
      "2025-03-07 23:19: **********Train Epoch 682: averaged Loss: 17.261397\n",
      "2025-03-07 23:19: **********Val Epoch 682: average Loss: 1893.685486\n",
      "2025-03-07 23:19: Train Epoch 683: 0/23 Loss: 6.939411\n",
      "2025-03-07 23:19: Train Epoch 683: 20/23 Loss: 38.546833\n",
      "2025-03-07 23:19: **********Train Epoch 683: averaged Loss: 17.194902\n",
      "2025-03-07 23:19: **********Val Epoch 683: average Loss: 1913.716858\n",
      "2025-03-07 23:19: Train Epoch 684: 0/23 Loss: 10.831413\n",
      "2025-03-07 23:19: Train Epoch 684: 20/23 Loss: 50.135929\n",
      "2025-03-07 23:19: **********Train Epoch 684: averaged Loss: 18.154146\n",
      "2025-03-07 23:19: **********Val Epoch 684: average Loss: 1854.299042\n",
      "2025-03-07 23:19: Train Epoch 685: 0/23 Loss: 6.926833\n",
      "2025-03-07 23:19: Train Epoch 685: 20/23 Loss: 45.431190\n",
      "2025-03-07 23:19: **********Train Epoch 685: averaged Loss: 16.158158\n",
      "2025-03-07 23:19: **********Val Epoch 685: average Loss: 1841.134888\n",
      "2025-03-07 23:19: Train Epoch 686: 0/23 Loss: 11.290265\n",
      "2025-03-07 23:19: Train Epoch 686: 20/23 Loss: 49.773994\n",
      "2025-03-07 23:19: **********Train Epoch 686: averaged Loss: 18.524489\n",
      "2025-03-07 23:19: **********Val Epoch 686: average Loss: 1847.875122\n",
      "2025-03-07 23:19: Train Epoch 687: 0/23 Loss: 2.436597\n",
      "2025-03-07 23:19: Train Epoch 687: 20/23 Loss: 45.280369\n",
      "2025-03-07 23:19: **********Train Epoch 687: averaged Loss: 24.397506\n",
      "2025-03-07 23:19: **********Val Epoch 687: average Loss: 1862.482117\n",
      "2025-03-07 23:19: Train Epoch 688: 0/23 Loss: 7.352238\n",
      "2025-03-07 23:19: Train Epoch 688: 20/23 Loss: 66.092606\n",
      "2025-03-07 23:19: **********Train Epoch 688: averaged Loss: 19.685971\n",
      "2025-03-07 23:19: **********Val Epoch 688: average Loss: 1843.765564\n",
      "2025-03-07 23:19: Train Epoch 689: 0/23 Loss: 5.763214\n",
      "2025-03-07 23:19: Train Epoch 689: 20/23 Loss: 77.344208\n",
      "2025-03-07 23:19: **********Train Epoch 689: averaged Loss: 20.401370\n",
      "2025-03-07 23:19: **********Val Epoch 689: average Loss: 1855.038696\n",
      "2025-03-07 23:19: Train Epoch 690: 0/23 Loss: 2.608238\n",
      "2025-03-07 23:19: Train Epoch 690: 20/23 Loss: 50.962448\n",
      "2025-03-07 23:19: **********Train Epoch 690: averaged Loss: 16.943299\n",
      "2025-03-07 23:19: **********Val Epoch 690: average Loss: 1820.965210\n",
      "2025-03-07 23:19: Train Epoch 691: 0/23 Loss: 13.616693\n",
      "2025-03-07 23:19: Train Epoch 691: 20/23 Loss: 69.144760\n",
      "2025-03-07 23:19: **********Train Epoch 691: averaged Loss: 18.322275\n",
      "2025-03-07 23:19: **********Val Epoch 691: average Loss: 1813.597473\n",
      "2025-03-07 23:19: Train Epoch 692: 0/23 Loss: 8.071771\n",
      "2025-03-07 23:19: Train Epoch 692: 20/23 Loss: 44.049706\n",
      "2025-03-07 23:19: **********Train Epoch 692: averaged Loss: 16.279318\n",
      "2025-03-07 23:19: **********Val Epoch 692: average Loss: 1839.825226\n",
      "2025-03-07 23:19: Train Epoch 693: 0/23 Loss: 16.185402\n",
      "2025-03-07 23:19: Train Epoch 693: 20/23 Loss: 56.483849\n",
      "2025-03-07 23:19: **********Train Epoch 693: averaged Loss: 22.295296\n",
      "2025-03-07 23:19: **********Val Epoch 693: average Loss: 1844.740601\n",
      "2025-03-07 23:19: Train Epoch 694: 0/23 Loss: 5.870036\n",
      "2025-03-07 23:19: Train Epoch 694: 20/23 Loss: 97.327827\n",
      "2025-03-07 23:19: **********Train Epoch 694: averaged Loss: 25.263662\n",
      "2025-03-07 23:19: **********Val Epoch 694: average Loss: 1896.657471\n",
      "2025-03-07 23:19: Train Epoch 695: 0/23 Loss: 18.291414\n",
      "2025-03-07 23:19: Train Epoch 695: 20/23 Loss: 69.606689\n",
      "2025-03-07 23:19: **********Train Epoch 695: averaged Loss: 31.728902\n",
      "2025-03-07 23:19: **********Val Epoch 695: average Loss: 1845.474365\n",
      "2025-03-07 23:19: Train Epoch 696: 0/23 Loss: 1.746496\n",
      "2025-03-07 23:19: Train Epoch 696: 20/23 Loss: 70.109138\n",
      "2025-03-07 23:19: **********Train Epoch 696: averaged Loss: 25.061320\n",
      "2025-03-07 23:19: **********Val Epoch 696: average Loss: 1835.392334\n",
      "2025-03-07 23:19: Train Epoch 697: 0/23 Loss: 15.604289\n",
      "2025-03-07 23:19: Train Epoch 697: 20/23 Loss: 42.157345\n",
      "2025-03-07 23:19: **********Train Epoch 697: averaged Loss: 22.472565\n",
      "2025-03-07 23:19: **********Val Epoch 697: average Loss: 1873.161133\n",
      "2025-03-07 23:19: Train Epoch 698: 0/23 Loss: 3.200994\n",
      "2025-03-07 23:19: Train Epoch 698: 20/23 Loss: 53.281990\n",
      "2025-03-07 23:19: **********Train Epoch 698: averaged Loss: 17.561923\n",
      "2025-03-07 23:19: **********Val Epoch 698: average Loss: 1860.499146\n",
      "2025-03-07 23:19: Train Epoch 699: 0/23 Loss: 16.226618\n",
      "2025-03-07 23:19: Train Epoch 699: 20/23 Loss: 44.600098\n",
      "2025-03-07 23:19: **********Train Epoch 699: averaged Loss: 19.534949\n",
      "2025-03-07 23:19: **********Val Epoch 699: average Loss: 1894.518127\n",
      "2025-03-07 23:19: Train Epoch 700: 0/23 Loss: 3.907769\n",
      "2025-03-07 23:19: Train Epoch 700: 20/23 Loss: 41.903854\n",
      "2025-03-07 23:19: **********Train Epoch 700: averaged Loss: 17.175633\n",
      "2025-03-07 23:19: **********Val Epoch 700: average Loss: 1840.520386\n",
      "2025-03-07 23:19: Train Epoch 701: 0/23 Loss: 4.690426\n",
      "2025-03-07 23:19: Train Epoch 701: 20/23 Loss: 40.548405\n",
      "2025-03-07 23:19: **********Train Epoch 701: averaged Loss: 17.323241\n",
      "2025-03-07 23:19: **********Val Epoch 701: average Loss: 1854.765381\n",
      "2025-03-07 23:19: Train Epoch 702: 0/23 Loss: 2.018365\n",
      "2025-03-07 23:19: Train Epoch 702: 20/23 Loss: 41.565933\n",
      "2025-03-07 23:19: **********Train Epoch 702: averaged Loss: 19.667750\n",
      "2025-03-07 23:19: **********Val Epoch 702: average Loss: 1908.994873\n",
      "2025-03-07 23:19: Train Epoch 703: 0/23 Loss: 1.427702\n",
      "2025-03-07 23:20: Train Epoch 703: 20/23 Loss: 57.001068\n",
      "2025-03-07 23:20: **********Train Epoch 703: averaged Loss: 18.604689\n",
      "2025-03-07 23:20: **********Val Epoch 703: average Loss: 1872.675293\n",
      "2025-03-07 23:20: Train Epoch 704: 0/23 Loss: 6.087947\n",
      "2025-03-07 23:20: Train Epoch 704: 20/23 Loss: 38.342384\n",
      "2025-03-07 23:20: **********Train Epoch 704: averaged Loss: 17.847549\n",
      "2025-03-07 23:20: **********Val Epoch 704: average Loss: 1874.424805\n",
      "2025-03-07 23:20: Train Epoch 705: 0/23 Loss: 1.556965\n",
      "2025-03-07 23:20: **********Val Epoch 705: average Loss: 1851.309784\n",
      "2025-03-07 23:20: Train Epoch 706: 0/23 Loss: 10.114959\n",
      "2025-03-07 23:20: Train Epoch 706: 20/23 Loss: 44.786430\n",
      "2025-03-07 23:20: **********Train Epoch 706: averaged Loss: 17.150438\n",
      "2025-03-07 23:20: **********Val Epoch 706: average Loss: 1863.422363\n",
      "2025-03-07 23:20: Train Epoch 707: 0/23 Loss: 0.738847\n",
      "2025-03-07 23:20: Train Epoch 707: 20/23 Loss: 72.598473\n",
      "2025-03-07 23:20: **********Train Epoch 707: averaged Loss: 17.798964\n",
      "2025-03-07 23:20: **********Val Epoch 707: average Loss: 1823.311279\n",
      "2025-03-07 23:20: Train Epoch 708: 0/23 Loss: 2.055201\n",
      "2025-03-07 23:20: Train Epoch 708: 20/23 Loss: 108.588318\n",
      "2025-03-07 23:20: **********Train Epoch 708: averaged Loss: 22.955016\n",
      "2025-03-07 23:20: **********Val Epoch 708: average Loss: 1810.155914\n",
      "2025-03-07 23:20: Train Epoch 709: 0/23 Loss: 7.145941\n",
      "2025-03-07 23:20: Train Epoch 709: 20/23 Loss: 45.261391\n",
      "2025-03-07 23:20: **********Train Epoch 709: averaged Loss: 17.412629\n",
      "2025-03-07 23:20: **********Val Epoch 709: average Loss: 1806.614990\n",
      "2025-03-07 23:20: Train Epoch 710: 0/23 Loss: 11.917438\n",
      "2025-03-07 23:20: Train Epoch 710: 20/23 Loss: 45.832222\n",
      "2025-03-07 23:20: **********Train Epoch 710: averaged Loss: 21.935557\n",
      "2025-03-07 23:20: **********Val Epoch 710: average Loss: 1874.341309\n",
      "2025-03-07 23:20: Train Epoch 711: 0/23 Loss: 8.006170\n",
      "2025-03-07 23:20: Train Epoch 711: 20/23 Loss: 62.013393\n",
      "2025-03-07 23:20: **********Train Epoch 711: averaged Loss: 25.118676\n",
      "2025-03-07 23:20: **********Val Epoch 711: average Loss: 1824.446167\n",
      "2025-03-07 23:20: Train Epoch 712: 0/23 Loss: 11.802725\n",
      "2025-03-07 23:20: Train Epoch 712: 20/23 Loss: 46.643776\n",
      "2025-03-07 23:20: **********Train Epoch 712: averaged Loss: 25.954077\n",
      "2025-03-07 23:20: **********Val Epoch 712: average Loss: 1863.860779\n",
      "2025-03-07 23:20: Train Epoch 713: 0/23 Loss: 6.031255\n",
      "2025-03-07 23:20: Train Epoch 713: 20/23 Loss: 68.236023\n",
      "2025-03-07 23:20: **********Train Epoch 713: averaged Loss: 23.696679\n",
      "2025-03-07 23:20: **********Val Epoch 713: average Loss: 1801.117432\n",
      "2025-03-07 23:20: *********************************Current best model saved!\n",
      "2025-03-07 23:20: Train Epoch 714: 0/23 Loss: 14.452988\n",
      "2025-03-07 23:20: Train Epoch 714: 20/23 Loss: 39.835297\n",
      "2025-03-07 23:20: **********Train Epoch 714: averaged Loss: 23.301641\n",
      "2025-03-07 23:20: **********Val Epoch 714: average Loss: 1887.739258\n",
      "2025-03-07 23:20: Train Epoch 715: 0/23 Loss: 4.268647\n",
      "2025-03-07 23:20: Train Epoch 715: 20/23 Loss: 42.465401\n",
      "2025-03-07 23:20: **********Train Epoch 715: averaged Loss: 17.927696\n",
      "2025-03-07 23:20: **********Val Epoch 715: average Loss: 1828.251923\n",
      "2025-03-07 23:20: Train Epoch 716: 0/23 Loss: 14.360176\n",
      "2025-03-07 23:20: Train Epoch 716: 20/23 Loss: 65.324875\n",
      "2025-03-07 23:20: **********Train Epoch 716: averaged Loss: 21.015967\n",
      "2025-03-07 23:20: **********Val Epoch 716: average Loss: 1836.671936\n",
      "2025-03-07 23:20: Train Epoch 717: 0/23 Loss: 6.774072\n",
      "2025-03-07 23:20: Train Epoch 717: 20/23 Loss: 43.052982\n",
      "2025-03-07 23:20: **********Train Epoch 717: averaged Loss: 19.960058\n",
      "2025-03-07 23:20: **********Val Epoch 717: average Loss: 1837.417877\n",
      "2025-03-07 23:20: Train Epoch 718: 0/23 Loss: 10.094819\n",
      "2025-03-07 23:20: Train Epoch 718: 20/23 Loss: 54.675171\n",
      "2025-03-07 23:20: **********Train Epoch 718: averaged Loss: 20.834312\n",
      "2025-03-07 23:20: **********Val Epoch 718: average Loss: 1917.268311\n",
      "2025-03-07 23:20: Train Epoch 719: 0/23 Loss: 5.550034\n",
      "2025-03-07 23:20: Train Epoch 719: 20/23 Loss: 65.058357\n",
      "2025-03-07 23:20: **********Train Epoch 719: averaged Loss: 18.817351\n",
      "2025-03-07 23:20: **********Val Epoch 719: average Loss: 1864.435760\n",
      "2025-03-07 23:20: Train Epoch 720: 0/23 Loss: 6.365169\n",
      "2025-03-07 23:20: Train Epoch 720: 20/23 Loss: 49.131340\n",
      "2025-03-07 23:20: **********Train Epoch 720: averaged Loss: 18.228282\n",
      "2025-03-07 23:20: **********Val Epoch 720: average Loss: 1849.154236\n",
      "2025-03-07 23:20: Train Epoch 721: 0/23 Loss: 4.645942\n",
      "2025-03-07 23:20: Train Epoch 721: 20/23 Loss: 38.063099\n",
      "2025-03-07 23:20: **********Train Epoch 721: averaged Loss: 16.618217\n",
      "2025-03-07 23:20: **********Val Epoch 721: average Loss: 1853.307800\n",
      "2025-03-07 23:20: Train Epoch 722: 0/23 Loss: 10.232599\n",
      "2025-03-07 23:20: Train Epoch 722: 20/23 Loss: 38.203995\n",
      "2025-03-07 23:20: **********Train Epoch 722: averaged Loss: 15.764420\n",
      "2025-03-07 23:20: **********Val Epoch 722: average Loss: 1897.713196\n",
      "2025-03-07 23:20: Train Epoch 723: 0/23 Loss: 4.829159\n",
      "2025-03-07 23:20: Train Epoch 723: 20/23 Loss: 58.708721\n",
      "2025-03-07 23:20: **********Train Epoch 723: averaged Loss: 15.599090\n",
      "2025-03-07 23:20: **********Val Epoch 723: average Loss: 1859.716187\n",
      "2025-03-07 23:20: Train Epoch 724: 0/23 Loss: 13.302771\n",
      "2025-03-07 23:20: Train Epoch 724: 20/23 Loss: 40.709084\n",
      "2025-03-07 23:20: **********Train Epoch 724: averaged Loss: 16.603242\n",
      "2025-03-07 23:20: **********Val Epoch 724: average Loss: 1866.315308\n",
      "2025-03-07 23:20: Train Epoch 725: 0/23 Loss: 1.223052\n",
      "2025-03-07 23:20: Train Epoch 725: 20/23 Loss: 35.889812\n",
      "2025-03-07 23:20: **********Train Epoch 725: averaged Loss: 14.855403\n",
      "2025-03-07 23:20: **********Val Epoch 725: average Loss: 1853.652100\n",
      "2025-03-07 23:20: Train Epoch 726: 0/23 Loss: 2.921561\n",
      "2025-03-07 23:20: Train Epoch 726: 20/23 Loss: 57.587105\n",
      "2025-03-07 23:20: **********Train Epoch 726: averaged Loss: 20.519624\n",
      "2025-03-07 23:20: **********Val Epoch 726: average Loss: 1854.280823\n",
      "2025-03-07 23:20: Train Epoch 727: 0/23 Loss: 10.036806\n",
      "2025-03-07 23:20: Train Epoch 727: 20/23 Loss: 46.457993\n",
      "2025-03-07 23:20: **********Train Epoch 727: averaged Loss: 26.074652\n",
      "2025-03-07 23:20: **********Val Epoch 727: average Loss: 1836.253052\n",
      "2025-03-07 23:20: Train Epoch 728: 0/23 Loss: 10.237494\n",
      "2025-03-07 23:20: Train Epoch 728: 20/23 Loss: 72.376930\n",
      "2025-03-07 23:20: **********Train Epoch 728: averaged Loss: 21.165256\n",
      "2025-03-07 23:20: **********Val Epoch 728: average Loss: 1783.318420\n",
      "2025-03-07 23:20: *********************************Current best model saved!\n",
      "2025-03-07 23:20: Train Epoch 729: 0/23 Loss: 12.523623\n",
      "2025-03-07 23:20: Train Epoch 729: 20/23 Loss: 57.039093\n",
      "2025-03-07 23:20: **********Train Epoch 729: averaged Loss: 19.724561\n",
      "2025-03-07 23:20: **********Val Epoch 729: average Loss: 1821.293396\n",
      "2025-03-07 23:20: Train Epoch 730: 0/23 Loss: 9.231124\n",
      "2025-03-07 23:20: Train Epoch 730: 20/23 Loss: 61.606796\n",
      "2025-03-07 23:20: **********Train Epoch 730: averaged Loss: 18.182489\n",
      "2025-03-07 23:20: **********Val Epoch 730: average Loss: 1856.099731\n",
      "2025-03-07 23:20: Train Epoch 731: 0/23 Loss: 10.658154\n",
      "2025-03-07 23:20: Train Epoch 731: 20/23 Loss: 43.945259\n",
      "2025-03-07 23:20: **********Train Epoch 731: averaged Loss: 18.349126\n",
      "2025-03-07 23:20: **********Val Epoch 731: average Loss: 1840.307129\n",
      "2025-03-07 23:20: Train Epoch 732: 0/23 Loss: 0.947913\n",
      "2025-03-07 23:20: Train Epoch 732: 20/23 Loss: 41.649780\n",
      "2025-03-07 23:20: **********Train Epoch 732: averaged Loss: 16.924355\n",
      "2025-03-07 23:20: **********Val Epoch 732: average Loss: 1803.062561\n",
      "2025-03-07 23:20: Train Epoch 733: 0/23 Loss: 10.465694\n",
      "2025-03-07 23:20: Train Epoch 733: 20/23 Loss: 47.402805\n",
      "2025-03-07 23:20: **********Train Epoch 733: averaged Loss: 21.387446\n",
      "2025-03-07 23:20: **********Val Epoch 733: average Loss: 1802.939087\n",
      "2025-03-07 23:20: Train Epoch 734: 0/23 Loss: 7.219489\n",
      "2025-03-07 23:20: Train Epoch 734: 20/23 Loss: 57.749176\n",
      "2025-03-07 23:20: **********Train Epoch 734: averaged Loss: 23.335036\n",
      "2025-03-07 23:20: **********Val Epoch 734: average Loss: 1803.010956\n",
      "2025-03-07 23:20: Train Epoch 735: 0/23 Loss: 7.993745\n",
      "2025-03-07 23:20: Train Epoch 735: 20/23 Loss: 58.131615\n",
      "2025-03-07 23:20: **********Train Epoch 735: averaged Loss: 20.152687\n",
      "2025-03-07 23:20: **********Val Epoch 735: average Loss: 1843.316162\n",
      "2025-03-07 23:20: Train Epoch 736: 0/23 Loss: 13.555116\n",
      "2025-03-07 23:20: Train Epoch 736: 20/23 Loss: 69.943642\n",
      "2025-03-07 23:20: **********Train Epoch 736: averaged Loss: 21.038092\n",
      "2025-03-07 23:20: **********Val Epoch 736: average Loss: 1855.535767\n",
      "2025-03-07 23:20: Train Epoch 737: 0/23 Loss: 1.858642\n",
      "2025-03-07 23:20: Train Epoch 737: 20/23 Loss: 43.093346\n",
      "2025-03-07 23:20: **********Train Epoch 737: averaged Loss: 17.380006\n",
      "2025-03-07 23:20: **********Val Epoch 737: average Loss: 1856.579590\n",
      "2025-03-07 23:20: Train Epoch 738: 0/23 Loss: 8.950972\n",
      "2025-03-07 23:20: Train Epoch 738: 20/23 Loss: 49.577709\n",
      "2025-03-07 23:20: **********Train Epoch 738: averaged Loss: 17.180091\n",
      "2025-03-07 23:20: **********Val Epoch 738: average Loss: 1816.030914\n",
      "2025-03-07 23:20: Train Epoch 739: 0/23 Loss: 5.338759\n",
      "2025-03-07 23:20: Train Epoch 739: 20/23 Loss: 48.249283\n",
      "2025-03-07 23:20: **********Train Epoch 739: averaged Loss: 17.147440\n",
      "2025-03-07 23:21: **********Val Epoch 739: average Loss: 1848.029053\n",
      "2025-03-07 23:21: Train Epoch 740: 0/23 Loss: 10.556496\n",
      "2025-03-07 23:21: Train Epoch 740: 20/23 Loss: 38.561459\n",
      "2025-03-07 23:21: **********Train Epoch 740: averaged Loss: 17.480242\n",
      "2025-03-07 23:21: **********Val Epoch 740: average Loss: 1842.496948\n",
      "2025-03-07 23:21: Train Epoch 741: 0/23 Loss: 9.062585\n",
      "2025-03-07 23:21: Train Epoch 741: 20/23 Loss: 45.643894\n",
      "2025-03-07 23:21: **********Train Epoch 741: averaged Loss: 15.823061\n",
      "2025-03-07 23:21: **********Val Epoch 741: average Loss: 1853.223907\n",
      "2025-03-07 23:21: Train Epoch 742: 0/23 Loss: 8.369488\n",
      "2025-03-07 23:21: Train Epoch 742: 20/23 Loss: 41.295540\n",
      "2025-03-07 23:21: **********Train Epoch 742: averaged Loss: 17.569924\n",
      "2025-03-07 23:21: **********Val Epoch 742: average Loss: 1826.715332\n",
      "2025-03-07 23:21: Train Epoch 743: 0/23 Loss: 2.421173\n",
      "2025-03-07 23:21: Train Epoch 743: 20/23 Loss: 65.198235\n",
      "2025-03-07 23:21: **********Train Epoch 743: averaged Loss: 17.268812\n",
      "2025-03-07 23:21: **********Val Epoch 743: average Loss: 1790.203918\n",
      "2025-03-07 23:21: Train Epoch 744: 0/23 Loss: 4.386786\n",
      "2025-03-07 23:21: Train Epoch 744: 20/23 Loss: 38.097656\n",
      "2025-03-07 23:21: **********Train Epoch 744: averaged Loss: 14.847625\n",
      "2025-03-07 23:21: **********Val Epoch 744: average Loss: 1857.609680\n",
      "2025-03-07 23:21: Train Epoch 745: 0/23 Loss: 5.103574\n",
      "2025-03-07 23:21: Train Epoch 745: 20/23 Loss: 35.970184\n",
      "2025-03-07 23:21: **********Train Epoch 745: averaged Loss: 18.022413\n",
      "2025-03-07 23:21: **********Val Epoch 745: average Loss: 1862.289551\n",
      "2025-03-07 23:21: Train Epoch 746: 0/23 Loss: 1.496059\n",
      "2025-03-07 23:21: Train Epoch 746: 20/23 Loss: 37.577301\n",
      "2025-03-07 23:21: **********Train Epoch 746: averaged Loss: 15.439934\n",
      "2025-03-07 23:21: **********Val Epoch 746: average Loss: 1881.816711\n",
      "2025-03-07 23:21: Train Epoch 747: 0/23 Loss: 5.274350\n",
      "2025-03-07 23:21: Train Epoch 747: 20/23 Loss: 37.684093\n",
      "2025-03-07 23:21: **********Train Epoch 747: averaged Loss: 18.079279\n",
      "2025-03-07 23:21: **********Val Epoch 747: average Loss: 1808.652283\n",
      "2025-03-07 23:21: Train Epoch 748: 0/23 Loss: 7.491415\n",
      "2025-03-07 23:21: Train Epoch 748: 20/23 Loss: 59.489994\n",
      "2025-03-07 23:21: **********Train Epoch 748: averaged Loss: 19.446166\n",
      "2025-03-07 23:21: **********Val Epoch 748: average Loss: 1876.675659\n",
      "2025-03-07 23:21: Train Epoch 749: 0/23 Loss: 6.719277\n",
      "2025-03-07 23:21: Train Epoch 749: 20/23 Loss: 87.366394\n",
      "2025-03-07 23:21: **********Train Epoch 749: averaged Loss: 26.159787\n",
      "2025-03-07 23:21: **********Val Epoch 749: average Loss: 1838.155518\n",
      "2025-03-07 23:21: Train Epoch 750: 0/23 Loss: 6.194793\n",
      "2025-03-07 23:21: Train Epoch 750: 20/23 Loss: 57.937134\n",
      "2025-03-07 23:21: **********Train Epoch 750: averaged Loss: 20.199298\n",
      "2025-03-07 23:21: **********Val Epoch 750: average Loss: 1843.715454\n",
      "2025-03-07 23:21: Train Epoch 751: 0/23 Loss: 11.232872\n",
      "2025-03-07 23:21: Train Epoch 751: 20/23 Loss: 47.469891\n",
      "2025-03-07 23:21: **********Train Epoch 751: averaged Loss: 16.993476\n",
      "2025-03-07 23:21: **********Val Epoch 751: average Loss: 1830.003143\n",
      "2025-03-07 23:21: Train Epoch 752: 0/23 Loss: 8.510551\n",
      "2025-03-07 23:21: Train Epoch 752: 20/23 Loss: 46.000313\n",
      "2025-03-07 23:21: **********Train Epoch 752: averaged Loss: 15.467412\n",
      "2025-03-07 23:21: **********Val Epoch 752: average Loss: 1836.340698\n",
      "2025-03-07 23:21: Train Epoch 753: 0/23 Loss: 11.368946\n",
      "2025-03-07 23:21: Train Epoch 753: 20/23 Loss: 48.950260\n",
      "2025-03-07 23:21: **********Train Epoch 753: averaged Loss: 15.796724\n",
      "2025-03-07 23:21: **********Val Epoch 753: average Loss: 1824.257812\n",
      "2025-03-07 23:21: Train Epoch 754: 0/23 Loss: 3.406892\n",
      "2025-03-07 23:21: Train Epoch 754: 20/23 Loss: 49.839275\n",
      "2025-03-07 23:21: **********Train Epoch 754: averaged Loss: 13.934077\n",
      "2025-03-07 23:21: **********Val Epoch 754: average Loss: 1798.151581\n",
      "2025-03-07 23:21: Train Epoch 755: 0/23 Loss: 4.526143\n",
      "2025-03-07 23:21: Train Epoch 755: 20/23 Loss: 43.304001\n",
      "2025-03-07 23:21: **********Train Epoch 755: averaged Loss: 13.082159\n",
      "2025-03-07 23:21: **********Val Epoch 755: average Loss: 1848.822510\n",
      "2025-03-07 23:21: Train Epoch 756: 0/23 Loss: 4.426282\n",
      "2025-03-07 23:21: Train Epoch 756: 20/23 Loss: 53.320194\n",
      "2025-03-07 23:21: **********Train Epoch 756: averaged Loss: 13.976628\n",
      "2025-03-07 23:21: **********Val Epoch 756: average Loss: 1827.528931\n",
      "2025-03-07 23:21: Train Epoch 757: 0/23 Loss: 1.689639\n",
      "2025-03-07 23:21: Train Epoch 757: 20/23 Loss: 38.214939\n",
      "2025-03-07 23:21: **********Train Epoch 757: averaged Loss: 12.812919\n",
      "2025-03-07 23:21: **********Val Epoch 757: average Loss: 1847.495728\n",
      "2025-03-07 23:21: Train Epoch 758: 0/23 Loss: 1.631286\n",
      "2025-03-07 23:21: Train Epoch 758: 20/23 Loss: 38.014027\n",
      "2025-03-07 23:21: **********Train Epoch 758: averaged Loss: 12.615076\n",
      "2025-03-07 23:21: **********Val Epoch 758: average Loss: 1797.897583\n",
      "2025-03-07 23:21: Train Epoch 759: 0/23 Loss: 1.006098\n",
      "2025-03-07 23:21: Train Epoch 759: 20/23 Loss: 37.787674\n",
      "2025-03-07 23:21: **********Train Epoch 759: averaged Loss: 12.561963\n",
      "2025-03-07 23:21: **********Val Epoch 759: average Loss: 1836.842590\n",
      "2025-03-07 23:21: Train Epoch 760: 0/23 Loss: 1.430786\n",
      "2025-03-07 23:21: Train Epoch 760: 20/23 Loss: 39.633156\n",
      "2025-03-07 23:21: **********Train Epoch 760: averaged Loss: 12.998297\n",
      "2025-03-07 23:21: **********Val Epoch 760: average Loss: 1833.595032\n",
      "2025-03-07 23:21: Train Epoch 761: 0/23 Loss: 0.783700\n",
      "2025-03-07 23:21: Train Epoch 761: 20/23 Loss: 36.841255\n",
      "2025-03-07 23:21: **********Train Epoch 761: averaged Loss: 11.916161\n",
      "2025-03-07 23:21: **********Val Epoch 761: average Loss: 1882.834229\n",
      "2025-03-07 23:21: Train Epoch 762: 0/23 Loss: 4.850997\n",
      "2025-03-07 23:21: Train Epoch 762: 20/23 Loss: 37.079704\n",
      "2025-03-07 23:21: **********Train Epoch 762: averaged Loss: 12.405979\n",
      "2025-03-07 23:21: **********Val Epoch 762: average Loss: 1866.661072\n",
      "2025-03-07 23:21: Train Epoch 763: 0/23 Loss: 2.824289\n",
      "2025-03-07 23:21: Train Epoch 763: 20/23 Loss: 45.882576\n",
      "2025-03-07 23:21: **********Train Epoch 763: averaged Loss: 12.921128\n",
      "2025-03-07 23:21: **********Val Epoch 763: average Loss: 1841.757202\n",
      "2025-03-07 23:21: Train Epoch 764: 0/23 Loss: 1.624822\n",
      "2025-03-07 23:21: Train Epoch 764: 20/23 Loss: 38.189407\n",
      "2025-03-07 23:21: **********Train Epoch 764: averaged Loss: 12.190835\n",
      "2025-03-07 23:21: **********Val Epoch 764: average Loss: 1848.622253\n",
      "2025-03-07 23:21: Train Epoch 765: 0/23 Loss: 6.098830\n",
      "2025-03-07 23:21: Train Epoch 765: 20/23 Loss: 44.265591\n",
      "2025-03-07 23:21: **********Train Epoch 765: averaged Loss: 12.965672\n",
      "2025-03-07 23:21: **********Val Epoch 765: average Loss: 1850.718750\n",
      "2025-03-07 23:21: Train Epoch 766: 0/23 Loss: 1.922828\n",
      "2025-03-07 23:21: Train Epoch 766: 20/23 Loss: 39.119980\n",
      "2025-03-07 23:21: **********Train Epoch 766: averaged Loss: 12.166950\n",
      "2025-03-07 23:21: **********Val Epoch 766: average Loss: 1822.570679\n",
      "2025-03-07 23:21: Train Epoch 767: 0/23 Loss: 4.078549\n",
      "2025-03-07 23:21: Train Epoch 767: 20/23 Loss: 48.983826\n",
      "2025-03-07 23:21: **********Train Epoch 767: averaged Loss: 13.138011\n",
      "2025-03-07 23:21: **********Val Epoch 767: average Loss: 1853.215881\n",
      "2025-03-07 23:21: Train Epoch 768: 0/23 Loss: 2.392236\n",
      "2025-03-07 23:21: Train Epoch 768: 20/23 Loss: 42.426483\n",
      "2025-03-07 23:21: **********Train Epoch 768: averaged Loss: 13.059271\n",
      "2025-03-07 23:21: **********Val Epoch 768: average Loss: 1847.599731\n",
      "2025-03-07 23:21: Train Epoch 769: 0/23 Loss: 2.297779\n",
      "2025-03-07 23:21: Train Epoch 769: 20/23 Loss: 51.735371\n",
      "2025-03-07 23:21: **********Train Epoch 769: averaged Loss: 13.404229\n",
      "2025-03-07 23:21: **********Val Epoch 769: average Loss: 1829.055908\n",
      "2025-03-07 23:21: Train Epoch 770: 0/23 Loss: 0.857674\n",
      "2025-03-07 23:21: Train Epoch 770: 20/23 Loss: 35.323479\n",
      "2025-03-07 23:21: **********Train Epoch 770: averaged Loss: 12.253893\n",
      "2025-03-07 23:21: **********Val Epoch 770: average Loss: 1838.075592\n",
      "2025-03-07 23:21: Train Epoch 771: 0/23 Loss: 3.186640\n",
      "2025-03-07 23:21: Train Epoch 771: 20/23 Loss: 35.578568\n",
      "2025-03-07 23:21: **********Train Epoch 771: averaged Loss: 12.875261\n",
      "2025-03-07 23:21: **********Val Epoch 771: average Loss: 1840.354309\n",
      "2025-03-07 23:21: Train Epoch 772: 0/23 Loss: 1.192874\n",
      "2025-03-07 23:21: Train Epoch 772: 20/23 Loss: 37.188278\n",
      "2025-03-07 23:21: **********Train Epoch 772: averaged Loss: 12.803936\n",
      "2025-03-07 23:21: **********Val Epoch 772: average Loss: 1852.722961\n",
      "2025-03-07 23:21: Train Epoch 773: 0/23 Loss: 1.833345\n",
      "2025-03-07 23:21: Train Epoch 773: 20/23 Loss: 35.698734\n",
      "2025-03-07 23:21: **********Train Epoch 773: averaged Loss: 12.165867\n",
      "2025-03-07 23:21: **********Val Epoch 773: average Loss: 1834.627014\n",
      "2025-03-07 23:21: Train Epoch 774: 0/23 Loss: 3.423673\n",
      "2025-03-07 23:21: Train Epoch 774: 20/23 Loss: 36.404442\n",
      "2025-03-07 23:21: **********Train Epoch 774: averaged Loss: 12.193511\n",
      "2025-03-07 23:21: **********Val Epoch 774: average Loss: 1854.365601\n",
      "2025-03-07 23:21: Train Epoch 775: 0/23 Loss: 1.227953\n",
      "2025-03-07 23:21: Train Epoch 775: 20/23 Loss: 37.434814\n",
      "2025-03-07 23:21: **********Train Epoch 775: averaged Loss: 12.380708\n",
      "2025-03-07 23:21: **********Val Epoch 775: average Loss: 1858.862701\n",
      "2025-03-07 23:21: Train Epoch 776: 0/23 Loss: 0.757829\n",
      "2025-03-07 23:22: Train Epoch 776: 20/23 Loss: 35.876228\n",
      "2025-03-07 23:22: **********Train Epoch 776: averaged Loss: 13.395696\n",
      "2025-03-07 23:22: **********Val Epoch 776: average Loss: 1855.811401\n",
      "2025-03-07 23:22: Train Epoch 777: 0/23 Loss: 1.293846\n",
      "2025-03-07 23:22: Train Epoch 777: 20/23 Loss: 40.789032\n",
      "2025-03-07 23:22: **********Train Epoch 777: averaged Loss: 12.478404\n",
      "2025-03-07 23:22: **********Val Epoch 777: average Loss: 1865.396606\n",
      "2025-03-07 23:22: Train Epoch 778: 0/23 Loss: 1.042257\n",
      "2025-03-07 23:22: Train Epoch 778: 20/23 Loss: 35.829803\n",
      "2025-03-07 23:22: **********Train Epoch 778: averaged Loss: 11.682902\n",
      "2025-03-07 23:22: **********Val Epoch 778: average Loss: 1836.686188\n",
      "2025-03-07 23:22: Train Epoch 779: 0/23 Loss: 0.802958\n",
      "2025-03-07 23:22: Train Epoch 779: 20/23 Loss: 35.259125\n",
      "2025-03-07 23:22: **********Train Epoch 779: averaged Loss: 12.376564\n",
      "2025-03-07 23:22: **********Val Epoch 779: average Loss: 1846.527588\n",
      "2025-03-07 23:22: Train Epoch 780: 0/23 Loss: 0.994226\n",
      "2025-03-07 23:22: Train Epoch 780: 20/23 Loss: 48.113789\n",
      "2025-03-07 23:22: **********Train Epoch 780: averaged Loss: 12.528134\n",
      "2025-03-07 23:22: **********Val Epoch 780: average Loss: 1830.991547\n",
      "2025-03-07 23:22: Train Epoch 781: 0/23 Loss: 1.589638\n",
      "2025-03-07 23:22: Train Epoch 781: 20/23 Loss: 39.004192\n",
      "2025-03-07 23:22: **********Train Epoch 781: averaged Loss: 12.322934\n",
      "2025-03-07 23:22: **********Val Epoch 781: average Loss: 1853.095703\n",
      "2025-03-07 23:22: Train Epoch 782: 0/23 Loss: 3.796889\n",
      "2025-03-07 23:22: Train Epoch 782: 20/23 Loss: 37.322975\n",
      "2025-03-07 23:22: **********Train Epoch 782: averaged Loss: 12.012802\n",
      "2025-03-07 23:22: **********Val Epoch 782: average Loss: 1842.453247\n",
      "2025-03-07 23:22: Train Epoch 783: 0/23 Loss: 2.089288\n",
      "2025-03-07 23:22: Train Epoch 783: 20/23 Loss: 36.639816\n",
      "2025-03-07 23:22: **********Train Epoch 783: averaged Loss: 12.494329\n",
      "2025-03-07 23:22: **********Val Epoch 783: average Loss: 1854.502563\n",
      "2025-03-07 23:22: Train Epoch 784: 0/23 Loss: 1.557285\n",
      "2025-03-07 23:22: Train Epoch 784: 20/23 Loss: 38.822723\n",
      "2025-03-07 23:22: **********Train Epoch 784: averaged Loss: 12.061962\n",
      "2025-03-07 23:22: **********Val Epoch 784: average Loss: 1819.539368\n",
      "2025-03-07 23:22: Train Epoch 785: 0/23 Loss: 1.696164\n",
      "2025-03-07 23:22: Train Epoch 785: 20/23 Loss: 40.925583\n",
      "2025-03-07 23:22: **********Train Epoch 785: averaged Loss: 12.689279\n",
      "2025-03-07 23:22: **********Val Epoch 785: average Loss: 1837.735565\n",
      "2025-03-07 23:22: Train Epoch 786: 0/23 Loss: 2.216279\n",
      "2025-03-07 23:22: Train Epoch 786: 20/23 Loss: 44.938351\n",
      "2025-03-07 23:22: **********Train Epoch 786: averaged Loss: 12.505734\n",
      "2025-03-07 23:22: **********Val Epoch 786: average Loss: 1880.013306\n",
      "2025-03-07 23:22: Train Epoch 787: 0/23 Loss: 5.178171\n",
      "2025-03-07 23:22: Train Epoch 787: 20/23 Loss: 35.145008\n",
      "2025-03-07 23:22: **********Train Epoch 787: averaged Loss: 12.346671\n",
      "2025-03-07 23:22: **********Val Epoch 787: average Loss: 1846.040222\n",
      "2025-03-07 23:22: Train Epoch 788: 0/23 Loss: 1.708856\n",
      "2025-03-07 23:22: Train Epoch 788: 20/23 Loss: 34.630867\n",
      "2025-03-07 23:22: **********Train Epoch 788: averaged Loss: 12.591196\n",
      "2025-03-07 23:22: **********Val Epoch 788: average Loss: 1802.893433\n",
      "2025-03-07 23:22: Train Epoch 789: 0/23 Loss: 3.374105\n",
      "2025-03-07 23:22: Train Epoch 789: 20/23 Loss: 35.006508\n",
      "2025-03-07 23:22: **********Train Epoch 789: averaged Loss: 12.165819\n",
      "2025-03-07 23:22: **********Val Epoch 789: average Loss: 1841.850159\n",
      "2025-03-07 23:22: Train Epoch 790: 0/23 Loss: 1.225762\n",
      "2025-03-07 23:22: Train Epoch 790: 20/23 Loss: 34.969692\n",
      "2025-03-07 23:22: **********Train Epoch 790: averaged Loss: 12.237964\n",
      "2025-03-07 23:22: **********Val Epoch 790: average Loss: 1844.913208\n",
      "2025-03-07 23:22: Train Epoch 791: 0/23 Loss: 0.774597\n",
      "2025-03-07 23:22: Train Epoch 791: 20/23 Loss: 35.011345\n",
      "2025-03-07 23:22: **********Train Epoch 791: averaged Loss: 13.060943\n",
      "2025-03-07 23:22: **********Val Epoch 791: average Loss: 1869.168640\n",
      "2025-03-07 23:22: Train Epoch 792: 0/23 Loss: 0.901278\n",
      "2025-03-07 23:22: Train Epoch 792: 20/23 Loss: 49.038040\n",
      "2025-03-07 23:22: **********Train Epoch 792: averaged Loss: 12.926839\n",
      "2025-03-07 23:22: **********Val Epoch 792: average Loss: 1851.121307\n",
      "2025-03-07 23:22: Train Epoch 793: 0/23 Loss: 0.847605\n",
      "2025-03-07 23:22: Train Epoch 793: 20/23 Loss: 34.999035\n",
      "2025-03-07 23:22: **********Train Epoch 793: averaged Loss: 12.427412\n",
      "2025-03-07 23:22: **********Val Epoch 793: average Loss: 1861.695312\n",
      "2025-03-07 23:22: Train Epoch 794: 0/23 Loss: 2.821702\n",
      "2025-03-07 23:22: Train Epoch 794: 20/23 Loss: 34.678650\n",
      "2025-03-07 23:22: **********Train Epoch 794: averaged Loss: 12.471133\n",
      "2025-03-07 23:22: **********Val Epoch 794: average Loss: 1847.128052\n",
      "2025-03-07 23:22: Train Epoch 795: 0/23 Loss: 1.977250\n",
      "2025-03-07 23:22: Train Epoch 795: 20/23 Loss: 52.575035\n",
      "2025-03-07 23:22: **********Train Epoch 795: averaged Loss: 12.387634\n",
      "2025-03-07 23:22: **********Val Epoch 795: average Loss: 1858.231812\n",
      "2025-03-07 23:22: Train Epoch 796: 0/23 Loss: 1.684664\n",
      "2025-03-07 23:22: Train Epoch 796: 20/23 Loss: 34.287502\n",
      "2025-03-07 23:22: **********Train Epoch 796: averaged Loss: 11.433731\n",
      "2025-03-07 23:22: **********Val Epoch 796: average Loss: 1829.721954\n",
      "2025-03-07 23:22: Train Epoch 797: 0/23 Loss: 3.031454\n",
      "2025-03-07 23:22: Train Epoch 797: 20/23 Loss: 51.741970\n",
      "2025-03-07 23:22: **********Train Epoch 797: averaged Loss: 12.142975\n",
      "2025-03-07 23:22: **********Val Epoch 797: average Loss: 1825.492126\n",
      "2025-03-07 23:22: Train Epoch 798: 0/23 Loss: 3.738999\n",
      "2025-03-07 23:22: Train Epoch 798: 20/23 Loss: 42.448769\n",
      "2025-03-07 23:22: **********Train Epoch 798: averaged Loss: 12.864526\n",
      "2025-03-07 23:22: **********Val Epoch 798: average Loss: 1863.180054\n",
      "2025-03-07 23:22: Train Epoch 799: 0/23 Loss: 4.649082\n",
      "2025-03-07 23:22: Train Epoch 799: 20/23 Loss: 37.467503\n",
      "2025-03-07 23:22: **********Train Epoch 799: averaged Loss: 12.433514\n",
      "2025-03-07 23:22: **********Val Epoch 799: average Loss: 1869.403687\n",
      "2025-03-07 23:22: Train Epoch 800: 0/23 Loss: 0.980654\n",
      "2025-03-07 23:22: Train Epoch 800: 20/23 Loss: 36.419003\n",
      "2025-03-07 23:22: **********Train Epoch 800: averaged Loss: 12.629111\n",
      "2025-03-07 23:22: **********Val Epoch 800: average Loss: 1853.178406\n",
      "2025-03-07 23:22: Train Epoch 801: 0/23 Loss: 5.591543\n",
      "2025-03-07 23:22: Train Epoch 801: 20/23 Loss: 35.201462\n",
      "2025-03-07 23:22: **********Train Epoch 801: averaged Loss: 13.061420\n",
      "2025-03-07 23:22: **********Val Epoch 801: average Loss: 1860.487244\n",
      "2025-03-07 23:22: Train Epoch 802: 0/23 Loss: 2.103505\n",
      "2025-03-07 23:22: Train Epoch 802: 20/23 Loss: 39.675205\n",
      "2025-03-07 23:22: **********Train Epoch 802: averaged Loss: 12.093325\n",
      "2025-03-07 23:22: **********Val Epoch 802: average Loss: 1827.735107\n",
      "2025-03-07 23:22: Train Epoch 803: 0/23 Loss: 1.089009\n",
      "2025-03-07 23:22: Train Epoch 803: 20/23 Loss: 43.038342\n",
      "2025-03-07 23:22: **********Train Epoch 803: averaged Loss: 12.443866\n",
      "2025-03-07 23:22: **********Val Epoch 803: average Loss: 1835.536926\n",
      "2025-03-07 23:22: Train Epoch 804: 0/23 Loss: 3.493293\n",
      "2025-03-07 23:22: Train Epoch 804: 20/23 Loss: 35.342243\n",
      "2025-03-07 23:22: **********Train Epoch 804: averaged Loss: 11.800445\n",
      "2025-03-07 23:22: **********Val Epoch 804: average Loss: 1845.975006\n",
      "2025-03-07 23:22: Train Epoch 805: 0/23 Loss: 2.937275\n",
      "2025-03-07 23:22: Train Epoch 805: 20/23 Loss: 36.058441\n",
      "2025-03-07 23:22: **********Train Epoch 805: averaged Loss: 12.824333\n",
      "2025-03-07 23:22: **********Val Epoch 805: average Loss: 1834.268738\n",
      "2025-03-07 23:22: Train Epoch 806: 0/23 Loss: 1.250894\n",
      "2025-03-07 23:22: Train Epoch 806: 20/23 Loss: 34.744789\n",
      "2025-03-07 23:22: **********Train Epoch 806: averaged Loss: 12.536551\n",
      "2025-03-07 23:22: **********Val Epoch 806: average Loss: 1813.213928\n",
      "2025-03-07 23:22: Train Epoch 807: 0/23 Loss: 1.951450\n",
      "2025-03-07 23:22: Train Epoch 807: 20/23 Loss: 34.257233\n",
      "2025-03-07 23:22: **********Train Epoch 807: averaged Loss: 12.032900\n",
      "2025-03-07 23:22: **********Val Epoch 807: average Loss: 1844.007019\n",
      "2025-03-07 23:22: Train Epoch 808: 0/23 Loss: 1.528795\n",
      "2025-03-07 23:22: Train Epoch 808: 20/23 Loss: 35.621819\n",
      "2025-03-07 23:22: **********Train Epoch 808: averaged Loss: 12.716982\n",
      "2025-03-07 23:22: **********Val Epoch 808: average Loss: 1839.173309\n",
      "2025-03-07 23:22: Train Epoch 809: 0/23 Loss: 1.248656\n",
      "2025-03-07 23:22: Train Epoch 809: 20/23 Loss: 36.342865\n",
      "2025-03-07 23:22: **********Train Epoch 809: averaged Loss: 12.559630\n",
      "2025-03-07 23:22: **********Val Epoch 809: average Loss: 1867.567505\n",
      "2025-03-07 23:22: Train Epoch 810: 0/23 Loss: 0.820683\n",
      "2025-03-07 23:22: Train Epoch 810: 20/23 Loss: 34.687454\n",
      "2025-03-07 23:22: **********Train Epoch 810: averaged Loss: 12.390063\n",
      "2025-03-07 23:22: **********Val Epoch 810: average Loss: 1825.887939\n",
      "2025-03-07 23:22: Train Epoch 811: 0/23 Loss: 2.369903\n",
      "2025-03-07 23:23: Train Epoch 811: 20/23 Loss: 45.990845\n",
      "2025-03-07 23:23: **********Train Epoch 811: averaged Loss: 12.550966\n",
      "2025-03-07 23:23: **********Val Epoch 811: average Loss: 1854.773315\n",
      "2025-03-07 23:23: Train Epoch 812: 0/23 Loss: 0.742891\n",
      "2025-03-07 23:23: Train Epoch 812: 20/23 Loss: 46.358292\n",
      "2025-03-07 23:23: **********Train Epoch 812: averaged Loss: 13.223983\n",
      "2025-03-07 23:23: **********Val Epoch 812: average Loss: 1915.197632\n",
      "2025-03-07 23:23: Train Epoch 813: 0/23 Loss: 3.322068\n",
      "2025-03-07 23:23: Train Epoch 813: 20/23 Loss: 36.980988\n",
      "2025-03-07 23:23: **********Train Epoch 813: averaged Loss: 12.266696\n",
      "2025-03-07 23:23: **********Val Epoch 813: average Loss: 1842.754456\n",
      "2025-03-07 23:23: Train Epoch 814: 0/23 Loss: 1.557162\n",
      "2025-03-07 23:23: Train Epoch 814: 20/23 Loss: 38.615494\n",
      "2025-03-07 23:23: **********Train Epoch 814: averaged Loss: 12.213752\n",
      "2025-03-07 23:23: **********Val Epoch 814: average Loss: 1839.251282\n",
      "2025-03-07 23:23: Train Epoch 815: 0/23 Loss: 5.039014\n",
      "2025-03-07 23:23: Train Epoch 815: 20/23 Loss: 54.386032\n",
      "2025-03-07 23:23: **********Train Epoch 815: averaged Loss: 13.564264\n",
      "2025-03-07 23:23: **********Val Epoch 815: average Loss: 1862.432617\n",
      "2025-03-07 23:23: Train Epoch 816: 0/23 Loss: 0.882282\n",
      "2025-03-07 23:23: Train Epoch 816: 20/23 Loss: 35.765575\n",
      "2025-03-07 23:23: **********Train Epoch 816: averaged Loss: 13.756194\n",
      "2025-03-07 23:23: **********Val Epoch 816: average Loss: 1883.721313\n",
      "2025-03-07 23:23: Train Epoch 817: 0/23 Loss: 2.870028\n",
      "2025-03-07 23:23: Train Epoch 817: 20/23 Loss: 33.811718\n",
      "2025-03-07 23:23: **********Train Epoch 817: averaged Loss: 14.072738\n",
      "2025-03-07 23:23: **********Val Epoch 817: average Loss: 1844.603210\n",
      "2025-03-07 23:23: Train Epoch 818: 0/23 Loss: 6.790468\n",
      "2025-03-07 23:23: Train Epoch 818: 20/23 Loss: 42.141029\n",
      "2025-03-07 23:23: **********Train Epoch 818: averaged Loss: 12.387926\n",
      "2025-03-07 23:23: **********Val Epoch 818: average Loss: 1870.510986\n",
      "2025-03-07 23:23: Train Epoch 819: 0/23 Loss: 2.543077\n",
      "2025-03-07 23:23: Train Epoch 819: 20/23 Loss: 34.713661\n",
      "2025-03-07 23:23: **********Train Epoch 819: averaged Loss: 12.049744\n",
      "2025-03-07 23:23: **********Val Epoch 819: average Loss: 1828.193817\n",
      "2025-03-07 23:23: Train Epoch 820: 0/23 Loss: 2.197245\n",
      "2025-03-07 23:23: Train Epoch 820: 20/23 Loss: 35.209354\n",
      "2025-03-07 23:23: **********Train Epoch 820: averaged Loss: 12.602972\n",
      "2025-03-07 23:23: **********Val Epoch 820: average Loss: 1870.112549\n",
      "2025-03-07 23:23: Train Epoch 821: 0/23 Loss: 5.418998\n",
      "2025-03-07 23:23: Train Epoch 821: 20/23 Loss: 33.467712\n",
      "2025-03-07 23:23: **********Train Epoch 821: averaged Loss: 13.790171\n",
      "2025-03-07 23:23: **********Val Epoch 821: average Loss: 1842.340210\n",
      "2025-03-07 23:23: Train Epoch 822: 0/23 Loss: 2.396554\n",
      "2025-03-07 23:23: Train Epoch 822: 20/23 Loss: 41.648232\n",
      "2025-03-07 23:23: **********Train Epoch 822: averaged Loss: 12.784365\n",
      "2025-03-07 23:23: **********Val Epoch 822: average Loss: 1849.339905\n",
      "2025-03-07 23:23: Train Epoch 823: 0/23 Loss: 1.093824\n",
      "2025-03-07 23:23: Train Epoch 823: 20/23 Loss: 39.110085\n",
      "2025-03-07 23:23: **********Train Epoch 823: averaged Loss: 12.269739\n",
      "2025-03-07 23:23: **********Val Epoch 823: average Loss: 1841.645020\n",
      "2025-03-07 23:23: Train Epoch 824: 0/23 Loss: 0.831866\n",
      "2025-03-07 23:23: Train Epoch 824: 20/23 Loss: 34.829483\n",
      "2025-03-07 23:23: **********Train Epoch 824: averaged Loss: 12.099263\n",
      "2025-03-07 23:23: **********Val Epoch 824: average Loss: 1841.283630\n",
      "2025-03-07 23:23: Train Epoch 825: 0/23 Loss: 1.393349\n",
      "2025-03-07 23:23: Train Epoch 825: 20/23 Loss: 33.921967\n",
      "2025-03-07 23:23: **********Train Epoch 825: averaged Loss: 11.693675\n",
      "2025-03-07 23:23: **********Val Epoch 825: average Loss: 1863.536438\n",
      "2025-03-07 23:23: Train Epoch 826: 0/23 Loss: 0.836149\n",
      "2025-03-07 23:23: Train Epoch 826: 20/23 Loss: 34.302956\n",
      "2025-03-07 23:23: **********Train Epoch 826: averaged Loss: 12.563059\n",
      "2025-03-07 23:23: **********Val Epoch 826: average Loss: 1844.158264\n",
      "2025-03-07 23:23: Train Epoch 827: 0/23 Loss: 5.586627\n",
      "2025-03-07 23:23: Train Epoch 827: 20/23 Loss: 36.687260\n",
      "2025-03-07 23:23: **********Train Epoch 827: averaged Loss: 12.336768\n",
      "2025-03-07 23:23: **********Val Epoch 827: average Loss: 1854.186462\n",
      "2025-03-07 23:23: Train Epoch 828: 0/23 Loss: 1.417139\n",
      "2025-03-07 23:23: Train Epoch 828: 20/23 Loss: 46.752804\n",
      "2025-03-07 23:23: **********Train Epoch 828: averaged Loss: 12.078676\n",
      "2025-03-07 23:23: **********Val Epoch 828: average Loss: 1873.393402\n",
      "2025-03-07 23:23: Train Epoch 829: 0/23 Loss: 2.391925\n",
      "2025-03-07 23:23: Train Epoch 829: 20/23 Loss: 36.205383\n",
      "2025-03-07 23:23: **********Train Epoch 829: averaged Loss: 11.761378\n",
      "2025-03-07 23:23: **********Val Epoch 829: average Loss: 1854.355957\n",
      "2025-03-07 23:23: Train Epoch 830: 0/23 Loss: 3.992551\n",
      "2025-03-07 23:23: Train Epoch 830: 20/23 Loss: 35.678692\n",
      "2025-03-07 23:23: **********Train Epoch 830: averaged Loss: 12.581761\n",
      "2025-03-07 23:23: **********Val Epoch 830: average Loss: 1849.728821\n",
      "2025-03-07 23:23: Train Epoch 831: 0/23 Loss: 1.626045\n",
      "2025-03-07 23:23: Train Epoch 831: 20/23 Loss: 33.687923\n",
      "2025-03-07 23:23: **********Train Epoch 831: averaged Loss: 12.179160\n",
      "2025-03-07 23:23: **********Val Epoch 831: average Loss: 1842.324127\n",
      "2025-03-07 23:23: Train Epoch 832: 0/23 Loss: 2.671938\n",
      "2025-03-07 23:23: Train Epoch 832: 20/23 Loss: 33.795700\n",
      "2025-03-07 23:23: **********Train Epoch 832: averaged Loss: 12.218309\n",
      "2025-03-07 23:23: **********Val Epoch 832: average Loss: 1856.825623\n",
      "2025-03-07 23:23: Train Epoch 833: 0/23 Loss: 3.612752\n",
      "2025-03-07 23:23: Train Epoch 833: 20/23 Loss: 33.594009\n",
      "2025-03-07 23:23: **********Train Epoch 833: averaged Loss: 12.080967\n",
      "2025-03-07 23:23: **********Val Epoch 833: average Loss: 1830.630310\n",
      "2025-03-07 23:23: Train Epoch 834: 0/23 Loss: 1.895799\n",
      "2025-03-07 23:23: Train Epoch 834: 20/23 Loss: 34.267185\n",
      "2025-03-07 23:23: **********Train Epoch 834: averaged Loss: 11.757544\n",
      "2025-03-07 23:23: **********Val Epoch 834: average Loss: 1842.212341\n",
      "2025-03-07 23:23: Train Epoch 835: 0/23 Loss: 0.801285\n",
      "2025-03-07 23:23: Train Epoch 835: 20/23 Loss: 33.698849\n",
      "2025-03-07 23:23: **********Train Epoch 835: averaged Loss: 12.684015\n",
      "2025-03-07 23:23: **********Val Epoch 835: average Loss: 1834.630951\n",
      "2025-03-07 23:23: Train Epoch 836: 0/23 Loss: 2.026776\n",
      "2025-03-07 23:23: Train Epoch 836: 20/23 Loss: 37.651466\n",
      "2025-03-07 23:23: **********Train Epoch 836: averaged Loss: 12.017562\n",
      "2025-03-07 23:23: **********Val Epoch 836: average Loss: 1840.319733\n",
      "2025-03-07 23:23: Train Epoch 837: 0/23 Loss: 0.834189\n",
      "2025-03-07 23:23: Train Epoch 837: 20/23 Loss: 51.526924\n",
      "2025-03-07 23:23: **********Train Epoch 837: averaged Loss: 12.737806\n",
      "2025-03-07 23:23: **********Val Epoch 837: average Loss: 1823.817078\n",
      "2025-03-07 23:23: Train Epoch 838: 0/23 Loss: 1.247118\n",
      "2025-03-07 23:23: Train Epoch 838: 20/23 Loss: 35.859505\n",
      "2025-03-07 23:23: **********Train Epoch 838: averaged Loss: 12.144291\n",
      "2025-03-07 23:23: **********Val Epoch 838: average Loss: 1853.642975\n",
      "2025-03-07 23:23: Train Epoch 839: 0/23 Loss: 1.138692\n",
      "2025-03-07 23:23: Train Epoch 839: 20/23 Loss: 33.195656\n",
      "2025-03-07 23:23: **********Train Epoch 839: averaged Loss: 11.707701\n",
      "2025-03-07 23:23: **********Val Epoch 839: average Loss: 1832.359222\n",
      "2025-03-07 23:23: Train Epoch 840: 0/23 Loss: 1.211560\n",
      "2025-03-07 23:23: Train Epoch 840: 20/23 Loss: 34.325024\n",
      "2025-03-07 23:23: **********Train Epoch 840: averaged Loss: 12.198148\n",
      "2025-03-07 23:23: **********Val Epoch 840: average Loss: 1889.478516\n",
      "2025-03-07 23:23: Train Epoch 841: 0/23 Loss: 2.390621\n",
      "2025-03-07 23:23: Train Epoch 841: 20/23 Loss: 35.949123\n",
      "2025-03-07 23:23: **********Train Epoch 841: averaged Loss: 13.834538\n",
      "2025-03-07 23:23: **********Val Epoch 841: average Loss: 1821.602020\n",
      "2025-03-07 23:23: Train Epoch 842: 0/23 Loss: 2.104453\n",
      "2025-03-07 23:23: Train Epoch 842: 20/23 Loss: 35.946533\n",
      "2025-03-07 23:23: **********Train Epoch 842: averaged Loss: 11.798847\n",
      "2025-03-07 23:23: **********Val Epoch 842: average Loss: 1876.049072\n",
      "2025-03-07 23:23: Train Epoch 843: 0/23 Loss: 2.636411\n",
      "2025-03-07 23:23: Train Epoch 843: 20/23 Loss: 45.361992\n",
      "2025-03-07 23:23: **********Train Epoch 843: averaged Loss: 13.197622\n",
      "2025-03-07 23:23: **********Val Epoch 843: average Loss: 1829.205261\n",
      "2025-03-07 23:23: Train Epoch 844: 0/23 Loss: 0.963881\n",
      "2025-03-07 23:23: Train Epoch 844: 20/23 Loss: 35.607925\n",
      "2025-03-07 23:23: **********Train Epoch 844: averaged Loss: 11.991010\n",
      "2025-03-07 23:23: **********Val Epoch 844: average Loss: 1857.520508\n",
      "2025-03-07 23:23: Train Epoch 845: 0/23 Loss: 1.846360\n",
      "2025-03-07 23:23: Train Epoch 845: 20/23 Loss: 33.507755\n",
      "2025-03-07 23:23: **********Train Epoch 845: averaged Loss: 12.516358\n",
      "2025-03-07 23:23: **********Val Epoch 845: average Loss: 1843.152649\n",
      "2025-03-07 23:23: Train Epoch 846: 0/23 Loss: 1.344028\n",
      "2025-03-07 23:24: Train Epoch 846: 20/23 Loss: 33.976978\n",
      "2025-03-07 23:24: **********Train Epoch 846: averaged Loss: 11.460772\n",
      "2025-03-07 23:24: **********Val Epoch 846: average Loss: 1850.281250\n",
      "2025-03-07 23:24: Train Epoch 847: 0/23 Loss: 4.683174\n",
      "2025-03-07 23:24: Train Epoch 847: 20/23 Loss: 35.474831\n",
      "2025-03-07 23:24: **********Train Epoch 847: averaged Loss: 13.098485\n",
      "2025-03-07 23:24: **********Val Epoch 847: average Loss: 1843.662567\n",
      "2025-03-07 23:24: Train Epoch 848: 0/23 Loss: 0.944537\n",
      "2025-03-07 23:24: Train Epoch 848: 20/23 Loss: 33.691376\n",
      "2025-03-07 23:24: **********Train Epoch 848: averaged Loss: 11.885082\n",
      "2025-03-07 23:24: **********Val Epoch 848: average Loss: 1859.579956\n",
      "2025-03-07 23:24: Train Epoch 849: 0/23 Loss: 0.864952\n",
      "2025-03-07 23:24: Train Epoch 849: 20/23 Loss: 34.578094\n",
      "2025-03-07 23:24: **********Train Epoch 849: averaged Loss: 11.706551\n",
      "2025-03-07 23:24: **********Val Epoch 849: average Loss: 1845.094910\n",
      "2025-03-07 23:24: Train Epoch 850: 0/23 Loss: 3.092577\n",
      "2025-03-07 23:24: Train Epoch 850: 20/23 Loss: 35.349709\n",
      "2025-03-07 23:24: **********Train Epoch 850: averaged Loss: 11.800935\n",
      "2025-03-07 23:24: **********Val Epoch 850: average Loss: 1851.468506\n",
      "2025-03-07 23:24: Train Epoch 851: 0/23 Loss: 1.516627\n",
      "2025-03-07 23:24: Train Epoch 851: 20/23 Loss: 33.143921\n",
      "2025-03-07 23:24: **********Train Epoch 851: averaged Loss: 11.449232\n",
      "2025-03-07 23:24: **********Val Epoch 851: average Loss: 1815.892670\n",
      "2025-03-07 23:24: Train Epoch 852: 0/23 Loss: 1.514094\n",
      "2025-03-07 23:24: Train Epoch 852: 20/23 Loss: 40.688221\n",
      "2025-03-07 23:24: **********Train Epoch 852: averaged Loss: 13.300586\n",
      "2025-03-07 23:24: **********Val Epoch 852: average Loss: 1854.578369\n",
      "2025-03-07 23:24: Train Epoch 853: 0/23 Loss: 2.579395\n",
      "2025-03-07 23:24: Train Epoch 853: 20/23 Loss: 33.192955\n",
      "2025-03-07 23:24: **********Train Epoch 853: averaged Loss: 12.199813\n",
      "2025-03-07 23:24: **********Val Epoch 853: average Loss: 1842.910156\n",
      "2025-03-07 23:24: Train Epoch 854: 0/23 Loss: 0.829510\n",
      "2025-03-07 23:24: Train Epoch 854: 20/23 Loss: 36.272125\n",
      "2025-03-07 23:24: **********Train Epoch 854: averaged Loss: 12.251760\n",
      "2025-03-07 23:24: **********Val Epoch 854: average Loss: 1827.890381\n",
      "2025-03-07 23:24: Train Epoch 855: 0/23 Loss: 3.174559\n",
      "2025-03-07 23:24: Train Epoch 855: 20/23 Loss: 33.175053\n",
      "2025-03-07 23:24: **********Train Epoch 855: averaged Loss: 12.045839\n",
      "2025-03-07 23:24: **********Val Epoch 855: average Loss: 1888.478333\n",
      "2025-03-07 23:24: Train Epoch 856: 0/23 Loss: 1.542819\n",
      "2025-03-07 23:24: Train Epoch 856: 20/23 Loss: 43.824043\n",
      "2025-03-07 23:24: **********Train Epoch 856: averaged Loss: 12.325905\n",
      "2025-03-07 23:24: **********Val Epoch 856: average Loss: 1870.303528\n",
      "2025-03-07 23:24: Train Epoch 857: 0/23 Loss: 1.196668\n",
      "2025-03-07 23:24: Train Epoch 857: 20/23 Loss: 43.208889\n",
      "2025-03-07 23:24: **********Train Epoch 857: averaged Loss: 11.900292\n",
      "2025-03-07 23:24: **********Val Epoch 857: average Loss: 1850.838074\n",
      "2025-03-07 23:24: Train Epoch 858: 0/23 Loss: 1.527817\n",
      "2025-03-07 23:24: Train Epoch 858: 20/23 Loss: 47.381859\n",
      "2025-03-07 23:24: **********Train Epoch 858: averaged Loss: 12.519195\n",
      "2025-03-07 23:24: **********Val Epoch 858: average Loss: 1843.607971\n",
      "2025-03-07 23:24: Train Epoch 859: 0/23 Loss: 4.170379\n",
      "2025-03-07 23:24: Train Epoch 859: 20/23 Loss: 40.901733\n",
      "2025-03-07 23:24: **********Train Epoch 859: averaged Loss: 11.827503\n",
      "2025-03-07 23:24: **********Val Epoch 859: average Loss: 1832.779633\n",
      "2025-03-07 23:24: Train Epoch 860: 0/23 Loss: 3.816405\n",
      "2025-03-07 23:24: Train Epoch 860: 20/23 Loss: 32.105965\n",
      "2025-03-07 23:24: **********Train Epoch 860: averaged Loss: 11.670299\n",
      "2025-03-07 23:24: **********Val Epoch 860: average Loss: 1813.495911\n",
      "2025-03-07 23:24: Train Epoch 861: 0/23 Loss: 4.277978\n",
      "2025-03-07 23:24: Train Epoch 861: 20/23 Loss: 32.679714\n",
      "2025-03-07 23:24: **********Train Epoch 861: averaged Loss: 11.453853\n",
      "2025-03-07 23:24: **********Val Epoch 861: average Loss: 1823.223450\n",
      "2025-03-07 23:24: Train Epoch 862: 0/23 Loss: 4.244332\n",
      "2025-03-07 23:24: Train Epoch 862: 20/23 Loss: 38.682568\n",
      "2025-03-07 23:24: **********Train Epoch 862: averaged Loss: 11.333325\n",
      "2025-03-07 23:24: **********Val Epoch 862: average Loss: 1866.948975\n",
      "2025-03-07 23:24: Train Epoch 863: 0/23 Loss: 1.638200\n",
      "2025-03-07 23:24: Train Epoch 863: 20/23 Loss: 34.809509\n",
      "2025-03-07 23:24: **********Train Epoch 863: averaged Loss: 12.207949\n",
      "2025-03-07 23:24: **********Val Epoch 863: average Loss: 1853.056396\n",
      "2025-03-07 23:24: Train Epoch 864: 0/23 Loss: 4.141487\n",
      "2025-03-07 23:24: Train Epoch 864: 20/23 Loss: 47.055233\n",
      "2025-03-07 23:24: **********Train Epoch 864: averaged Loss: 13.363530\n",
      "2025-03-07 23:24: **********Val Epoch 864: average Loss: 1854.449677\n",
      "2025-03-07 23:24: Train Epoch 865: 0/23 Loss: 3.116853\n",
      "2025-03-07 23:24: Train Epoch 865: 20/23 Loss: 36.754478\n",
      "2025-03-07 23:24: **********Train Epoch 865: averaged Loss: 12.585229\n",
      "2025-03-07 23:24: **********Val Epoch 865: average Loss: 1834.841797\n",
      "2025-03-07 23:24: Train Epoch 866: 0/23 Loss: 0.824672\n",
      "2025-03-07 23:24: Train Epoch 866: 20/23 Loss: 41.751106\n",
      "2025-03-07 23:24: **********Train Epoch 866: averaged Loss: 13.670188\n",
      "2025-03-07 23:24: **********Val Epoch 866: average Loss: 1833.088928\n",
      "2025-03-07 23:24: Train Epoch 867: 0/23 Loss: 2.453327\n",
      "2025-03-07 23:24: Train Epoch 867: 20/23 Loss: 39.240284\n",
      "2025-03-07 23:24: **********Train Epoch 867: averaged Loss: 12.641912\n",
      "2025-03-07 23:24: **********Val Epoch 867: average Loss: 1825.112823\n",
      "2025-03-07 23:24: Train Epoch 868: 0/23 Loss: 1.574547\n",
      "2025-03-07 23:24: Train Epoch 868: 20/23 Loss: 33.254959\n",
      "2025-03-07 23:24: **********Train Epoch 868: averaged Loss: 11.691126\n",
      "2025-03-07 23:24: **********Val Epoch 868: average Loss: 1834.179382\n",
      "2025-03-07 23:24: Train Epoch 869: 0/23 Loss: 1.900685\n",
      "2025-03-07 23:24: Train Epoch 869: 20/23 Loss: 32.489616\n",
      "2025-03-07 23:24: **********Train Epoch 869: averaged Loss: 11.449301\n",
      "2025-03-07 23:24: **********Val Epoch 869: average Loss: 1845.574707\n",
      "2025-03-07 23:24: Train Epoch 870: 0/23 Loss: 3.928475\n",
      "2025-03-07 23:24: Train Epoch 870: 20/23 Loss: 34.734085\n",
      "2025-03-07 23:24: **********Train Epoch 870: averaged Loss: 12.557168\n",
      "2025-03-07 23:24: **********Val Epoch 870: average Loss: 1850.861816\n",
      "2025-03-07 23:24: Train Epoch 871: 0/23 Loss: 4.543745\n",
      "2025-03-07 23:24: Train Epoch 871: 20/23 Loss: 38.517632\n",
      "2025-03-07 23:24: **********Train Epoch 871: averaged Loss: 12.327976\n",
      "2025-03-07 23:24: **********Val Epoch 871: average Loss: 1870.250763\n",
      "2025-03-07 23:24: Train Epoch 872: 0/23 Loss: 3.360972\n",
      "2025-03-07 23:24: Train Epoch 872: 20/23 Loss: 33.624817\n",
      "2025-03-07 23:24: **********Train Epoch 872: averaged Loss: 13.236235\n",
      "2025-03-07 23:24: **********Val Epoch 872: average Loss: 1824.448120\n",
      "2025-03-07 23:24: Train Epoch 873: 0/23 Loss: 0.899398\n",
      "2025-03-07 23:24: Train Epoch 873: 20/23 Loss: 38.958046\n",
      "2025-03-07 23:24: **********Train Epoch 873: averaged Loss: 12.606193\n",
      "2025-03-07 23:24: **********Val Epoch 873: average Loss: 1871.938232\n",
      "2025-03-07 23:24: Train Epoch 874: 0/23 Loss: 2.485013\n",
      "2025-03-07 23:24: Train Epoch 874: 20/23 Loss: 32.649929\n",
      "2025-03-07 23:24: **********Train Epoch 874: averaged Loss: 11.397154\n",
      "2025-03-07 23:24: **********Val Epoch 874: average Loss: 1853.984894\n",
      "2025-03-07 23:24: Train Epoch 875: 0/23 Loss: 1.534228\n",
      "2025-03-07 23:24: Train Epoch 875: 20/23 Loss: 32.309784\n",
      "2025-03-07 23:24: **********Train Epoch 875: averaged Loss: 11.920366\n",
      "2025-03-07 23:24: **********Val Epoch 875: average Loss: 1866.418335\n",
      "2025-03-07 23:24: Train Epoch 876: 0/23 Loss: 3.147498\n",
      "2025-03-07 23:24: Train Epoch 876: 20/23 Loss: 34.777126\n",
      "2025-03-07 23:24: **********Train Epoch 876: averaged Loss: 11.592182\n",
      "2025-03-07 23:24: **********Val Epoch 876: average Loss: 1858.554443\n",
      "2025-03-07 23:24: Train Epoch 877: 0/23 Loss: 4.303045\n",
      "2025-03-07 23:24: Train Epoch 877: 20/23 Loss: 37.417946\n",
      "2025-03-07 23:24: **********Train Epoch 877: averaged Loss: 12.864899\n",
      "2025-03-07 23:24: **********Val Epoch 877: average Loss: 1853.416138\n",
      "2025-03-07 23:24: Train Epoch 878: 0/23 Loss: 1.053083\n",
      "2025-03-07 23:24: Train Epoch 878: 20/23 Loss: 43.249409\n",
      "2025-03-07 23:24: **********Train Epoch 878: averaged Loss: 13.735465\n",
      "2025-03-07 23:24: **********Val Epoch 878: average Loss: 1832.044006\n",
      "2025-03-07 23:24: Train Epoch 879: 0/23 Loss: 1.342549\n",
      "2025-03-07 23:24: Train Epoch 879: 20/23 Loss: 43.117447\n",
      "2025-03-07 23:24: **********Train Epoch 879: averaged Loss: 11.856867\n",
      "2025-03-07 23:24: **********Val Epoch 879: average Loss: 1866.736145\n",
      "2025-03-07 23:24: Train Epoch 880: 0/23 Loss: 1.083791\n",
      "2025-03-07 23:24: Train Epoch 880: 20/23 Loss: 32.047813\n",
      "2025-03-07 23:24: **********Train Epoch 880: averaged Loss: 11.461970\n",
      "2025-03-07 23:24: **********Val Epoch 880: average Loss: 1861.394623\n",
      "2025-03-07 23:24: Train Epoch 881: 0/23 Loss: 5.665855\n",
      "2025-03-07 23:24: Train Epoch 881: 20/23 Loss: 31.918129\n",
      "2025-03-07 23:24: **********Train Epoch 881: averaged Loss: 13.083235\n",
      "2025-03-07 23:24: **********Val Epoch 881: average Loss: 1885.768311\n",
      "2025-03-07 23:24: Train Epoch 882: 0/23 Loss: 5.048234\n",
      "2025-03-07 23:25: Train Epoch 882: 20/23 Loss: 38.830856\n",
      "2025-03-07 23:25: **********Train Epoch 882: averaged Loss: 13.232573\n",
      "2025-03-07 23:25: **********Val Epoch 882: average Loss: 1810.808624\n",
      "2025-03-07 23:25: Train Epoch 883: 0/23 Loss: 2.276528\n",
      "2025-03-07 23:25: Train Epoch 883: 20/23 Loss: 33.858940\n",
      "2025-03-07 23:25: **********Train Epoch 883: averaged Loss: 11.263580\n",
      "2025-03-07 23:25: **********Val Epoch 883: average Loss: 1876.700989\n",
      "2025-03-07 23:25: Train Epoch 884: 0/23 Loss: 0.948246\n",
      "2025-03-07 23:25: Train Epoch 884: 20/23 Loss: 66.123672\n",
      "2025-03-07 23:25: **********Train Epoch 884: averaged Loss: 13.566921\n",
      "2025-03-07 23:25: **********Val Epoch 884: average Loss: 1831.595459\n",
      "2025-03-07 23:25: Train Epoch 885: 0/23 Loss: 4.538916\n",
      "2025-03-07 23:25: Train Epoch 885: 20/23 Loss: 32.653481\n",
      "2025-03-07 23:25: **********Train Epoch 885: averaged Loss: 11.887126\n",
      "2025-03-07 23:25: **********Val Epoch 885: average Loss: 1838.030029\n",
      "2025-03-07 23:25: Train Epoch 886: 0/23 Loss: 2.606605\n",
      "2025-03-07 23:25: Train Epoch 886: 20/23 Loss: 32.417145\n",
      "2025-03-07 23:25: **********Train Epoch 886: averaged Loss: 11.532072\n",
      "2025-03-07 23:25: **********Val Epoch 886: average Loss: 1860.381592\n",
      "2025-03-07 23:25: Train Epoch 887: 0/23 Loss: 3.699019\n",
      "2025-03-07 23:25: Train Epoch 887: 20/23 Loss: 32.787712\n",
      "2025-03-07 23:25: **********Train Epoch 887: averaged Loss: 11.608079\n",
      "2025-03-07 23:25: **********Val Epoch 887: average Loss: 1835.545349\n",
      "2025-03-07 23:25: Train Epoch 888: 0/23 Loss: 7.245923\n",
      "2025-03-07 23:25: Train Epoch 888: 20/23 Loss: 33.253628\n",
      "2025-03-07 23:25: **********Train Epoch 888: averaged Loss: 11.641735\n",
      "2025-03-07 23:25: **********Val Epoch 888: average Loss: 1831.216217\n",
      "2025-03-07 23:25: Train Epoch 889: 0/23 Loss: 0.884212\n",
      "2025-03-07 23:25: Train Epoch 889: 20/23 Loss: 44.453453\n",
      "2025-03-07 23:25: **********Train Epoch 889: averaged Loss: 12.119479\n",
      "2025-03-07 23:25: **********Val Epoch 889: average Loss: 1839.968567\n",
      "2025-03-07 23:25: Train Epoch 890: 0/23 Loss: 3.197755\n",
      "2025-03-07 23:25: Train Epoch 890: 20/23 Loss: 32.064148\n",
      "2025-03-07 23:25: **********Train Epoch 890: averaged Loss: 14.173212\n",
      "2025-03-07 23:25: **********Val Epoch 890: average Loss: 1845.406738\n",
      "2025-03-07 23:25: Train Epoch 891: 0/23 Loss: 1.139281\n",
      "2025-03-07 23:25: Train Epoch 891: 20/23 Loss: 43.768299\n",
      "2025-03-07 23:25: **********Train Epoch 891: averaged Loss: 11.935673\n",
      "2025-03-07 23:25: **********Val Epoch 891: average Loss: 1858.374329\n",
      "2025-03-07 23:25: Train Epoch 892: 0/23 Loss: 4.857005\n",
      "2025-03-07 23:25: Train Epoch 892: 20/23 Loss: 31.858154\n",
      "2025-03-07 23:25: **********Train Epoch 892: averaged Loss: 12.102015\n",
      "2025-03-07 23:25: **********Val Epoch 892: average Loss: 1863.084717\n",
      "2025-03-07 23:25: Train Epoch 893: 0/23 Loss: 1.038072\n",
      "2025-03-07 23:25: Train Epoch 893: 20/23 Loss: 37.692204\n",
      "2025-03-07 23:25: **********Train Epoch 893: averaged Loss: 11.809957\n",
      "2025-03-07 23:25: **********Val Epoch 893: average Loss: 1857.373688\n",
      "2025-03-07 23:25: Train Epoch 894: 0/23 Loss: 5.270991\n",
      "2025-03-07 23:25: Train Epoch 894: 20/23 Loss: 38.990379\n",
      "2025-03-07 23:25: **********Train Epoch 894: averaged Loss: 12.291354\n",
      "2025-03-07 23:25: **********Val Epoch 894: average Loss: 1854.038086\n",
      "2025-03-07 23:25: Train Epoch 895: 0/23 Loss: 2.529697\n",
      "2025-03-07 23:25: Train Epoch 895: 20/23 Loss: 32.602924\n",
      "2025-03-07 23:25: **********Train Epoch 895: averaged Loss: 11.941598\n",
      "2025-03-07 23:25: **********Val Epoch 895: average Loss: 1831.124725\n",
      "2025-03-07 23:25: Train Epoch 896: 0/23 Loss: 0.881429\n",
      "2025-03-07 23:25: Train Epoch 896: 20/23 Loss: 42.527100\n",
      "2025-03-07 23:25: **********Train Epoch 896: averaged Loss: 12.940487\n",
      "2025-03-07 23:25: **********Val Epoch 896: average Loss: 1865.829529\n",
      "2025-03-07 23:25: Train Epoch 897: 0/23 Loss: 0.896886\n",
      "2025-03-07 23:25: Train Epoch 897: 20/23 Loss: 39.342064\n",
      "2025-03-07 23:25: **********Train Epoch 897: averaged Loss: 12.228335\n",
      "2025-03-07 23:25: **********Val Epoch 897: average Loss: 1843.366302\n",
      "2025-03-07 23:25: Train Epoch 898: 0/23 Loss: 1.331542\n",
      "2025-03-07 23:25: Train Epoch 898: 20/23 Loss: 32.811737\n",
      "2025-03-07 23:25: **********Train Epoch 898: averaged Loss: 11.688258\n",
      "2025-03-07 23:25: **********Val Epoch 898: average Loss: 1816.347412\n",
      "2025-03-07 23:25: Train Epoch 899: 0/23 Loss: 3.378886\n",
      "2025-03-07 23:25: Train Epoch 899: 20/23 Loss: 48.024048\n",
      "2025-03-07 23:25: **********Train Epoch 899: averaged Loss: 12.520694\n",
      "2025-03-07 23:25: **********Val Epoch 899: average Loss: 1836.552002\n",
      "2025-03-07 23:25: Train Epoch 900: 0/23 Loss: 1.222609\n",
      "2025-03-07 23:25: Train Epoch 900: 20/23 Loss: 45.241562\n",
      "2025-03-07 23:25: **********Train Epoch 900: averaged Loss: 13.186390\n",
      "2025-03-07 23:25: **********Val Epoch 900: average Loss: 1821.066711\n",
      "2025-03-07 23:25: Train Epoch 901: 0/23 Loss: 1.263821\n",
      "2025-03-07 23:25: Train Epoch 901: 20/23 Loss: 36.877216\n",
      "2025-03-07 23:25: **********Train Epoch 901: averaged Loss: 12.683902\n",
      "2025-03-07 23:25: **********Val Epoch 901: average Loss: 1869.290649\n",
      "2025-03-07 23:25: Train Epoch 902: 0/23 Loss: 1.147621\n",
      "2025-03-07 23:25: Train Epoch 902: 20/23 Loss: 32.737251\n",
      "2025-03-07 23:25: **********Train Epoch 902: averaged Loss: 12.854416\n",
      "2025-03-07 23:25: **********Val Epoch 902: average Loss: 1832.824615\n",
      "2025-03-07 23:25: Train Epoch 903: 0/23 Loss: 1.036897\n",
      "2025-03-07 23:25: Train Epoch 903: 20/23 Loss: 44.598557\n",
      "2025-03-07 23:25: **********Train Epoch 903: averaged Loss: 12.411056\n",
      "2025-03-07 23:25: **********Val Epoch 903: average Loss: 1830.845093\n",
      "2025-03-07 23:25: Train Epoch 904: 0/23 Loss: 1.761388\n",
      "2025-03-07 23:25: Train Epoch 904: 20/23 Loss: 31.938118\n",
      "2025-03-07 23:25: **********Train Epoch 904: averaged Loss: 10.658537\n",
      "2025-03-07 23:25: **********Val Epoch 904: average Loss: 1830.830078\n",
      "2025-03-07 23:25: Train Epoch 905: 0/23 Loss: 2.513051\n",
      "2025-03-07 23:25: Train Epoch 905: 20/23 Loss: 32.420719\n",
      "2025-03-07 23:25: **********Train Epoch 905: averaged Loss: 11.462729\n",
      "2025-03-07 23:25: **********Val Epoch 905: average Loss: 1874.213562\n",
      "2025-03-07 23:25: Train Epoch 906: 0/23 Loss: 2.695560\n",
      "2025-03-07 23:25: Train Epoch 906: 20/23 Loss: 31.556837\n",
      "2025-03-07 23:25: **********Train Epoch 906: averaged Loss: 11.715791\n",
      "2025-03-07 23:25: **********Val Epoch 906: average Loss: 1833.557037\n",
      "2025-03-07 23:25: Train Epoch 907: 0/23 Loss: 0.799994\n",
      "2025-03-07 23:25: Train Epoch 907: 20/23 Loss: 34.696598\n",
      "2025-03-07 23:25: **********Train Epoch 907: averaged Loss: 12.337962\n",
      "2025-03-07 23:25: **********Val Epoch 907: average Loss: 1849.458191\n",
      "2025-03-07 23:25: Train Epoch 908: 0/23 Loss: 1.445543\n",
      "2025-03-07 23:25: Train Epoch 908: 20/23 Loss: 36.596962\n",
      "2025-03-07 23:25: **********Train Epoch 908: averaged Loss: 12.929554\n",
      "2025-03-07 23:25: **********Val Epoch 908: average Loss: 1861.975464\n",
      "2025-03-07 23:25: Train Epoch 909: 0/23 Loss: 0.897264\n",
      "2025-03-07 23:25: Train Epoch 909: 20/23 Loss: 33.114407\n",
      "2025-03-07 23:25: **********Train Epoch 909: averaged Loss: 11.989823\n",
      "2025-03-07 23:25: **********Val Epoch 909: average Loss: 1836.346191\n",
      "2025-03-07 23:25: Train Epoch 910: 0/23 Loss: 1.796704\n",
      "2025-03-07 23:25: Train Epoch 910: 20/23 Loss: 39.270653\n",
      "2025-03-07 23:25: **********Train Epoch 910: averaged Loss: 12.338764\n",
      "2025-03-07 23:25: **********Val Epoch 910: average Loss: 1845.461426\n",
      "2025-03-07 23:25: Train Epoch 911: 0/23 Loss: 1.107759\n",
      "2025-03-07 23:25: Train Epoch 911: 20/23 Loss: 31.392725\n",
      "2025-03-07 23:25: **********Train Epoch 911: averaged Loss: 11.353662\n",
      "2025-03-07 23:25: **********Val Epoch 911: average Loss: 1864.353394\n",
      "2025-03-07 23:25: Train Epoch 912: 0/23 Loss: 0.905078\n",
      "2025-03-07 23:25: Train Epoch 912: 20/23 Loss: 42.443390\n",
      "2025-03-07 23:25: **********Train Epoch 912: averaged Loss: 11.771048\n",
      "2025-03-07 23:25: **********Val Epoch 912: average Loss: 1871.835938\n",
      "2025-03-07 23:25: Train Epoch 913: 0/23 Loss: 0.918988\n",
      "2025-03-07 23:25: Train Epoch 913: 20/23 Loss: 32.714771\n",
      "2025-03-07 23:25: **********Train Epoch 913: averaged Loss: 10.993952\n",
      "2025-03-07 23:25: **********Val Epoch 913: average Loss: 1852.792358\n",
      "2025-03-07 23:25: Train Epoch 914: 0/23 Loss: 1.105215\n",
      "2025-03-07 23:25: Train Epoch 914: 20/23 Loss: 36.009281\n",
      "2025-03-07 23:25: **********Train Epoch 914: averaged Loss: 11.476514\n",
      "2025-03-07 23:25: **********Val Epoch 914: average Loss: 1849.850494\n",
      "2025-03-07 23:25: Train Epoch 915: 0/23 Loss: 1.193624\n",
      "2025-03-07 23:25: Train Epoch 915: 20/23 Loss: 32.902008\n",
      "2025-03-07 23:25: **********Train Epoch 915: averaged Loss: 12.941789\n",
      "2025-03-07 23:25: **********Val Epoch 915: average Loss: 1832.225769\n",
      "2025-03-07 23:25: Train Epoch 916: 0/23 Loss: 1.785001\n",
      "2025-03-07 23:25: Train Epoch 916: 20/23 Loss: 38.095177\n",
      "2025-03-07 23:25: **********Train Epoch 916: averaged Loss: 12.130112\n",
      "2025-03-07 23:25: **********Val Epoch 916: average Loss: 1860.186707\n",
      "2025-03-07 23:25: Train Epoch 917: 0/23 Loss: 2.467233\n",
      "2025-03-07 23:25: Train Epoch 917: 20/23 Loss: 32.335194\n",
      "2025-03-07 23:25: **********Train Epoch 917: averaged Loss: 13.551408\n",
      "2025-03-07 23:25: **********Val Epoch 917: average Loss: 1823.035400\n",
      "2025-03-07 23:26: Train Epoch 918: 0/23 Loss: 3.066070\n",
      "2025-03-07 23:26: Train Epoch 918: 20/23 Loss: 37.688416\n",
      "2025-03-07 23:26: **********Train Epoch 918: averaged Loss: 12.119345\n",
      "2025-03-07 23:26: **********Val Epoch 918: average Loss: 1870.431213\n",
      "2025-03-07 23:26: Train Epoch 919: 0/23 Loss: 1.470653\n",
      "2025-03-07 23:26: Train Epoch 919: 20/23 Loss: 32.474556\n",
      "2025-03-07 23:26: **********Train Epoch 919: averaged Loss: 11.514932\n",
      "2025-03-07 23:26: **********Val Epoch 919: average Loss: 1843.135681\n",
      "2025-03-07 23:26: Train Epoch 920: 0/23 Loss: 0.893164\n",
      "2025-03-07 23:26: Train Epoch 920: 20/23 Loss: 30.909184\n",
      "2025-03-07 23:26: **********Train Epoch 920: averaged Loss: 11.278909\n",
      "2025-03-07 23:26: **********Val Epoch 920: average Loss: 1867.925415\n",
      "2025-03-07 23:26: Train Epoch 921: 0/23 Loss: 1.462764\n",
      "2025-03-07 23:26: Train Epoch 921: 20/23 Loss: 37.189163\n",
      "2025-03-07 23:26: **********Train Epoch 921: averaged Loss: 11.374250\n",
      "2025-03-07 23:26: **********Val Epoch 921: average Loss: 1861.730713\n",
      "2025-03-07 23:26: Train Epoch 922: 0/23 Loss: 1.260391\n",
      "2025-03-07 23:26: Train Epoch 922: 20/23 Loss: 32.808468\n",
      "2025-03-07 23:26: **********Train Epoch 922: averaged Loss: 10.919467\n",
      "2025-03-07 23:26: **********Val Epoch 922: average Loss: 1842.853455\n",
      "2025-03-07 23:26: Train Epoch 923: 0/23 Loss: 2.831387\n",
      "2025-03-07 23:26: Train Epoch 923: 20/23 Loss: 34.817112\n",
      "2025-03-07 23:26: **********Train Epoch 923: averaged Loss: 11.617126\n",
      "2025-03-07 23:26: **********Val Epoch 923: average Loss: 1853.419006\n",
      "2025-03-07 23:26: Train Epoch 924: 0/23 Loss: 0.966841\n",
      "2025-03-07 23:26: Train Epoch 924: 20/23 Loss: 44.028934\n",
      "2025-03-07 23:26: **********Train Epoch 924: averaged Loss: 11.581093\n",
      "2025-03-07 23:26: **********Val Epoch 924: average Loss: 1851.134033\n",
      "2025-03-07 23:26: Train Epoch 925: 0/23 Loss: 2.916541\n",
      "2025-03-07 23:26: Train Epoch 925: 20/23 Loss: 31.040684\n",
      "2025-03-07 23:26: **********Train Epoch 925: averaged Loss: 12.009948\n",
      "2025-03-07 23:26: **********Val Epoch 925: average Loss: 1832.191284\n",
      "2025-03-07 23:26: Train Epoch 926: 0/23 Loss: 0.996244\n",
      "2025-03-07 23:26: Train Epoch 926: 20/23 Loss: 48.207146\n",
      "2025-03-07 23:26: **********Train Epoch 926: averaged Loss: 11.613212\n",
      "2025-03-07 23:26: **********Val Epoch 926: average Loss: 1864.726318\n",
      "2025-03-07 23:26: Train Epoch 927: 0/23 Loss: 0.987718\n",
      "2025-03-07 23:26: Train Epoch 927: 20/23 Loss: 33.992096\n",
      "2025-03-07 23:26: **********Train Epoch 927: averaged Loss: 11.670492\n",
      "2025-03-07 23:26: **********Val Epoch 927: average Loss: 1834.457397\n",
      "2025-03-07 23:26: Train Epoch 928: 0/23 Loss: 2.666494\n",
      "2025-03-07 23:26: Train Epoch 928: 20/23 Loss: 42.605698\n",
      "2025-03-07 23:26: **********Train Epoch 928: averaged Loss: 12.366648\n",
      "2025-03-07 23:26: **********Val Epoch 928: average Loss: 1868.717834\n",
      "2025-03-07 23:26: Validation performance didn't improve for 200 epochs. Training stops.\n",
      "2025-03-07 23:26: Total training time: 26.1413min, best loss: 1783.318420\n",
      "2025-03-07 23:26: Average Horizon, MAE: 528.2721, MSE: 1007.0141\n",
      "2025-03-07 23:26: Average Horizon, MAE: 543.1746, MSE: 1034.4224\n",
      "/tmp/ipykernel_498255/1939091181.py:186: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y_p=np.array(y1[:,0,:].cpu())\n",
      "/tmp/ipykernel_498255/1939091181.py:188: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y_t=np.array(y2[:,0,:].cpu())\n",
      "/tmp/ipykernel_498255/1939091181.py:225: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  f.write(str(np.array(IC)))\n",
      "/tmp/ipykernel_498255/1939091181.py:227: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  f.write(str(np.array(RIC)))\n",
      "/tmp/ipykernel_498255/1939091181.py:229: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  f.write(str(np.array(mae)))\n",
      "/tmp/ipykernel_498255/1939091181.py:231: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  f.write(str(np.array(rmse)))\n",
      "2025-03-07 23:26: Experiment log path in: ./\n",
      "2025-03-07 23:26: Experiment log path in: ./\n",
      "2025-03-07 23:26: Train Epoch 1: 0/23 Loss: 3.106956\n",
      "2025-03-07 23:26: Train Epoch 1: 0/23 Loss: 3.106956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([17, 10]) True\n",
      "embed_w torch.Size([10, 10]) True\n",
      "weights_pool torch.Size([10, 3, 5, 1]) True\n",
      "bias_pool torch.Size([10, 1]) True\n",
      "mam1.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers.3.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.3.mixer.D torch.Size([64]) True\n",
      "mam1.layers.3.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.3.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.3.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.3.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.3.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.3.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.3.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.3.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.3.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.3.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.3.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.3.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.3.norm.weight torch.Size([32]) True\n",
      "mam1.layers.3.norm.bias torch.Size([32]) True\n",
      "mam1.layers.4.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.4.mixer.D torch.Size([64]) True\n",
      "mam1.layers.4.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers.4.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.4.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.4.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.4.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.4.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.4.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.4.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.4.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers.4.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.4.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.4.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.4.norm.weight torch.Size([32]) True\n",
      "mam1.layers.4.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.3.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.3.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.3.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.3.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.3.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.3.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.3.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.3.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.3.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.3.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.3.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.3.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.3.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.3.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.3.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.3.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.4.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.4.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.4.mixer.embedding.weight torch.Size([32, 17]) True\n",
      "mam1.layers2.4.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.4.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.4.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.4.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.4.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.4.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.4.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.4.mixer.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.layers2.4.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.4.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.4.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.4.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.4.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.lin.3.0.weight torch.Size([5]) True\n",
      "mam1.lin.3.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.3.1.bias torch.Size([32]) True\n",
      "mam1.lin.3.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.3.3.bias torch.Size([5]) True\n",
      "mam1.lin.4.0.weight torch.Size([5]) True\n",
      "mam1.lin.4.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.4.1.bias torch.Size([32]) True\n",
      "mam1.lin.4.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.4.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([17, 32]) True\n",
      "mam1.lm_head.bias torch.Size([17]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([17]) True\n",
      "mam1.nnl.bias torch.Size([17]) True\n",
      "proj.weight torch.Size([1, 17]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 350102\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 23:26: Train Epoch 1: 20/23 Loss: 1107.653564\n",
      "2025-03-07 23:26: Train Epoch 1: 20/23 Loss: 1107.653564\n",
      "2025-03-07 23:26: **********Train Epoch 1: averaged Loss: 277.624926\n",
      "2025-03-07 23:26: **********Train Epoch 1: averaged Loss: 277.624926\n",
      "2025-03-07 23:26: **********Val Epoch 1: average Loss: 3598.509766\n",
      "2025-03-07 23:26: **********Val Epoch 1: average Loss: 3598.509766\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 2: 0/23 Loss: 0.921634\n",
      "2025-03-07 23:26: Train Epoch 2: 0/23 Loss: 0.921634\n",
      "2025-03-07 23:26: Train Epoch 2: 20/23 Loss: 1106.925049\n",
      "2025-03-07 23:26: Train Epoch 2: 20/23 Loss: 1106.925049\n",
      "2025-03-07 23:26: **********Train Epoch 2: averaged Loss: 276.170107\n",
      "2025-03-07 23:26: **********Train Epoch 2: averaged Loss: 276.170107\n",
      "2025-03-07 23:26: **********Val Epoch 2: average Loss: 3597.874756\n",
      "2025-03-07 23:26: **********Val Epoch 2: average Loss: 3597.874756\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 3: 0/23 Loss: 0.422423\n",
      "2025-03-07 23:26: Train Epoch 3: 0/23 Loss: 0.422423\n",
      "2025-03-07 23:26: Train Epoch 3: 20/23 Loss: 1106.592041\n",
      "2025-03-07 23:26: Train Epoch 3: 20/23 Loss: 1106.592041\n",
      "2025-03-07 23:26: **********Train Epoch 3: averaged Loss: 275.258217\n",
      "2025-03-07 23:26: **********Train Epoch 3: averaged Loss: 275.258217\n",
      "2025-03-07 23:26: **********Val Epoch 3: average Loss: 3597.541382\n",
      "2025-03-07 23:26: **********Val Epoch 3: average Loss: 3597.541382\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 4: 0/23 Loss: 0.647302\n",
      "2025-03-07 23:26: Train Epoch 4: 0/23 Loss: 0.647302\n",
      "2025-03-07 23:26: Train Epoch 4: 20/23 Loss: 1105.738647\n",
      "2025-03-07 23:26: Train Epoch 4: 20/23 Loss: 1105.738647\n",
      "2025-03-07 23:26: **********Train Epoch 4: averaged Loss: 274.170030\n",
      "2025-03-07 23:26: **********Train Epoch 4: averaged Loss: 274.170030\n",
      "2025-03-07 23:26: **********Val Epoch 4: average Loss: 3596.988770\n",
      "2025-03-07 23:26: **********Val Epoch 4: average Loss: 3596.988770\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 5: 0/23 Loss: 1.973935\n",
      "2025-03-07 23:26: Train Epoch 5: 0/23 Loss: 1.973935\n",
      "2025-03-07 23:26: Train Epoch 5: 20/23 Loss: 1105.146729\n",
      "2025-03-07 23:26: Train Epoch 5: 20/23 Loss: 1105.146729\n",
      "2025-03-07 23:26: **********Train Epoch 5: averaged Loss: 273.301557\n",
      "2025-03-07 23:26: **********Train Epoch 5: averaged Loss: 273.301557\n",
      "2025-03-07 23:26: **********Val Epoch 5: average Loss: 3596.695190\n",
      "2025-03-07 23:26: **********Val Epoch 5: average Loss: 3596.695190\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 6: 0/23 Loss: 3.515634\n",
      "2025-03-07 23:26: Train Epoch 6: 0/23 Loss: 3.515634\n",
      "2025-03-07 23:26: Train Epoch 6: 20/23 Loss: 1104.594482\n",
      "2025-03-07 23:26: Train Epoch 6: 20/23 Loss: 1104.594482\n",
      "2025-03-07 23:26: **********Train Epoch 6: averaged Loss: 272.506416\n",
      "2025-03-07 23:26: **********Train Epoch 6: averaged Loss: 272.506416\n",
      "2025-03-07 23:26: **********Val Epoch 6: average Loss: 3596.272705\n",
      "2025-03-07 23:26: **********Val Epoch 6: average Loss: 3596.272705\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 7: 0/23 Loss: 3.672874\n",
      "2025-03-07 23:26: Train Epoch 7: 0/23 Loss: 3.672874\n",
      "2025-03-07 23:26: Train Epoch 7: 20/23 Loss: 1104.004150\n",
      "2025-03-07 23:26: Train Epoch 7: 20/23 Loss: 1104.004150\n",
      "2025-03-07 23:26: **********Train Epoch 7: averaged Loss: 271.956435\n",
      "2025-03-07 23:26: **********Train Epoch 7: averaged Loss: 271.956435\n",
      "2025-03-07 23:26: **********Val Epoch 7: average Loss: 3596.089844\n",
      "2025-03-07 23:26: **********Val Epoch 7: average Loss: 3596.089844\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 8: 0/23 Loss: 4.070707\n",
      "2025-03-07 23:26: Train Epoch 8: 0/23 Loss: 4.070707\n",
      "2025-03-07 23:26: Train Epoch 8: 20/23 Loss: 1102.276123\n",
      "2025-03-07 23:26: Train Epoch 8: 20/23 Loss: 1102.276123\n",
      "2025-03-07 23:26: **********Train Epoch 8: averaged Loss: 271.583635\n",
      "2025-03-07 23:26: **********Train Epoch 8: averaged Loss: 271.583635\n",
      "2025-03-07 23:26: **********Val Epoch 8: average Loss: 3595.648438\n",
      "2025-03-07 23:26: **********Val Epoch 8: average Loss: 3595.648438\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 9: 0/23 Loss: 5.199443\n",
      "2025-03-07 23:26: Train Epoch 9: 0/23 Loss: 5.199443\n",
      "2025-03-07 23:26: Train Epoch 9: 20/23 Loss: 1102.049072\n",
      "2025-03-07 23:26: Train Epoch 9: 20/23 Loss: 1102.049072\n",
      "2025-03-07 23:26: **********Train Epoch 9: averaged Loss: 271.462732\n",
      "2025-03-07 23:26: **********Train Epoch 9: averaged Loss: 271.462732\n",
      "2025-03-07 23:26: **********Val Epoch 9: average Loss: 3594.917603\n",
      "2025-03-07 23:26: **********Val Epoch 9: average Loss: 3594.917603\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 10: 0/23 Loss: 5.128876\n",
      "2025-03-07 23:26: Train Epoch 10: 0/23 Loss: 5.128876\n",
      "2025-03-07 23:26: Train Epoch 10: 20/23 Loss: 1100.955078\n",
      "2025-03-07 23:26: Train Epoch 10: 20/23 Loss: 1100.955078\n",
      "2025-03-07 23:26: **********Train Epoch 10: averaged Loss: 270.977985\n",
      "2025-03-07 23:26: **********Train Epoch 10: averaged Loss: 270.977985\n",
      "2025-03-07 23:26: **********Val Epoch 10: average Loss: 3594.640137\n",
      "2025-03-07 23:26: **********Val Epoch 10: average Loss: 3594.640137\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 11: 0/23 Loss: 5.473138\n",
      "2025-03-07 23:26: Train Epoch 11: 0/23 Loss: 5.473138\n",
      "2025-03-07 23:26: Train Epoch 11: 20/23 Loss: 1100.345703\n",
      "2025-03-07 23:26: Train Epoch 11: 20/23 Loss: 1100.345703\n",
      "2025-03-07 23:26: **********Train Epoch 11: averaged Loss: 270.732719\n",
      "2025-03-07 23:26: **********Train Epoch 11: averaged Loss: 270.732719\n",
      "2025-03-07 23:26: **********Val Epoch 11: average Loss: 3593.876709\n",
      "2025-03-07 23:26: **********Val Epoch 11: average Loss: 3593.876709\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 12: 0/23 Loss: 4.713720\n",
      "2025-03-07 23:26: Train Epoch 12: 0/23 Loss: 4.713720\n",
      "2025-03-07 23:26: Train Epoch 12: 20/23 Loss: 1099.489990\n",
      "2025-03-07 23:26: Train Epoch 12: 20/23 Loss: 1099.489990\n",
      "2025-03-07 23:26: **********Train Epoch 12: averaged Loss: 270.270499\n",
      "2025-03-07 23:26: **********Train Epoch 12: averaged Loss: 270.270499\n",
      "2025-03-07 23:26: **********Val Epoch 12: average Loss: 3593.694458\n",
      "2025-03-07 23:26: **********Val Epoch 12: average Loss: 3593.694458\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 13: 0/23 Loss: 4.334900\n",
      "2025-03-07 23:26: Train Epoch 13: 0/23 Loss: 4.334900\n",
      "2025-03-07 23:26: Train Epoch 13: 20/23 Loss: 1098.978760\n",
      "2025-03-07 23:26: Train Epoch 13: 20/23 Loss: 1098.978760\n",
      "2025-03-07 23:26: **********Train Epoch 13: averaged Loss: 269.877139\n",
      "2025-03-07 23:26: **********Train Epoch 13: averaged Loss: 269.877139\n",
      "2025-03-07 23:26: **********Val Epoch 13: average Loss: 3593.355103\n",
      "2025-03-07 23:26: **********Val Epoch 13: average Loss: 3593.355103\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 14: 0/23 Loss: 3.782547\n",
      "2025-03-07 23:26: Train Epoch 14: 0/23 Loss: 3.782547\n",
      "2025-03-07 23:26: Train Epoch 14: 20/23 Loss: 1097.622070\n",
      "2025-03-07 23:26: Train Epoch 14: 20/23 Loss: 1097.622070\n",
      "2025-03-07 23:26: **********Train Epoch 14: averaged Loss: 269.309746\n",
      "2025-03-07 23:26: **********Train Epoch 14: averaged Loss: 269.309746\n",
      "2025-03-07 23:26: **********Val Epoch 14: average Loss: 3592.889160\n",
      "2025-03-07 23:26: **********Val Epoch 14: average Loss: 3592.889160\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 15: 0/23 Loss: 4.118354\n",
      "2025-03-07 23:26: Train Epoch 15: 0/23 Loss: 4.118354\n",
      "2025-03-07 23:26: Train Epoch 15: 20/23 Loss: 1098.032349\n",
      "2025-03-07 23:26: Train Epoch 15: 20/23 Loss: 1098.032349\n",
      "2025-03-07 23:26: **********Train Epoch 15: averaged Loss: 269.106873\n",
      "2025-03-07 23:26: **********Train Epoch 15: averaged Loss: 269.106873\n",
      "2025-03-07 23:26: **********Val Epoch 15: average Loss: 3592.190674\n",
      "2025-03-07 23:26: **********Val Epoch 15: average Loss: 3592.190674\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 16: 0/23 Loss: 3.289281\n",
      "2025-03-07 23:26: Train Epoch 16: 0/23 Loss: 3.289281\n",
      "2025-03-07 23:26: Train Epoch 16: 20/23 Loss: 1095.055908\n",
      "2025-03-07 23:26: Train Epoch 16: 20/23 Loss: 1095.055908\n",
      "2025-03-07 23:26: **********Train Epoch 16: averaged Loss: 268.335911\n",
      "2025-03-07 23:26: **********Train Epoch 16: averaged Loss: 268.335911\n",
      "2025-03-07 23:26: **********Val Epoch 16: average Loss: 3591.117432\n",
      "2025-03-07 23:26: **********Val Epoch 16: average Loss: 3591.117432\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 17: 0/23 Loss: 4.212209\n",
      "2025-03-07 23:26: Train Epoch 17: 0/23 Loss: 4.212209\n",
      "2025-03-07 23:26: Train Epoch 17: 20/23 Loss: 1095.384521\n",
      "2025-03-07 23:26: Train Epoch 17: 20/23 Loss: 1095.384521\n",
      "2025-03-07 23:26: **********Train Epoch 17: averaged Loss: 267.979709\n",
      "2025-03-07 23:26: **********Train Epoch 17: averaged Loss: 267.979709\n",
      "2025-03-07 23:26: **********Val Epoch 17: average Loss: 3590.482788\n",
      "2025-03-07 23:26: **********Val Epoch 17: average Loss: 3590.482788\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 18: 0/23 Loss: 2.977087\n",
      "2025-03-07 23:26: Train Epoch 18: 0/23 Loss: 2.977087\n",
      "2025-03-07 23:26: Train Epoch 18: 20/23 Loss: 1093.438721\n",
      "2025-03-07 23:26: Train Epoch 18: 20/23 Loss: 1093.438721\n",
      "2025-03-07 23:26: **********Train Epoch 18: averaged Loss: 267.452841\n",
      "2025-03-07 23:26: **********Train Epoch 18: averaged Loss: 267.452841\n",
      "2025-03-07 23:26: **********Val Epoch 18: average Loss: 3589.718994\n",
      "2025-03-07 23:26: **********Val Epoch 18: average Loss: 3589.718994\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 19: 0/23 Loss: 2.633194\n",
      "2025-03-07 23:26: Train Epoch 19: 0/23 Loss: 2.633194\n",
      "2025-03-07 23:26: Train Epoch 19: 20/23 Loss: 1093.697754\n",
      "2025-03-07 23:26: Train Epoch 19: 20/23 Loss: 1093.697754\n",
      "2025-03-07 23:26: **********Train Epoch 19: averaged Loss: 266.990565\n",
      "2025-03-07 23:26: **********Train Epoch 19: averaged Loss: 266.990565\n",
      "2025-03-07 23:26: **********Val Epoch 19: average Loss: 3589.063721\n",
      "2025-03-07 23:26: **********Val Epoch 19: average Loss: 3589.063721\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 20: 0/23 Loss: 3.518078\n",
      "2025-03-07 23:26: Train Epoch 20: 0/23 Loss: 3.518078\n",
      "2025-03-07 23:26: Train Epoch 20: 20/23 Loss: 1091.524902\n",
      "2025-03-07 23:26: Train Epoch 20: 20/23 Loss: 1091.524902\n",
      "2025-03-07 23:26: **********Train Epoch 20: averaged Loss: 266.530218\n",
      "2025-03-07 23:26: **********Train Epoch 20: averaged Loss: 266.530218\n",
      "2025-03-07 23:26: **********Val Epoch 20: average Loss: 3588.019043\n",
      "2025-03-07 23:26: **********Val Epoch 20: average Loss: 3588.019043\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 21: 0/23 Loss: 3.389830\n",
      "2025-03-07 23:26: Train Epoch 21: 0/23 Loss: 3.389830\n",
      "2025-03-07 23:26: Train Epoch 21: 20/23 Loss: 1091.831543\n",
      "2025-03-07 23:26: Train Epoch 21: 20/23 Loss: 1091.831543\n",
      "2025-03-07 23:26: **********Train Epoch 21: averaged Loss: 266.127972\n",
      "2025-03-07 23:26: **********Train Epoch 21: averaged Loss: 266.127972\n",
      "2025-03-07 23:26: **********Val Epoch 21: average Loss: 3587.276367\n",
      "2025-03-07 23:26: **********Val Epoch 21: average Loss: 3587.276367\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 22: 0/23 Loss: 3.008259\n",
      "2025-03-07 23:26: Train Epoch 22: 0/23 Loss: 3.008259\n",
      "2025-03-07 23:26: Train Epoch 22: 20/23 Loss: 1090.302002\n",
      "2025-03-07 23:26: Train Epoch 22: 20/23 Loss: 1090.302002\n",
      "2025-03-07 23:26: **********Train Epoch 22: averaged Loss: 265.422550\n",
      "2025-03-07 23:26: **********Train Epoch 22: averaged Loss: 265.422550\n",
      "2025-03-07 23:26: **********Val Epoch 22: average Loss: 3586.617920\n",
      "2025-03-07 23:26: **********Val Epoch 22: average Loss: 3586.617920\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: *********************************Current best model saved!\n",
      "2025-03-07 23:26: Train Epoch 23: 0/23 Loss: 2.740239\n",
      "2025-03-07 23:26: Train Epoch 23: 0/23 Loss: 2.740239\n",
      "2025-03-07 23:27: Train Epoch 23: 20/23 Loss: 1091.020752\n",
      "2025-03-07 23:27: Train Epoch 23: 20/23 Loss: 1091.020752\n",
      "2025-03-07 23:27: **********Train Epoch 23: averaged Loss: 264.840120\n",
      "2025-03-07 23:27: **********Train Epoch 23: averaged Loss: 264.840120\n",
      "2025-03-07 23:27: **********Val Epoch 23: average Loss: 3585.479004\n",
      "2025-03-07 23:27: **********Val Epoch 23: average Loss: 3585.479004\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 24: 0/23 Loss: 3.380089\n",
      "2025-03-07 23:27: Train Epoch 24: 0/23 Loss: 3.380089\n",
      "2025-03-07 23:27: Train Epoch 24: 20/23 Loss: 1087.977539\n",
      "2025-03-07 23:27: Train Epoch 24: 20/23 Loss: 1087.977539\n",
      "2025-03-07 23:27: **********Train Epoch 24: averaged Loss: 264.572840\n",
      "2025-03-07 23:27: **********Train Epoch 24: averaged Loss: 264.572840\n",
      "2025-03-07 23:27: **********Val Epoch 24: average Loss: 3583.771484\n",
      "2025-03-07 23:27: **********Val Epoch 24: average Loss: 3583.771484\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 25: 0/23 Loss: 2.048100\n",
      "2025-03-07 23:27: Train Epoch 25: 0/23 Loss: 2.048100\n",
      "2025-03-07 23:27: Train Epoch 25: 20/23 Loss: 1087.710571\n",
      "2025-03-07 23:27: Train Epoch 25: 20/23 Loss: 1087.710571\n",
      "2025-03-07 23:27: **********Train Epoch 25: averaged Loss: 263.958341\n",
      "2025-03-07 23:27: **********Train Epoch 25: averaged Loss: 263.958341\n",
      "2025-03-07 23:27: **********Val Epoch 25: average Loss: 3582.523193\n",
      "2025-03-07 23:27: **********Val Epoch 25: average Loss: 3582.523193\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 26: 0/23 Loss: 4.712395\n",
      "2025-03-07 23:27: Train Epoch 26: 0/23 Loss: 4.712395\n",
      "2025-03-07 23:27: Train Epoch 26: 20/23 Loss: 1086.724365\n",
      "2025-03-07 23:27: Train Epoch 26: 20/23 Loss: 1086.724365\n",
      "2025-03-07 23:27: **********Train Epoch 26: averaged Loss: 263.528259\n",
      "2025-03-07 23:27: **********Train Epoch 26: averaged Loss: 263.528259\n",
      "2025-03-07 23:27: **********Val Epoch 26: average Loss: 3582.778076\n",
      "2025-03-07 23:27: **********Val Epoch 26: average Loss: 3582.778076\n",
      "2025-03-07 23:27: Train Epoch 27: 0/23 Loss: 2.867641\n",
      "2025-03-07 23:27: Train Epoch 27: 0/23 Loss: 2.867641\n",
      "2025-03-07 23:27: Train Epoch 27: 20/23 Loss: 1085.044922\n",
      "2025-03-07 23:27: Train Epoch 27: 20/23 Loss: 1085.044922\n",
      "2025-03-07 23:27: **********Train Epoch 27: averaged Loss: 263.016125\n",
      "2025-03-07 23:27: **********Train Epoch 27: averaged Loss: 263.016125\n",
      "2025-03-07 23:27: **********Val Epoch 27: average Loss: 3581.401733\n",
      "2025-03-07 23:27: **********Val Epoch 27: average Loss: 3581.401733\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 28: 0/23 Loss: 3.337917\n",
      "2025-03-07 23:27: Train Epoch 28: 0/23 Loss: 3.337917\n",
      "2025-03-07 23:27: Train Epoch 28: 20/23 Loss: 1084.754761\n",
      "2025-03-07 23:27: Train Epoch 28: 20/23 Loss: 1084.754761\n",
      "2025-03-07 23:27: **********Train Epoch 28: averaged Loss: 262.506596\n",
      "2025-03-07 23:27: **********Train Epoch 28: averaged Loss: 262.506596\n",
      "2025-03-07 23:27: **********Val Epoch 28: average Loss: 3579.831665\n",
      "2025-03-07 23:27: **********Val Epoch 28: average Loss: 3579.831665\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 29: 0/23 Loss: 4.658237\n",
      "2025-03-07 23:27: Train Epoch 29: 0/23 Loss: 4.658237\n",
      "2025-03-07 23:27: Train Epoch 29: 20/23 Loss: 1083.035645\n",
      "2025-03-07 23:27: Train Epoch 29: 20/23 Loss: 1083.035645\n",
      "2025-03-07 23:27: **********Train Epoch 29: averaged Loss: 262.403263\n",
      "2025-03-07 23:27: **********Train Epoch 29: averaged Loss: 262.403263\n",
      "2025-03-07 23:27: **********Val Epoch 29: average Loss: 3578.284912\n",
      "2025-03-07 23:27: **********Val Epoch 29: average Loss: 3578.284912\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 30: 0/23 Loss: 4.033113\n",
      "2025-03-07 23:27: Train Epoch 30: 0/23 Loss: 4.033113\n",
      "2025-03-07 23:27: Train Epoch 30: 20/23 Loss: 1083.321289\n",
      "2025-03-07 23:27: Train Epoch 30: 20/23 Loss: 1083.321289\n",
      "2025-03-07 23:27: **********Train Epoch 30: averaged Loss: 261.896225\n",
      "2025-03-07 23:27: **********Train Epoch 30: averaged Loss: 261.896225\n",
      "2025-03-07 23:27: **********Val Epoch 30: average Loss: 3576.531006\n",
      "2025-03-07 23:27: **********Val Epoch 30: average Loss: 3576.531006\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 31: 0/23 Loss: 5.736600\n",
      "2025-03-07 23:27: Train Epoch 31: 0/23 Loss: 5.736600\n",
      "2025-03-07 23:27: Train Epoch 31: 20/23 Loss: 1081.291748\n",
      "2025-03-07 23:27: Train Epoch 31: 20/23 Loss: 1081.291748\n",
      "2025-03-07 23:27: **********Train Epoch 31: averaged Loss: 261.011927\n",
      "2025-03-07 23:27: **********Train Epoch 31: averaged Loss: 261.011927\n",
      "2025-03-07 23:27: **********Val Epoch 31: average Loss: 3575.493896\n",
      "2025-03-07 23:27: **********Val Epoch 31: average Loss: 3575.493896\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 32: 0/23 Loss: 3.584795\n",
      "2025-03-07 23:27: Train Epoch 32: 0/23 Loss: 3.584795\n",
      "2025-03-07 23:27: Train Epoch 32: 20/23 Loss: 1079.189941\n",
      "2025-03-07 23:27: Train Epoch 32: 20/23 Loss: 1079.189941\n",
      "2025-03-07 23:27: **********Train Epoch 32: averaged Loss: 259.750132\n",
      "2025-03-07 23:27: **********Train Epoch 32: averaged Loss: 259.750132\n",
      "2025-03-07 23:27: **********Val Epoch 32: average Loss: 3575.008179\n",
      "2025-03-07 23:27: **********Val Epoch 32: average Loss: 3575.008179\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 33: 0/23 Loss: 4.562124\n",
      "2025-03-07 23:27: Train Epoch 33: 0/23 Loss: 4.562124\n",
      "2025-03-07 23:27: Train Epoch 33: 20/23 Loss: 1077.161987\n",
      "2025-03-07 23:27: Train Epoch 33: 20/23 Loss: 1077.161987\n",
      "2025-03-07 23:27: **********Train Epoch 33: averaged Loss: 259.225606\n",
      "2025-03-07 23:27: **********Train Epoch 33: averaged Loss: 259.225606\n",
      "2025-03-07 23:27: **********Val Epoch 33: average Loss: 3573.113281\n",
      "2025-03-07 23:27: **********Val Epoch 33: average Loss: 3573.113281\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 34: 0/23 Loss: 4.393794\n",
      "2025-03-07 23:27: Train Epoch 34: 0/23 Loss: 4.393794\n",
      "2025-03-07 23:27: Train Epoch 34: 20/23 Loss: 1074.742188\n",
      "2025-03-07 23:27: Train Epoch 34: 20/23 Loss: 1074.742188\n",
      "2025-03-07 23:27: **********Train Epoch 34: averaged Loss: 258.358705\n",
      "2025-03-07 23:27: **********Train Epoch 34: averaged Loss: 258.358705\n",
      "2025-03-07 23:27: **********Val Epoch 34: average Loss: 3572.483887\n",
      "2025-03-07 23:27: **********Val Epoch 34: average Loss: 3572.483887\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 35: 0/23 Loss: 4.625338\n",
      "2025-03-07 23:27: Train Epoch 35: 0/23 Loss: 4.625338\n",
      "2025-03-07 23:27: Train Epoch 35: 20/23 Loss: 1073.547363\n",
      "2025-03-07 23:27: Train Epoch 35: 20/23 Loss: 1073.547363\n",
      "2025-03-07 23:27: **********Train Epoch 35: averaged Loss: 257.464623\n",
      "2025-03-07 23:27: **********Train Epoch 35: averaged Loss: 257.464623\n",
      "2025-03-07 23:27: **********Val Epoch 35: average Loss: 3569.587769\n",
      "2025-03-07 23:27: **********Val Epoch 35: average Loss: 3569.587769\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 36: 0/23 Loss: 3.681878\n",
      "2025-03-07 23:27: Train Epoch 36: 0/23 Loss: 3.681878\n",
      "2025-03-07 23:27: Train Epoch 36: 20/23 Loss: 1073.954224\n",
      "2025-03-07 23:27: Train Epoch 36: 20/23 Loss: 1073.954224\n",
      "2025-03-07 23:27: **********Train Epoch 36: averaged Loss: 257.055043\n",
      "2025-03-07 23:27: **********Train Epoch 36: averaged Loss: 257.055043\n",
      "2025-03-07 23:27: **********Val Epoch 36: average Loss: 3568.066284\n",
      "2025-03-07 23:27: **********Val Epoch 36: average Loss: 3568.066284\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 37: 0/23 Loss: 3.815023\n",
      "2025-03-07 23:27: Train Epoch 37: 0/23 Loss: 3.815023\n",
      "2025-03-07 23:27: Train Epoch 37: 20/23 Loss: 1070.393921\n",
      "2025-03-07 23:27: Train Epoch 37: 20/23 Loss: 1070.393921\n",
      "2025-03-07 23:27: **********Train Epoch 37: averaged Loss: 256.204739\n",
      "2025-03-07 23:27: **********Train Epoch 37: averaged Loss: 256.204739\n",
      "2025-03-07 23:27: **********Val Epoch 37: average Loss: 3564.624756\n",
      "2025-03-07 23:27: **********Val Epoch 37: average Loss: 3564.624756\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 38: 0/23 Loss: 3.183375\n",
      "2025-03-07 23:27: Train Epoch 38: 0/23 Loss: 3.183375\n",
      "2025-03-07 23:27: Train Epoch 38: 20/23 Loss: 1069.711182\n",
      "2025-03-07 23:27: Train Epoch 38: 20/23 Loss: 1069.711182\n",
      "2025-03-07 23:27: **********Train Epoch 38: averaged Loss: 255.217287\n",
      "2025-03-07 23:27: **********Train Epoch 38: averaged Loss: 255.217287\n",
      "2025-03-07 23:27: **********Val Epoch 38: average Loss: 3566.227783\n",
      "2025-03-07 23:27: **********Val Epoch 38: average Loss: 3566.227783\n",
      "2025-03-07 23:27: Train Epoch 39: 0/23 Loss: 3.212117\n",
      "2025-03-07 23:27: Train Epoch 39: 0/23 Loss: 3.212117\n",
      "2025-03-07 23:27: Train Epoch 39: 20/23 Loss: 1066.337646\n",
      "2025-03-07 23:27: Train Epoch 39: 20/23 Loss: 1066.337646\n",
      "2025-03-07 23:27: **********Train Epoch 39: averaged Loss: 254.143359\n",
      "2025-03-07 23:27: **********Train Epoch 39: averaged Loss: 254.143359\n",
      "2025-03-07 23:27: **********Val Epoch 39: average Loss: 3562.497559\n",
      "2025-03-07 23:27: **********Val Epoch 39: average Loss: 3562.497559\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 40: 0/23 Loss: 5.131054\n",
      "2025-03-07 23:27: Train Epoch 40: 0/23 Loss: 5.131054\n",
      "2025-03-07 23:27: Train Epoch 40: 20/23 Loss: 1066.813965\n",
      "2025-03-07 23:27: Train Epoch 40: 20/23 Loss: 1066.813965\n",
      "2025-03-07 23:27: **********Train Epoch 40: averaged Loss: 254.063584\n",
      "2025-03-07 23:27: **********Train Epoch 40: averaged Loss: 254.063584\n",
      "2025-03-07 23:27: **********Val Epoch 40: average Loss: 3560.796509\n",
      "2025-03-07 23:27: **********Val Epoch 40: average Loss: 3560.796509\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 41: 0/23 Loss: 1.734451\n",
      "2025-03-07 23:27: Train Epoch 41: 0/23 Loss: 1.734451\n",
      "2025-03-07 23:27: Train Epoch 41: 20/23 Loss: 1064.300293\n",
      "2025-03-07 23:27: Train Epoch 41: 20/23 Loss: 1064.300293\n",
      "2025-03-07 23:27: **********Train Epoch 41: averaged Loss: 252.906612\n",
      "2025-03-07 23:27: **********Train Epoch 41: averaged Loss: 252.906612\n",
      "2025-03-07 23:27: **********Val Epoch 41: average Loss: 3559.627441\n",
      "2025-03-07 23:27: **********Val Epoch 41: average Loss: 3559.627441\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 42: 0/23 Loss: 5.834429\n",
      "2025-03-07 23:27: Train Epoch 42: 0/23 Loss: 5.834429\n",
      "2025-03-07 23:27: Train Epoch 42: 20/23 Loss: 1060.974243\n",
      "2025-03-07 23:27: Train Epoch 42: 20/23 Loss: 1060.974243\n",
      "2025-03-07 23:27: **********Train Epoch 42: averaged Loss: 252.851944\n",
      "2025-03-07 23:27: **********Train Epoch 42: averaged Loss: 252.851944\n",
      "2025-03-07 23:27: **********Val Epoch 42: average Loss: 3557.479126\n",
      "2025-03-07 23:27: **********Val Epoch 42: average Loss: 3557.479126\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 43: 0/23 Loss: 3.049366\n",
      "2025-03-07 23:27: Train Epoch 43: 0/23 Loss: 3.049366\n",
      "2025-03-07 23:27: Train Epoch 43: 20/23 Loss: 1062.768066\n",
      "2025-03-07 23:27: Train Epoch 43: 20/23 Loss: 1062.768066\n",
      "2025-03-07 23:27: **********Train Epoch 43: averaged Loss: 251.254111\n",
      "2025-03-07 23:27: **********Train Epoch 43: averaged Loss: 251.254111\n",
      "2025-03-07 23:27: **********Val Epoch 43: average Loss: 3554.587646\n",
      "2025-03-07 23:27: **********Val Epoch 43: average Loss: 3554.587646\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 44: 0/23 Loss: 5.577075\n",
      "2025-03-07 23:27: Train Epoch 44: 0/23 Loss: 5.577075\n",
      "2025-03-07 23:27: Train Epoch 44: 20/23 Loss: 1057.385498\n",
      "2025-03-07 23:27: Train Epoch 44: 20/23 Loss: 1057.385498\n",
      "2025-03-07 23:27: **********Train Epoch 44: averaged Loss: 250.638502\n",
      "2025-03-07 23:27: **********Train Epoch 44: averaged Loss: 250.638502\n",
      "2025-03-07 23:27: **********Val Epoch 44: average Loss: 3553.005371\n",
      "2025-03-07 23:27: **********Val Epoch 44: average Loss: 3553.005371\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 45: 0/23 Loss: 3.549021\n",
      "2025-03-07 23:27: Train Epoch 45: 0/23 Loss: 3.549021\n",
      "2025-03-07 23:27: Train Epoch 45: 20/23 Loss: 1051.077148\n",
      "2025-03-07 23:27: Train Epoch 45: 20/23 Loss: 1051.077148\n",
      "2025-03-07 23:27: **********Train Epoch 45: averaged Loss: 249.245868\n",
      "2025-03-07 23:27: **********Train Epoch 45: averaged Loss: 249.245868\n",
      "2025-03-07 23:27: **********Val Epoch 45: average Loss: 3549.328491\n",
      "2025-03-07 23:27: **********Val Epoch 45: average Loss: 3549.328491\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 46: 0/23 Loss: 6.186530\n",
      "2025-03-07 23:27: Train Epoch 46: 0/23 Loss: 6.186530\n",
      "2025-03-07 23:27: Train Epoch 46: 20/23 Loss: 1051.894287\n",
      "2025-03-07 23:27: Train Epoch 46: 20/23 Loss: 1051.894287\n",
      "2025-03-07 23:27: **********Train Epoch 46: averaged Loss: 248.224044\n",
      "2025-03-07 23:27: **********Train Epoch 46: averaged Loss: 248.224044\n",
      "2025-03-07 23:27: **********Val Epoch 46: average Loss: 3546.137329\n",
      "2025-03-07 23:27: **********Val Epoch 46: average Loss: 3546.137329\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 47: 0/23 Loss: 3.304649\n",
      "2025-03-07 23:27: Train Epoch 47: 0/23 Loss: 3.304649\n",
      "2025-03-07 23:27: Train Epoch 47: 20/23 Loss: 1053.449463\n",
      "2025-03-07 23:27: Train Epoch 47: 20/23 Loss: 1053.449463\n",
      "2025-03-07 23:27: **********Train Epoch 47: averaged Loss: 246.704797\n",
      "2025-03-07 23:27: **********Train Epoch 47: averaged Loss: 246.704797\n",
      "2025-03-07 23:27: **********Val Epoch 47: average Loss: 3542.087158\n",
      "2025-03-07 23:27: **********Val Epoch 47: average Loss: 3542.087158\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 48: 0/23 Loss: 3.991697\n",
      "2025-03-07 23:27: Train Epoch 48: 0/23 Loss: 3.991697\n",
      "2025-03-07 23:27: Train Epoch 48: 20/23 Loss: 1052.557495\n",
      "2025-03-07 23:27: Train Epoch 48: 20/23 Loss: 1052.557495\n",
      "2025-03-07 23:27: **********Train Epoch 48: averaged Loss: 247.549808\n",
      "2025-03-07 23:27: **********Train Epoch 48: averaged Loss: 247.549808\n",
      "2025-03-07 23:27: **********Val Epoch 48: average Loss: 3541.374512\n",
      "2025-03-07 23:27: **********Val Epoch 48: average Loss: 3541.374512\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 49: 0/23 Loss: 1.996976\n",
      "2025-03-07 23:27: Train Epoch 49: 0/23 Loss: 1.996976\n",
      "2025-03-07 23:27: Train Epoch 49: 20/23 Loss: 1047.694946\n",
      "2025-03-07 23:27: Train Epoch 49: 20/23 Loss: 1047.694946\n",
      "2025-03-07 23:27: **********Train Epoch 49: averaged Loss: 244.527053\n",
      "2025-03-07 23:27: **********Train Epoch 49: averaged Loss: 244.527053\n",
      "2025-03-07 23:27: **********Val Epoch 49: average Loss: 3538.621704\n",
      "2025-03-07 23:27: **********Val Epoch 49: average Loss: 3538.621704\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 50: 0/23 Loss: 4.724993\n",
      "2025-03-07 23:27: Train Epoch 50: 0/23 Loss: 4.724993\n",
      "2025-03-07 23:27: Train Epoch 50: 20/23 Loss: 1043.535767\n",
      "2025-03-07 23:27: Train Epoch 50: 20/23 Loss: 1043.535767\n",
      "2025-03-07 23:27: **********Train Epoch 50: averaged Loss: 242.468565\n",
      "2025-03-07 23:27: **********Train Epoch 50: averaged Loss: 242.468565\n",
      "2025-03-07 23:27: **********Val Epoch 50: average Loss: 3538.666504\n",
      "2025-03-07 23:27: **********Val Epoch 50: average Loss: 3538.666504\n",
      "2025-03-07 23:27: Train Epoch 51: 0/23 Loss: 6.023497\n",
      "2025-03-07 23:27: Train Epoch 51: 0/23 Loss: 6.023497\n",
      "2025-03-07 23:27: Train Epoch 51: 20/23 Loss: 1038.918945\n",
      "2025-03-07 23:27: Train Epoch 51: 20/23 Loss: 1038.918945\n",
      "2025-03-07 23:27: **********Train Epoch 51: averaged Loss: 243.275515\n",
      "2025-03-07 23:27: **********Train Epoch 51: averaged Loss: 243.275515\n",
      "2025-03-07 23:27: **********Val Epoch 51: average Loss: 3532.643066\n",
      "2025-03-07 23:27: **********Val Epoch 51: average Loss: 3532.643066\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 52: 0/23 Loss: 2.942169\n",
      "2025-03-07 23:27: Train Epoch 52: 0/23 Loss: 2.942169\n",
      "2025-03-07 23:27: Train Epoch 52: 20/23 Loss: 1031.953857\n",
      "2025-03-07 23:27: Train Epoch 52: 20/23 Loss: 1031.953857\n",
      "2025-03-07 23:27: **********Train Epoch 52: averaged Loss: 240.891883\n",
      "2025-03-07 23:27: **********Train Epoch 52: averaged Loss: 240.891883\n",
      "2025-03-07 23:27: **********Val Epoch 52: average Loss: 3530.002930\n",
      "2025-03-07 23:27: **********Val Epoch 52: average Loss: 3530.002930\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 53: 0/23 Loss: 6.165688\n",
      "2025-03-07 23:27: Train Epoch 53: 0/23 Loss: 6.165688\n",
      "2025-03-07 23:27: Train Epoch 53: 20/23 Loss: 1028.961914\n",
      "2025-03-07 23:27: Train Epoch 53: 20/23 Loss: 1028.961914\n",
      "2025-03-07 23:27: **********Train Epoch 53: averaged Loss: 241.648020\n",
      "2025-03-07 23:27: **********Train Epoch 53: averaged Loss: 241.648020\n",
      "2025-03-07 23:27: **********Val Epoch 53: average Loss: 3526.007446\n",
      "2025-03-07 23:27: **********Val Epoch 53: average Loss: 3526.007446\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 54: 0/23 Loss: 2.247442\n",
      "2025-03-07 23:27: Train Epoch 54: 0/23 Loss: 2.247442\n",
      "2025-03-07 23:27: Train Epoch 54: 20/23 Loss: 1031.738037\n",
      "2025-03-07 23:27: Train Epoch 54: 20/23 Loss: 1031.738037\n",
      "2025-03-07 23:27: **********Train Epoch 54: averaged Loss: 238.805457\n",
      "2025-03-07 23:27: **********Train Epoch 54: averaged Loss: 238.805457\n",
      "2025-03-07 23:27: **********Val Epoch 54: average Loss: 3523.773193\n",
      "2025-03-07 23:27: **********Val Epoch 54: average Loss: 3523.773193\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 55: 0/23 Loss: 4.820499\n",
      "2025-03-07 23:27: Train Epoch 55: 0/23 Loss: 4.820499\n",
      "2025-03-07 23:27: Train Epoch 55: 20/23 Loss: 1024.471436\n",
      "2025-03-07 23:27: Train Epoch 55: 20/23 Loss: 1024.471436\n",
      "2025-03-07 23:27: **********Train Epoch 55: averaged Loss: 240.073426\n",
      "2025-03-07 23:27: **********Train Epoch 55: averaged Loss: 240.073426\n",
      "2025-03-07 23:27: **********Val Epoch 55: average Loss: 3522.160156\n",
      "2025-03-07 23:27: **********Val Epoch 55: average Loss: 3522.160156\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 56: 0/23 Loss: 6.554668\n",
      "2025-03-07 23:27: Train Epoch 56: 0/23 Loss: 6.554668\n",
      "2025-03-07 23:27: Train Epoch 56: 20/23 Loss: 1018.143982\n",
      "2025-03-07 23:27: Train Epoch 56: 20/23 Loss: 1018.143982\n",
      "2025-03-07 23:27: **********Train Epoch 56: averaged Loss: 235.894851\n",
      "2025-03-07 23:27: **********Train Epoch 56: averaged Loss: 235.894851\n",
      "2025-03-07 23:27: **********Val Epoch 56: average Loss: 3520.752563\n",
      "2025-03-07 23:27: **********Val Epoch 56: average Loss: 3520.752563\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: *********************************Current best model saved!\n",
      "2025-03-07 23:27: Train Epoch 57: 0/23 Loss: 4.588136\n",
      "2025-03-07 23:27: Train Epoch 57: 0/23 Loss: 4.588136\n",
      "2025-03-07 23:28: Train Epoch 57: 20/23 Loss: 1021.104614\n",
      "2025-03-07 23:28: Train Epoch 57: 20/23 Loss: 1021.104614\n",
      "2025-03-07 23:28: **********Train Epoch 57: averaged Loss: 236.870690\n",
      "2025-03-07 23:28: **********Train Epoch 57: averaged Loss: 236.870690\n",
      "2025-03-07 23:28: **********Val Epoch 57: average Loss: 3517.268677\n",
      "2025-03-07 23:28: **********Val Epoch 57: average Loss: 3517.268677\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 58: 0/23 Loss: 4.537154\n",
      "2025-03-07 23:28: Train Epoch 58: 0/23 Loss: 4.537154\n",
      "2025-03-07 23:28: Train Epoch 58: 20/23 Loss: 1019.805176\n",
      "2025-03-07 23:28: Train Epoch 58: 20/23 Loss: 1019.805176\n",
      "2025-03-07 23:28: **********Train Epoch 58: averaged Loss: 234.082050\n",
      "2025-03-07 23:28: **********Train Epoch 58: averaged Loss: 234.082050\n",
      "2025-03-07 23:28: **********Val Epoch 58: average Loss: 3507.864258\n",
      "2025-03-07 23:28: **********Val Epoch 58: average Loss: 3507.864258\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 59: 0/23 Loss: 3.536475\n",
      "2025-03-07 23:28: Train Epoch 59: 0/23 Loss: 3.536475\n",
      "2025-03-07 23:28: Train Epoch 59: 20/23 Loss: 1014.972900\n",
      "2025-03-07 23:28: Train Epoch 59: 20/23 Loss: 1014.972900\n",
      "2025-03-07 23:28: **********Train Epoch 59: averaged Loss: 235.089880\n",
      "2025-03-07 23:28: **********Train Epoch 59: averaged Loss: 235.089880\n",
      "2025-03-07 23:28: **********Val Epoch 59: average Loss: 3509.613525\n",
      "2025-03-07 23:28: **********Val Epoch 59: average Loss: 3509.613525\n",
      "2025-03-07 23:28: Train Epoch 60: 0/23 Loss: 3.247714\n",
      "2025-03-07 23:28: Train Epoch 60: 0/23 Loss: 3.247714\n",
      "2025-03-07 23:28: Train Epoch 60: 20/23 Loss: 1010.772583\n",
      "2025-03-07 23:28: Train Epoch 60: 20/23 Loss: 1010.772583\n",
      "2025-03-07 23:28: **********Train Epoch 60: averaged Loss: 231.520035\n",
      "2025-03-07 23:28: **********Train Epoch 60: averaged Loss: 231.520035\n",
      "2025-03-07 23:28: **********Val Epoch 60: average Loss: 3504.561768\n",
      "2025-03-07 23:28: **********Val Epoch 60: average Loss: 3504.561768\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 61: 0/23 Loss: 1.947336\n",
      "2025-03-07 23:28: Train Epoch 61: 0/23 Loss: 1.947336\n",
      "2025-03-07 23:28: Train Epoch 61: 20/23 Loss: 1005.989380\n",
      "2025-03-07 23:28: Train Epoch 61: 20/23 Loss: 1005.989380\n",
      "2025-03-07 23:28: **********Train Epoch 61: averaged Loss: 231.572458\n",
      "2025-03-07 23:28: **********Train Epoch 61: averaged Loss: 231.572458\n",
      "2025-03-07 23:28: **********Val Epoch 61: average Loss: 3498.902344\n",
      "2025-03-07 23:28: **********Val Epoch 61: average Loss: 3498.902344\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 62: 0/23 Loss: 6.085371\n",
      "2025-03-07 23:28: Train Epoch 62: 0/23 Loss: 6.085371\n",
      "2025-03-07 23:28: Train Epoch 62: 20/23 Loss: 1005.512817\n",
      "2025-03-07 23:28: Train Epoch 62: 20/23 Loss: 1005.512817\n",
      "2025-03-07 23:28: **********Train Epoch 62: averaged Loss: 230.058277\n",
      "2025-03-07 23:28: **********Train Epoch 62: averaged Loss: 230.058277\n",
      "2025-03-07 23:28: **********Val Epoch 62: average Loss: 3500.000488\n",
      "2025-03-07 23:28: **********Val Epoch 62: average Loss: 3500.000488\n",
      "2025-03-07 23:28: Train Epoch 63: 0/23 Loss: 3.572732\n",
      "2025-03-07 23:28: Train Epoch 63: 0/23 Loss: 3.572732\n",
      "2025-03-07 23:28: Train Epoch 63: 20/23 Loss: 999.928528\n",
      "2025-03-07 23:28: Train Epoch 63: 20/23 Loss: 999.928528\n",
      "2025-03-07 23:28: **********Train Epoch 63: averaged Loss: 229.061457\n",
      "2025-03-07 23:28: **********Train Epoch 63: averaged Loss: 229.061457\n",
      "2025-03-07 23:28: **********Val Epoch 63: average Loss: 3494.449463\n",
      "2025-03-07 23:28: **********Val Epoch 63: average Loss: 3494.449463\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 64: 0/23 Loss: 3.368153\n",
      "2025-03-07 23:28: Train Epoch 64: 0/23 Loss: 3.368153\n",
      "2025-03-07 23:28: Train Epoch 64: 20/23 Loss: 994.673706\n",
      "2025-03-07 23:28: Train Epoch 64: 20/23 Loss: 994.673706\n",
      "2025-03-07 23:28: **********Train Epoch 64: averaged Loss: 229.665953\n",
      "2025-03-07 23:28: **********Train Epoch 64: averaged Loss: 229.665953\n",
      "2025-03-07 23:28: **********Val Epoch 64: average Loss: 3495.799561\n",
      "2025-03-07 23:28: **********Val Epoch 64: average Loss: 3495.799561\n",
      "2025-03-07 23:28: Train Epoch 65: 0/23 Loss: 1.800336\n",
      "2025-03-07 23:28: Train Epoch 65: 0/23 Loss: 1.800336\n",
      "2025-03-07 23:28: Train Epoch 65: 20/23 Loss: 995.055664\n",
      "2025-03-07 23:28: Train Epoch 65: 20/23 Loss: 995.055664\n",
      "2025-03-07 23:28: **********Train Epoch 65: averaged Loss: 232.516856\n",
      "2025-03-07 23:28: **********Train Epoch 65: averaged Loss: 232.516856\n",
      "2025-03-07 23:28: **********Val Epoch 65: average Loss: 3494.670166\n",
      "2025-03-07 23:28: **********Val Epoch 65: average Loss: 3494.670166\n",
      "2025-03-07 23:28: Train Epoch 66: 0/23 Loss: 4.743573\n",
      "2025-03-07 23:28: Train Epoch 66: 0/23 Loss: 4.743573\n",
      "2025-03-07 23:28: Train Epoch 66: 20/23 Loss: 992.360474\n",
      "2025-03-07 23:28: Train Epoch 66: 20/23 Loss: 992.360474\n",
      "2025-03-07 23:28: **********Train Epoch 66: averaged Loss: 228.126818\n",
      "2025-03-07 23:28: **********Train Epoch 66: averaged Loss: 228.126818\n",
      "2025-03-07 23:28: **********Val Epoch 66: average Loss: 3488.057739\n",
      "2025-03-07 23:28: **********Val Epoch 66: average Loss: 3488.057739\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 67: 0/23 Loss: 4.760640\n",
      "2025-03-07 23:28: Train Epoch 67: 0/23 Loss: 4.760640\n",
      "2025-03-07 23:28: Train Epoch 67: 20/23 Loss: 992.314636\n",
      "2025-03-07 23:28: Train Epoch 67: 20/23 Loss: 992.314636\n",
      "2025-03-07 23:28: **********Train Epoch 67: averaged Loss: 226.069564\n",
      "2025-03-07 23:28: **********Train Epoch 67: averaged Loss: 226.069564\n",
      "2025-03-07 23:28: **********Val Epoch 67: average Loss: 3487.587402\n",
      "2025-03-07 23:28: **********Val Epoch 67: average Loss: 3487.587402\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 68: 0/23 Loss: 5.325206\n",
      "2025-03-07 23:28: Train Epoch 68: 0/23 Loss: 5.325206\n",
      "2025-03-07 23:28: Train Epoch 68: 20/23 Loss: 982.074829\n",
      "2025-03-07 23:28: Train Epoch 68: 20/23 Loss: 982.074829\n",
      "2025-03-07 23:28: **********Train Epoch 68: averaged Loss: 224.843374\n",
      "2025-03-07 23:28: **********Train Epoch 68: averaged Loss: 224.843374\n",
      "2025-03-07 23:28: **********Val Epoch 68: average Loss: 3485.219116\n",
      "2025-03-07 23:28: **********Val Epoch 68: average Loss: 3485.219116\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 69: 0/23 Loss: 2.842467\n",
      "2025-03-07 23:28: Train Epoch 69: 0/23 Loss: 2.842467\n",
      "2025-03-07 23:28: Train Epoch 69: 20/23 Loss: 980.131104\n",
      "2025-03-07 23:28: Train Epoch 69: 20/23 Loss: 980.131104\n",
      "2025-03-07 23:28: **********Train Epoch 69: averaged Loss: 224.032461\n",
      "2025-03-07 23:28: **********Train Epoch 69: averaged Loss: 224.032461\n",
      "2025-03-07 23:28: **********Val Epoch 69: average Loss: 3484.241577\n",
      "2025-03-07 23:28: **********Val Epoch 69: average Loss: 3484.241577\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 70: 0/23 Loss: 3.080328\n",
      "2025-03-07 23:28: Train Epoch 70: 0/23 Loss: 3.080328\n",
      "2025-03-07 23:28: Train Epoch 70: 20/23 Loss: 978.860779\n",
      "2025-03-07 23:28: Train Epoch 70: 20/23 Loss: 978.860779\n",
      "2025-03-07 23:28: **********Train Epoch 70: averaged Loss: 224.251352\n",
      "2025-03-07 23:28: **********Train Epoch 70: averaged Loss: 224.251352\n",
      "2025-03-07 23:28: **********Val Epoch 70: average Loss: 3477.757568\n",
      "2025-03-07 23:28: **********Val Epoch 70: average Loss: 3477.757568\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 71: 0/23 Loss: 3.884833\n",
      "2025-03-07 23:28: Train Epoch 71: 0/23 Loss: 3.884833\n",
      "2025-03-07 23:28: Train Epoch 71: 20/23 Loss: 982.581909\n",
      "2025-03-07 23:28: Train Epoch 71: 20/23 Loss: 982.581909\n",
      "2025-03-07 23:28: **********Train Epoch 71: averaged Loss: 223.076885\n",
      "2025-03-07 23:28: **********Train Epoch 71: averaged Loss: 223.076885\n",
      "2025-03-07 23:28: **********Val Epoch 71: average Loss: 3476.104736\n",
      "2025-03-07 23:28: **********Val Epoch 71: average Loss: 3476.104736\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 72: 0/23 Loss: 1.960336\n",
      "2025-03-07 23:28: Train Epoch 72: 0/23 Loss: 1.960336\n",
      "2025-03-07 23:28: Train Epoch 72: 20/23 Loss: 976.706360\n",
      "2025-03-07 23:28: Train Epoch 72: 20/23 Loss: 976.706360\n",
      "2025-03-07 23:28: **********Train Epoch 72: averaged Loss: 225.655826\n",
      "2025-03-07 23:28: **********Train Epoch 72: averaged Loss: 225.655826\n",
      "2025-03-07 23:28: **********Val Epoch 72: average Loss: 3471.322388\n",
      "2025-03-07 23:28: **********Val Epoch 72: average Loss: 3471.322388\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 73: 0/23 Loss: 5.705489\n",
      "2025-03-07 23:28: Train Epoch 73: 0/23 Loss: 5.705489\n",
      "2025-03-07 23:28: Train Epoch 73: 20/23 Loss: 971.853577\n",
      "2025-03-07 23:28: Train Epoch 73: 20/23 Loss: 971.853577\n",
      "2025-03-07 23:28: **********Train Epoch 73: averaged Loss: 223.365637\n",
      "2025-03-07 23:28: **********Train Epoch 73: averaged Loss: 223.365637\n",
      "2025-03-07 23:28: **********Val Epoch 73: average Loss: 3467.750000\n",
      "2025-03-07 23:28: **********Val Epoch 73: average Loss: 3467.750000\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 74: 0/23 Loss: 2.696435\n",
      "2025-03-07 23:28: Train Epoch 74: 0/23 Loss: 2.696435\n",
      "2025-03-07 23:28: Train Epoch 74: 20/23 Loss: 974.154541\n",
      "2025-03-07 23:28: Train Epoch 74: 20/23 Loss: 974.154541\n",
      "2025-03-07 23:28: **********Train Epoch 74: averaged Loss: 222.207773\n",
      "2025-03-07 23:28: **********Train Epoch 74: averaged Loss: 222.207773\n",
      "2025-03-07 23:28: **********Val Epoch 74: average Loss: 3463.802002\n",
      "2025-03-07 23:28: **********Val Epoch 74: average Loss: 3463.802002\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 75: 0/23 Loss: 4.794732\n",
      "2025-03-07 23:28: Train Epoch 75: 0/23 Loss: 4.794732\n",
      "2025-03-07 23:28: Train Epoch 75: 20/23 Loss: 967.535400\n",
      "2025-03-07 23:28: Train Epoch 75: 20/23 Loss: 967.535400\n",
      "2025-03-07 23:28: **********Train Epoch 75: averaged Loss: 220.373108\n",
      "2025-03-07 23:28: **********Train Epoch 75: averaged Loss: 220.373108\n",
      "2025-03-07 23:28: **********Val Epoch 75: average Loss: 3464.646240\n",
      "2025-03-07 23:28: **********Val Epoch 75: average Loss: 3464.646240\n",
      "2025-03-07 23:28: Train Epoch 76: 0/23 Loss: 2.974221\n",
      "2025-03-07 23:28: Train Epoch 76: 0/23 Loss: 2.974221\n",
      "2025-03-07 23:28: Train Epoch 76: 20/23 Loss: 965.681519\n",
      "2025-03-07 23:28: Train Epoch 76: 20/23 Loss: 965.681519\n",
      "2025-03-07 23:28: **********Train Epoch 76: averaged Loss: 220.074336\n",
      "2025-03-07 23:28: **********Train Epoch 76: averaged Loss: 220.074336\n",
      "2025-03-07 23:28: **********Val Epoch 76: average Loss: 3459.934204\n",
      "2025-03-07 23:28: **********Val Epoch 76: average Loss: 3459.934204\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 77: 0/23 Loss: 3.963889\n",
      "2025-03-07 23:28: Train Epoch 77: 0/23 Loss: 3.963889\n",
      "2025-03-07 23:28: Train Epoch 77: 20/23 Loss: 963.831665\n",
      "2025-03-07 23:28: Train Epoch 77: 20/23 Loss: 963.831665\n",
      "2025-03-07 23:28: **********Train Epoch 77: averaged Loss: 218.973375\n",
      "2025-03-07 23:28: **********Train Epoch 77: averaged Loss: 218.973375\n",
      "2025-03-07 23:28: **********Val Epoch 77: average Loss: 3457.711426\n",
      "2025-03-07 23:28: **********Val Epoch 77: average Loss: 3457.711426\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 78: 0/23 Loss: 1.561181\n",
      "2025-03-07 23:28: Train Epoch 78: 0/23 Loss: 1.561181\n",
      "2025-03-07 23:28: Train Epoch 78: 20/23 Loss: 959.389648\n",
      "2025-03-07 23:28: Train Epoch 78: 20/23 Loss: 959.389648\n",
      "2025-03-07 23:28: **********Train Epoch 78: averaged Loss: 218.997846\n",
      "2025-03-07 23:28: **********Train Epoch 78: averaged Loss: 218.997846\n",
      "2025-03-07 23:28: **********Val Epoch 78: average Loss: 3452.634766\n",
      "2025-03-07 23:28: **********Val Epoch 78: average Loss: 3452.634766\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 79: 0/23 Loss: 5.454545\n",
      "2025-03-07 23:28: Train Epoch 79: 0/23 Loss: 5.454545\n",
      "2025-03-07 23:28: Train Epoch 79: 20/23 Loss: 957.627502\n",
      "2025-03-07 23:28: Train Epoch 79: 20/23 Loss: 957.627502\n",
      "2025-03-07 23:28: **********Train Epoch 79: averaged Loss: 217.946607\n",
      "2025-03-07 23:28: **********Train Epoch 79: averaged Loss: 217.946607\n",
      "2025-03-07 23:28: **********Val Epoch 79: average Loss: 3456.393188\n",
      "2025-03-07 23:28: **********Val Epoch 79: average Loss: 3456.393188\n",
      "2025-03-07 23:28: Train Epoch 80: 0/23 Loss: 2.608903\n",
      "2025-03-07 23:28: Train Epoch 80: 0/23 Loss: 2.608903\n",
      "2025-03-07 23:28: Train Epoch 80: 20/23 Loss: 950.138672\n",
      "2025-03-07 23:28: Train Epoch 80: 20/23 Loss: 950.138672\n",
      "2025-03-07 23:28: **********Train Epoch 80: averaged Loss: 216.954078\n",
      "2025-03-07 23:28: **********Train Epoch 80: averaged Loss: 216.954078\n",
      "2025-03-07 23:28: **********Val Epoch 80: average Loss: 3446.647705\n",
      "2025-03-07 23:28: **********Val Epoch 80: average Loss: 3446.647705\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 81: 0/23 Loss: 5.181591\n",
      "2025-03-07 23:28: Train Epoch 81: 0/23 Loss: 5.181591\n",
      "2025-03-07 23:28: Train Epoch 81: 20/23 Loss: 943.702271\n",
      "2025-03-07 23:28: Train Epoch 81: 20/23 Loss: 943.702271\n",
      "2025-03-07 23:28: **********Train Epoch 81: averaged Loss: 213.826355\n",
      "2025-03-07 23:28: **********Train Epoch 81: averaged Loss: 213.826355\n",
      "2025-03-07 23:28: **********Val Epoch 81: average Loss: 3442.021973\n",
      "2025-03-07 23:28: **********Val Epoch 81: average Loss: 3442.021973\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 82: 0/23 Loss: 0.933642\n",
      "2025-03-07 23:28: Train Epoch 82: 0/23 Loss: 0.933642\n",
      "2025-03-07 23:28: Train Epoch 82: 20/23 Loss: 946.381592\n",
      "2025-03-07 23:28: Train Epoch 82: 20/23 Loss: 946.381592\n",
      "2025-03-07 23:28: **********Train Epoch 82: averaged Loss: 213.697143\n",
      "2025-03-07 23:28: **********Train Epoch 82: averaged Loss: 213.697143\n",
      "2025-03-07 23:28: **********Val Epoch 82: average Loss: 3439.453003\n",
      "2025-03-07 23:28: **********Val Epoch 82: average Loss: 3439.453003\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 83: 0/23 Loss: 1.958542\n",
      "2025-03-07 23:28: Train Epoch 83: 0/23 Loss: 1.958542\n",
      "2025-03-07 23:28: Train Epoch 83: 20/23 Loss: 944.916138\n",
      "2025-03-07 23:28: Train Epoch 83: 20/23 Loss: 944.916138\n",
      "2025-03-07 23:28: **********Train Epoch 83: averaged Loss: 213.908344\n",
      "2025-03-07 23:28: **********Train Epoch 83: averaged Loss: 213.908344\n",
      "2025-03-07 23:28: **********Val Epoch 83: average Loss: 3436.108643\n",
      "2025-03-07 23:28: **********Val Epoch 83: average Loss: 3436.108643\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 84: 0/23 Loss: 3.017050\n",
      "2025-03-07 23:28: Train Epoch 84: 0/23 Loss: 3.017050\n",
      "2025-03-07 23:28: Train Epoch 84: 20/23 Loss: 933.651123\n",
      "2025-03-07 23:28: Train Epoch 84: 20/23 Loss: 933.651123\n",
      "2025-03-07 23:28: **********Train Epoch 84: averaged Loss: 211.088542\n",
      "2025-03-07 23:28: **********Train Epoch 84: averaged Loss: 211.088542\n",
      "2025-03-07 23:28: **********Val Epoch 84: average Loss: 3430.825317\n",
      "2025-03-07 23:28: **********Val Epoch 84: average Loss: 3430.825317\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 85: 0/23 Loss: 2.342116\n",
      "2025-03-07 23:28: Train Epoch 85: 0/23 Loss: 2.342116\n",
      "2025-03-07 23:28: Train Epoch 85: 20/23 Loss: 939.902222\n",
      "2025-03-07 23:28: Train Epoch 85: 20/23 Loss: 939.902222\n",
      "2025-03-07 23:28: **********Train Epoch 85: averaged Loss: 215.750458\n",
      "2025-03-07 23:28: **********Train Epoch 85: averaged Loss: 215.750458\n",
      "2025-03-07 23:28: **********Val Epoch 85: average Loss: 3428.173462\n",
      "2025-03-07 23:28: **********Val Epoch 85: average Loss: 3428.173462\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 86: 0/23 Loss: 5.349910\n",
      "2025-03-07 23:28: Train Epoch 86: 0/23 Loss: 5.349910\n",
      "2025-03-07 23:28: Train Epoch 86: 20/23 Loss: 929.610107\n",
      "2025-03-07 23:28: Train Epoch 86: 20/23 Loss: 929.610107\n",
      "2025-03-07 23:28: **********Train Epoch 86: averaged Loss: 214.529861\n",
      "2025-03-07 23:28: **********Train Epoch 86: averaged Loss: 214.529861\n",
      "2025-03-07 23:28: **********Val Epoch 86: average Loss: 3430.087402\n",
      "2025-03-07 23:28: **********Val Epoch 86: average Loss: 3430.087402\n",
      "2025-03-07 23:28: Train Epoch 87: 0/23 Loss: 0.683916\n",
      "2025-03-07 23:28: Train Epoch 87: 0/23 Loss: 0.683916\n",
      "2025-03-07 23:28: Train Epoch 87: 20/23 Loss: 927.940979\n",
      "2025-03-07 23:28: Train Epoch 87: 20/23 Loss: 927.940979\n",
      "2025-03-07 23:28: **********Train Epoch 87: averaged Loss: 217.214854\n",
      "2025-03-07 23:28: **********Train Epoch 87: averaged Loss: 217.214854\n",
      "2025-03-07 23:28: **********Val Epoch 87: average Loss: 3417.559448\n",
      "2025-03-07 23:28: **********Val Epoch 87: average Loss: 3417.559448\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 88: 0/23 Loss: 6.429515\n",
      "2025-03-07 23:28: Train Epoch 88: 0/23 Loss: 6.429515\n",
      "2025-03-07 23:28: Train Epoch 88: 20/23 Loss: 926.304321\n",
      "2025-03-07 23:28: Train Epoch 88: 20/23 Loss: 926.304321\n",
      "2025-03-07 23:28: **********Train Epoch 88: averaged Loss: 210.293314\n",
      "2025-03-07 23:28: **********Train Epoch 88: averaged Loss: 210.293314\n",
      "2025-03-07 23:28: **********Val Epoch 88: average Loss: 3421.205688\n",
      "2025-03-07 23:28: **********Val Epoch 88: average Loss: 3421.205688\n",
      "2025-03-07 23:28: Train Epoch 89: 0/23 Loss: 5.556540\n",
      "2025-03-07 23:28: Train Epoch 89: 0/23 Loss: 5.556540\n",
      "2025-03-07 23:28: Train Epoch 89: 20/23 Loss: 921.608093\n",
      "2025-03-07 23:28: Train Epoch 89: 20/23 Loss: 921.608093\n",
      "2025-03-07 23:28: **********Train Epoch 89: averaged Loss: 209.563093\n",
      "2025-03-07 23:28: **********Train Epoch 89: averaged Loss: 209.563093\n",
      "2025-03-07 23:28: **********Val Epoch 89: average Loss: 3408.257568\n",
      "2025-03-07 23:28: **********Val Epoch 89: average Loss: 3408.257568\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: *********************************Current best model saved!\n",
      "2025-03-07 23:28: Train Epoch 90: 0/23 Loss: 3.830391\n",
      "2025-03-07 23:28: Train Epoch 90: 0/23 Loss: 3.830391\n",
      "2025-03-07 23:28: Train Epoch 90: 20/23 Loss: 913.599304\n",
      "2025-03-07 23:28: Train Epoch 90: 20/23 Loss: 913.599304\n",
      "2025-03-07 23:28: **********Train Epoch 90: averaged Loss: 207.550318\n",
      "2025-03-07 23:28: **********Train Epoch 90: averaged Loss: 207.550318\n",
      "2025-03-07 23:28: **********Val Epoch 90: average Loss: 3414.721802\n",
      "2025-03-07 23:28: **********Val Epoch 90: average Loss: 3414.721802\n",
      "2025-03-07 23:28: Train Epoch 91: 0/23 Loss: 1.997316\n",
      "2025-03-07 23:28: Train Epoch 91: 0/23 Loss: 1.997316\n",
      "2025-03-07 23:29: Train Epoch 91: 20/23 Loss: 912.094666\n",
      "2025-03-07 23:29: Train Epoch 91: 20/23 Loss: 912.094666\n",
      "2025-03-07 23:29: **********Train Epoch 91: averaged Loss: 207.176376\n",
      "2025-03-07 23:29: **********Train Epoch 91: averaged Loss: 207.176376\n",
      "2025-03-07 23:29: **********Val Epoch 91: average Loss: 3401.011963\n",
      "2025-03-07 23:29: **********Val Epoch 91: average Loss: 3401.011963\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 92: 0/23 Loss: 2.837149\n",
      "2025-03-07 23:29: Train Epoch 92: 0/23 Loss: 2.837149\n",
      "2025-03-07 23:29: Train Epoch 92: 20/23 Loss: 899.391174\n",
      "2025-03-07 23:29: Train Epoch 92: 20/23 Loss: 899.391174\n",
      "2025-03-07 23:29: **********Train Epoch 92: averaged Loss: 206.434739\n",
      "2025-03-07 23:29: **********Train Epoch 92: averaged Loss: 206.434739\n",
      "2025-03-07 23:29: **********Val Epoch 92: average Loss: 3402.974609\n",
      "2025-03-07 23:29: **********Val Epoch 92: average Loss: 3402.974609\n",
      "2025-03-07 23:29: Train Epoch 93: 0/23 Loss: 1.030701\n",
      "2025-03-07 23:29: Train Epoch 93: 0/23 Loss: 1.030701\n",
      "2025-03-07 23:29: Train Epoch 93: 20/23 Loss: 903.309692\n",
      "2025-03-07 23:29: Train Epoch 93: 20/23 Loss: 903.309692\n",
      "2025-03-07 23:29: **********Train Epoch 93: averaged Loss: 209.955455\n",
      "2025-03-07 23:29: **********Train Epoch 93: averaged Loss: 209.955455\n",
      "2025-03-07 23:29: **********Val Epoch 93: average Loss: 3402.657837\n",
      "2025-03-07 23:29: **********Val Epoch 93: average Loss: 3402.657837\n",
      "2025-03-07 23:29: Train Epoch 94: 0/23 Loss: 6.047760\n",
      "2025-03-07 23:29: Train Epoch 94: 0/23 Loss: 6.047760\n",
      "2025-03-07 23:29: Train Epoch 94: 20/23 Loss: 881.736816\n",
      "2025-03-07 23:29: Train Epoch 94: 20/23 Loss: 881.736816\n",
      "2025-03-07 23:29: **********Train Epoch 94: averaged Loss: 202.700719\n",
      "2025-03-07 23:29: **********Train Epoch 94: averaged Loss: 202.700719\n",
      "2025-03-07 23:29: **********Val Epoch 94: average Loss: 3382.919678\n",
      "2025-03-07 23:29: **********Val Epoch 94: average Loss: 3382.919678\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 95: 0/23 Loss: 5.120743\n",
      "2025-03-07 23:29: Train Epoch 95: 0/23 Loss: 5.120743\n",
      "2025-03-07 23:29: Train Epoch 95: 20/23 Loss: 898.660889\n",
      "2025-03-07 23:29: Train Epoch 95: 20/23 Loss: 898.660889\n",
      "2025-03-07 23:29: **********Train Epoch 95: averaged Loss: 202.440154\n",
      "2025-03-07 23:29: **********Train Epoch 95: averaged Loss: 202.440154\n",
      "2025-03-07 23:29: **********Val Epoch 95: average Loss: 3397.908081\n",
      "2025-03-07 23:29: **********Val Epoch 95: average Loss: 3397.908081\n",
      "2025-03-07 23:29: Train Epoch 96: 0/23 Loss: 5.937948\n",
      "2025-03-07 23:29: Train Epoch 96: 0/23 Loss: 5.937948\n",
      "2025-03-07 23:29: Train Epoch 96: 20/23 Loss: 883.816772\n",
      "2025-03-07 23:29: Train Epoch 96: 20/23 Loss: 883.816772\n",
      "2025-03-07 23:29: **********Train Epoch 96: averaged Loss: 200.836082\n",
      "2025-03-07 23:29: **********Train Epoch 96: averaged Loss: 200.836082\n",
      "2025-03-07 23:29: **********Val Epoch 96: average Loss: 3385.296387\n",
      "2025-03-07 23:29: **********Val Epoch 96: average Loss: 3385.296387\n",
      "2025-03-07 23:29: Train Epoch 97: 0/23 Loss: 0.462376\n",
      "2025-03-07 23:29: Train Epoch 97: 0/23 Loss: 0.462376\n",
      "2025-03-07 23:29: Train Epoch 97: 20/23 Loss: 886.400879\n",
      "2025-03-07 23:29: Train Epoch 97: 20/23 Loss: 886.400879\n",
      "2025-03-07 23:29: **********Train Epoch 97: averaged Loss: 198.850461\n",
      "2025-03-07 23:29: **********Train Epoch 97: averaged Loss: 198.850461\n",
      "2025-03-07 23:29: **********Val Epoch 97: average Loss: 3371.359863\n",
      "2025-03-07 23:29: **********Val Epoch 97: average Loss: 3371.359863\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 98: 0/23 Loss: 5.170055\n",
      "2025-03-07 23:29: Train Epoch 98: 0/23 Loss: 5.170055\n",
      "2025-03-07 23:29: Train Epoch 98: 20/23 Loss: 870.165527\n",
      "2025-03-07 23:29: Train Epoch 98: 20/23 Loss: 870.165527\n",
      "2025-03-07 23:29: **********Train Epoch 98: averaged Loss: 196.331519\n",
      "2025-03-07 23:29: **********Train Epoch 98: averaged Loss: 196.331519\n",
      "2025-03-07 23:29: **********Val Epoch 98: average Loss: 3378.369385\n",
      "2025-03-07 23:29: **********Val Epoch 98: average Loss: 3378.369385\n",
      "2025-03-07 23:29: Train Epoch 99: 0/23 Loss: 1.785787\n",
      "2025-03-07 23:29: Train Epoch 99: 0/23 Loss: 1.785787\n",
      "2025-03-07 23:29: Train Epoch 99: 20/23 Loss: 873.081604\n",
      "2025-03-07 23:29: Train Epoch 99: 20/23 Loss: 873.081604\n",
      "2025-03-07 23:29: **********Train Epoch 99: averaged Loss: 197.151997\n",
      "2025-03-07 23:29: **********Train Epoch 99: averaged Loss: 197.151997\n",
      "2025-03-07 23:29: **********Val Epoch 99: average Loss: 3376.747314\n",
      "2025-03-07 23:29: **********Val Epoch 99: average Loss: 3376.747314\n",
      "2025-03-07 23:29: Train Epoch 100: 0/23 Loss: 5.371867\n",
      "2025-03-07 23:29: Train Epoch 100: 0/23 Loss: 5.371867\n",
      "2025-03-07 23:29: Train Epoch 100: 20/23 Loss: 858.983398\n",
      "2025-03-07 23:29: Train Epoch 100: 20/23 Loss: 858.983398\n",
      "2025-03-07 23:29: **********Train Epoch 100: averaged Loss: 203.428332\n",
      "2025-03-07 23:29: **********Train Epoch 100: averaged Loss: 203.428332\n",
      "2025-03-07 23:29: **********Val Epoch 100: average Loss: 3359.208984\n",
      "2025-03-07 23:29: **********Val Epoch 100: average Loss: 3359.208984\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 101: 0/23 Loss: 8.610680\n",
      "2025-03-07 23:29: Train Epoch 101: 0/23 Loss: 8.610680\n",
      "2025-03-07 23:29: Train Epoch 101: 20/23 Loss: 863.369019\n",
      "2025-03-07 23:29: Train Epoch 101: 20/23 Loss: 863.369019\n",
      "2025-03-07 23:29: **********Train Epoch 101: averaged Loss: 195.691046\n",
      "2025-03-07 23:29: **********Train Epoch 101: averaged Loss: 195.691046\n",
      "2025-03-07 23:29: **********Val Epoch 101: average Loss: 3357.850342\n",
      "2025-03-07 23:29: **********Val Epoch 101: average Loss: 3357.850342\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 102: 0/23 Loss: 2.420675\n",
      "2025-03-07 23:29: Train Epoch 102: 0/23 Loss: 2.420675\n",
      "2025-03-07 23:29: Train Epoch 102: 20/23 Loss: 853.660156\n",
      "2025-03-07 23:29: Train Epoch 102: 20/23 Loss: 853.660156\n",
      "2025-03-07 23:29: **********Train Epoch 102: averaged Loss: 195.463251\n",
      "2025-03-07 23:29: **********Train Epoch 102: averaged Loss: 195.463251\n",
      "2025-03-07 23:29: **********Val Epoch 102: average Loss: 3360.896484\n",
      "2025-03-07 23:29: **********Val Epoch 102: average Loss: 3360.896484\n",
      "2025-03-07 23:29: Train Epoch 103: 0/23 Loss: 3.762031\n",
      "2025-03-07 23:29: Train Epoch 103: 0/23 Loss: 3.762031\n",
      "2025-03-07 23:29: Train Epoch 103: 20/23 Loss: 853.918640\n",
      "2025-03-07 23:29: Train Epoch 103: 20/23 Loss: 853.918640\n",
      "2025-03-07 23:29: **********Train Epoch 103: averaged Loss: 193.678884\n",
      "2025-03-07 23:29: **********Train Epoch 103: averaged Loss: 193.678884\n",
      "2025-03-07 23:29: **********Val Epoch 103: average Loss: 3347.763428\n",
      "2025-03-07 23:29: **********Val Epoch 103: average Loss: 3347.763428\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 104: 0/23 Loss: 2.410438\n",
      "2025-03-07 23:29: Train Epoch 104: 0/23 Loss: 2.410438\n",
      "2025-03-07 23:29: Train Epoch 104: 20/23 Loss: 851.052002\n",
      "2025-03-07 23:29: Train Epoch 104: 20/23 Loss: 851.052002\n",
      "2025-03-07 23:29: **********Train Epoch 104: averaged Loss: 198.042960\n",
      "2025-03-07 23:29: **********Train Epoch 104: averaged Loss: 198.042960\n",
      "2025-03-07 23:29: **********Val Epoch 104: average Loss: 3338.255005\n",
      "2025-03-07 23:29: **********Val Epoch 104: average Loss: 3338.255005\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 105: 0/23 Loss: 5.195303\n",
      "2025-03-07 23:29: Train Epoch 105: 0/23 Loss: 5.195303\n",
      "2025-03-07 23:29: Train Epoch 105: 20/23 Loss: 846.441772\n",
      "2025-03-07 23:29: Train Epoch 105: 20/23 Loss: 846.441772\n",
      "2025-03-07 23:29: **********Train Epoch 105: averaged Loss: 192.369578\n",
      "2025-03-07 23:29: **********Train Epoch 105: averaged Loss: 192.369578\n",
      "2025-03-07 23:29: **********Val Epoch 105: average Loss: 3336.200317\n",
      "2025-03-07 23:29: **********Val Epoch 105: average Loss: 3336.200317\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 106: 0/23 Loss: 2.786831\n",
      "2025-03-07 23:29: Train Epoch 106: 0/23 Loss: 2.786831\n",
      "2025-03-07 23:29: Train Epoch 106: 20/23 Loss: 825.592468\n",
      "2025-03-07 23:29: Train Epoch 106: 20/23 Loss: 825.592468\n",
      "2025-03-07 23:29: **********Train Epoch 106: averaged Loss: 190.870188\n",
      "2025-03-07 23:29: **********Train Epoch 106: averaged Loss: 190.870188\n",
      "2025-03-07 23:29: **********Val Epoch 106: average Loss: 3328.826904\n",
      "2025-03-07 23:29: **********Val Epoch 106: average Loss: 3328.826904\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 107: 0/23 Loss: 8.762589\n",
      "2025-03-07 23:29: Train Epoch 107: 0/23 Loss: 8.762589\n",
      "2025-03-07 23:29: Train Epoch 107: 20/23 Loss: 833.616394\n",
      "2025-03-07 23:29: Train Epoch 107: 20/23 Loss: 833.616394\n",
      "2025-03-07 23:29: **********Train Epoch 107: averaged Loss: 192.058861\n",
      "2025-03-07 23:29: **********Train Epoch 107: averaged Loss: 192.058861\n",
      "2025-03-07 23:29: **********Val Epoch 107: average Loss: 3333.901001\n",
      "2025-03-07 23:29: **********Val Epoch 107: average Loss: 3333.901001\n",
      "2025-03-07 23:29: Train Epoch 108: 0/23 Loss: 2.408613\n",
      "2025-03-07 23:29: Train Epoch 108: 0/23 Loss: 2.408613\n",
      "2025-03-07 23:29: Train Epoch 108: 20/23 Loss: 822.813782\n",
      "2025-03-07 23:29: Train Epoch 108: 20/23 Loss: 822.813782\n",
      "2025-03-07 23:29: **********Train Epoch 108: averaged Loss: 190.616449\n",
      "2025-03-07 23:29: **********Train Epoch 108: averaged Loss: 190.616449\n",
      "2025-03-07 23:29: **********Val Epoch 108: average Loss: 3323.811279\n",
      "2025-03-07 23:29: **********Val Epoch 108: average Loss: 3323.811279\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 109: 0/23 Loss: 6.257421\n",
      "2025-03-07 23:29: Train Epoch 109: 0/23 Loss: 6.257421\n",
      "2025-03-07 23:29: Train Epoch 109: 20/23 Loss: 825.654541\n",
      "2025-03-07 23:29: Train Epoch 109: 20/23 Loss: 825.654541\n",
      "2025-03-07 23:29: **********Train Epoch 109: averaged Loss: 187.104384\n",
      "2025-03-07 23:29: **********Train Epoch 109: averaged Loss: 187.104384\n",
      "2025-03-07 23:29: **********Val Epoch 109: average Loss: 3315.243896\n",
      "2025-03-07 23:29: **********Val Epoch 109: average Loss: 3315.243896\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 110: 0/23 Loss: 2.559195\n",
      "2025-03-07 23:29: Train Epoch 110: 0/23 Loss: 2.559195\n",
      "2025-03-07 23:29: Train Epoch 110: 20/23 Loss: 812.682739\n",
      "2025-03-07 23:29: Train Epoch 110: 20/23 Loss: 812.682739\n",
      "2025-03-07 23:29: **********Train Epoch 110: averaged Loss: 183.175293\n",
      "2025-03-07 23:29: **********Train Epoch 110: averaged Loss: 183.175293\n",
      "2025-03-07 23:29: **********Val Epoch 110: average Loss: 3325.309326\n",
      "2025-03-07 23:29: **********Val Epoch 110: average Loss: 3325.309326\n",
      "2025-03-07 23:29: Train Epoch 111: 0/23 Loss: 6.687494\n",
      "2025-03-07 23:29: Train Epoch 111: 0/23 Loss: 6.687494\n",
      "2025-03-07 23:29: Train Epoch 111: 20/23 Loss: 804.007141\n",
      "2025-03-07 23:29: Train Epoch 111: 20/23 Loss: 804.007141\n",
      "2025-03-07 23:29: **********Train Epoch 111: averaged Loss: 184.309343\n",
      "2025-03-07 23:29: **********Train Epoch 111: averaged Loss: 184.309343\n",
      "2025-03-07 23:29: **********Val Epoch 111: average Loss: 3322.779663\n",
      "2025-03-07 23:29: **********Val Epoch 111: average Loss: 3322.779663\n",
      "2025-03-07 23:29: Train Epoch 112: 0/23 Loss: 4.430685\n",
      "2025-03-07 23:29: Train Epoch 112: 0/23 Loss: 4.430685\n",
      "2025-03-07 23:29: Train Epoch 112: 20/23 Loss: 803.506348\n",
      "2025-03-07 23:29: Train Epoch 112: 20/23 Loss: 803.506348\n",
      "2025-03-07 23:29: **********Train Epoch 112: averaged Loss: 186.093241\n",
      "2025-03-07 23:29: **********Train Epoch 112: averaged Loss: 186.093241\n",
      "2025-03-07 23:29: **********Val Epoch 112: average Loss: 3305.607910\n",
      "2025-03-07 23:29: **********Val Epoch 112: average Loss: 3305.607910\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 113: 0/23 Loss: 6.986732\n",
      "2025-03-07 23:29: Train Epoch 113: 0/23 Loss: 6.986732\n",
      "2025-03-07 23:29: Train Epoch 113: 20/23 Loss: 801.794434\n",
      "2025-03-07 23:29: Train Epoch 113: 20/23 Loss: 801.794434\n",
      "2025-03-07 23:29: **********Train Epoch 113: averaged Loss: 181.456345\n",
      "2025-03-07 23:29: **********Train Epoch 113: averaged Loss: 181.456345\n",
      "2025-03-07 23:29: **********Val Epoch 113: average Loss: 3308.748047\n",
      "2025-03-07 23:29: **********Val Epoch 113: average Loss: 3308.748047\n",
      "2025-03-07 23:29: Train Epoch 114: 0/23 Loss: 2.642872\n",
      "2025-03-07 23:29: Train Epoch 114: 0/23 Loss: 2.642872\n",
      "2025-03-07 23:29: Train Epoch 114: 20/23 Loss: 783.732178\n",
      "2025-03-07 23:29: Train Epoch 114: 20/23 Loss: 783.732178\n",
      "2025-03-07 23:29: **********Train Epoch 114: averaged Loss: 179.680090\n",
      "2025-03-07 23:29: **********Train Epoch 114: averaged Loss: 179.680090\n",
      "2025-03-07 23:29: **********Val Epoch 114: average Loss: 3306.658691\n",
      "2025-03-07 23:29: **********Val Epoch 114: average Loss: 3306.658691\n",
      "2025-03-07 23:29: Train Epoch 115: 0/23 Loss: 5.587892\n",
      "2025-03-07 23:29: Train Epoch 115: 0/23 Loss: 5.587892\n",
      "2025-03-07 23:29: Train Epoch 115: 20/23 Loss: 784.105347\n",
      "2025-03-07 23:29: Train Epoch 115: 20/23 Loss: 784.105347\n",
      "2025-03-07 23:29: **********Train Epoch 115: averaged Loss: 180.314541\n",
      "2025-03-07 23:29: **********Train Epoch 115: averaged Loss: 180.314541\n",
      "2025-03-07 23:29: **********Val Epoch 115: average Loss: 3296.156250\n",
      "2025-03-07 23:29: **********Val Epoch 115: average Loss: 3296.156250\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 116: 0/23 Loss: 2.698215\n",
      "2025-03-07 23:29: Train Epoch 116: 0/23 Loss: 2.698215\n",
      "2025-03-07 23:29: Train Epoch 116: 20/23 Loss: 792.751099\n",
      "2025-03-07 23:29: Train Epoch 116: 20/23 Loss: 792.751099\n",
      "2025-03-07 23:29: **********Train Epoch 116: averaged Loss: 183.168744\n",
      "2025-03-07 23:29: **********Train Epoch 116: averaged Loss: 183.168744\n",
      "2025-03-07 23:29: **********Val Epoch 116: average Loss: 3293.434326\n",
      "2025-03-07 23:29: **********Val Epoch 116: average Loss: 3293.434326\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 117: 0/23 Loss: 7.658266\n",
      "2025-03-07 23:29: Train Epoch 117: 0/23 Loss: 7.658266\n",
      "2025-03-07 23:29: Train Epoch 117: 20/23 Loss: 777.993286\n",
      "2025-03-07 23:29: Train Epoch 117: 20/23 Loss: 777.993286\n",
      "2025-03-07 23:29: **********Train Epoch 117: averaged Loss: 176.453633\n",
      "2025-03-07 23:29: **********Train Epoch 117: averaged Loss: 176.453633\n",
      "2025-03-07 23:29: **********Val Epoch 117: average Loss: 3270.256226\n",
      "2025-03-07 23:29: **********Val Epoch 117: average Loss: 3270.256226\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 118: 0/23 Loss: 1.197179\n",
      "2025-03-07 23:29: Train Epoch 118: 0/23 Loss: 1.197179\n",
      "2025-03-07 23:29: Train Epoch 118: 20/23 Loss: 783.338135\n",
      "2025-03-07 23:29: Train Epoch 118: 20/23 Loss: 783.338135\n",
      "2025-03-07 23:29: **********Train Epoch 118: averaged Loss: 175.089976\n",
      "2025-03-07 23:29: **********Train Epoch 118: averaged Loss: 175.089976\n",
      "2025-03-07 23:29: **********Val Epoch 118: average Loss: 3267.487671\n",
      "2025-03-07 23:29: **********Val Epoch 118: average Loss: 3267.487671\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 119: 0/23 Loss: 8.190464\n",
      "2025-03-07 23:29: Train Epoch 119: 0/23 Loss: 8.190464\n",
      "2025-03-07 23:29: Train Epoch 119: 20/23 Loss: 768.546448\n",
      "2025-03-07 23:29: Train Epoch 119: 20/23 Loss: 768.546448\n",
      "2025-03-07 23:29: **********Train Epoch 119: averaged Loss: 177.013532\n",
      "2025-03-07 23:29: **********Train Epoch 119: averaged Loss: 177.013532\n",
      "2025-03-07 23:29: **********Val Epoch 119: average Loss: 3270.440796\n",
      "2025-03-07 23:29: **********Val Epoch 119: average Loss: 3270.440796\n",
      "2025-03-07 23:29: Train Epoch 120: 0/23 Loss: 4.012687\n",
      "2025-03-07 23:29: Train Epoch 120: 0/23 Loss: 4.012687\n",
      "2025-03-07 23:29: Train Epoch 120: 20/23 Loss: 784.512451\n",
      "2025-03-07 23:29: Train Epoch 120: 20/23 Loss: 784.512451\n",
      "2025-03-07 23:29: **********Train Epoch 120: averaged Loss: 178.538339\n",
      "2025-03-07 23:29: **********Train Epoch 120: averaged Loss: 178.538339\n",
      "2025-03-07 23:29: **********Val Epoch 120: average Loss: 3260.118286\n",
      "2025-03-07 23:29: **********Val Epoch 120: average Loss: 3260.118286\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 121: 0/23 Loss: 7.103601\n",
      "2025-03-07 23:29: Train Epoch 121: 0/23 Loss: 7.103601\n",
      "2025-03-07 23:29: Train Epoch 121: 20/23 Loss: 744.410034\n",
      "2025-03-07 23:29: Train Epoch 121: 20/23 Loss: 744.410034\n",
      "2025-03-07 23:29: **********Train Epoch 121: averaged Loss: 170.262952\n",
      "2025-03-07 23:29: **********Train Epoch 121: averaged Loss: 170.262952\n",
      "2025-03-07 23:29: **********Val Epoch 121: average Loss: 3248.880493\n",
      "2025-03-07 23:29: **********Val Epoch 121: average Loss: 3248.880493\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 122: 0/23 Loss: 4.578146\n",
      "2025-03-07 23:29: Train Epoch 122: 0/23 Loss: 4.578146\n",
      "2025-03-07 23:29: Train Epoch 122: 20/23 Loss: 734.861938\n",
      "2025-03-07 23:29: Train Epoch 122: 20/23 Loss: 734.861938\n",
      "2025-03-07 23:29: **********Train Epoch 122: averaged Loss: 170.655772\n",
      "2025-03-07 23:29: **********Train Epoch 122: averaged Loss: 170.655772\n",
      "2025-03-07 23:29: **********Val Epoch 122: average Loss: 3245.464600\n",
      "2025-03-07 23:29: **********Val Epoch 122: average Loss: 3245.464600\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 123: 0/23 Loss: 4.430548\n",
      "2025-03-07 23:29: Train Epoch 123: 0/23 Loss: 4.430548\n",
      "2025-03-07 23:29: Train Epoch 123: 20/23 Loss: 738.409790\n",
      "2025-03-07 23:29: Train Epoch 123: 20/23 Loss: 738.409790\n",
      "2025-03-07 23:29: **********Train Epoch 123: averaged Loss: 169.749562\n",
      "2025-03-07 23:29: **********Train Epoch 123: averaged Loss: 169.749562\n",
      "2025-03-07 23:29: **********Val Epoch 123: average Loss: 3230.607056\n",
      "2025-03-07 23:29: **********Val Epoch 123: average Loss: 3230.607056\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: *********************************Current best model saved!\n",
      "2025-03-07 23:29: Train Epoch 124: 0/23 Loss: 3.131131\n",
      "2025-03-07 23:29: Train Epoch 124: 0/23 Loss: 3.131131\n",
      "2025-03-07 23:29: Train Epoch 124: 20/23 Loss: 745.902222\n",
      "2025-03-07 23:29: Train Epoch 124: 20/23 Loss: 745.902222\n",
      "2025-03-07 23:29: **********Train Epoch 124: averaged Loss: 172.077773\n",
      "2025-03-07 23:29: **********Train Epoch 124: averaged Loss: 172.077773\n",
      "2025-03-07 23:29: **********Val Epoch 124: average Loss: 3233.782227\n",
      "2025-03-07 23:29: **********Val Epoch 124: average Loss: 3233.782227\n",
      "2025-03-07 23:29: Train Epoch 125: 0/23 Loss: 8.254938\n",
      "2025-03-07 23:29: Train Epoch 125: 0/23 Loss: 8.254938\n",
      "2025-03-07 23:29: Train Epoch 125: 20/23 Loss: 714.030090\n",
      "2025-03-07 23:29: Train Epoch 125: 20/23 Loss: 714.030090\n",
      "2025-03-07 23:29: **********Train Epoch 125: averaged Loss: 169.768916\n",
      "2025-03-07 23:29: **********Train Epoch 125: averaged Loss: 169.768916\n",
      "2025-03-07 23:29: **********Val Epoch 125: average Loss: 3232.325562\n",
      "2025-03-07 23:29: **********Val Epoch 125: average Loss: 3232.325562\n",
      "2025-03-07 23:29: Train Epoch 126: 0/23 Loss: 4.346606\n",
      "2025-03-07 23:29: Train Epoch 126: 0/23 Loss: 4.346606\n",
      "2025-03-07 23:30: Train Epoch 126: 20/23 Loss: 720.348389\n",
      "2025-03-07 23:30: Train Epoch 126: 20/23 Loss: 720.348389\n",
      "2025-03-07 23:30: **********Train Epoch 126: averaged Loss: 169.942761\n",
      "2025-03-07 23:30: **********Train Epoch 126: averaged Loss: 169.942761\n",
      "2025-03-07 23:30: **********Val Epoch 126: average Loss: 3229.886597\n",
      "2025-03-07 23:30: **********Val Epoch 126: average Loss: 3229.886597\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 127: 0/23 Loss: 8.373075\n",
      "2025-03-07 23:30: Train Epoch 127: 0/23 Loss: 8.373075\n",
      "2025-03-07 23:30: Train Epoch 127: 20/23 Loss: 717.824829\n",
      "2025-03-07 23:30: Train Epoch 127: 20/23 Loss: 717.824829\n",
      "2025-03-07 23:30: **********Train Epoch 127: averaged Loss: 165.311578\n",
      "2025-03-07 23:30: **********Train Epoch 127: averaged Loss: 165.311578\n",
      "2025-03-07 23:30: **********Val Epoch 127: average Loss: 3224.522095\n",
      "2025-03-07 23:30: **********Val Epoch 127: average Loss: 3224.522095\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 128: 0/23 Loss: 2.627221\n",
      "2025-03-07 23:30: Train Epoch 128: 0/23 Loss: 2.627221\n",
      "2025-03-07 23:30: Train Epoch 128: 20/23 Loss: 725.395325\n",
      "2025-03-07 23:30: Train Epoch 128: 20/23 Loss: 725.395325\n",
      "2025-03-07 23:30: **********Train Epoch 128: averaged Loss: 164.829306\n",
      "2025-03-07 23:30: **********Train Epoch 128: averaged Loss: 164.829306\n",
      "2025-03-07 23:30: **********Val Epoch 128: average Loss: 3210.918457\n",
      "2025-03-07 23:30: **********Val Epoch 128: average Loss: 3210.918457\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 129: 0/23 Loss: 1.076309\n",
      "2025-03-07 23:30: Train Epoch 129: 0/23 Loss: 1.076309\n",
      "2025-03-07 23:30: Train Epoch 129: 20/23 Loss: 694.614685\n",
      "2025-03-07 23:30: Train Epoch 129: 20/23 Loss: 694.614685\n",
      "2025-03-07 23:30: **********Train Epoch 129: averaged Loss: 161.997848\n",
      "2025-03-07 23:30: **********Train Epoch 129: averaged Loss: 161.997848\n",
      "2025-03-07 23:30: **********Val Epoch 129: average Loss: 3202.606567\n",
      "2025-03-07 23:30: **********Val Epoch 129: average Loss: 3202.606567\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 130: 0/23 Loss: 1.335811\n",
      "2025-03-07 23:30: Train Epoch 130: 0/23 Loss: 1.335811\n",
      "2025-03-07 23:30: Train Epoch 130: 20/23 Loss: 707.897400\n",
      "2025-03-07 23:30: Train Epoch 130: 20/23 Loss: 707.897400\n",
      "2025-03-07 23:30: **********Train Epoch 130: averaged Loss: 159.468497\n",
      "2025-03-07 23:30: **********Train Epoch 130: averaged Loss: 159.468497\n",
      "2025-03-07 23:30: **********Val Epoch 130: average Loss: 3192.059082\n",
      "2025-03-07 23:30: **********Val Epoch 130: average Loss: 3192.059082\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 131: 0/23 Loss: 6.104825\n",
      "2025-03-07 23:30: Train Epoch 131: 0/23 Loss: 6.104825\n",
      "2025-03-07 23:30: Train Epoch 131: 20/23 Loss: 703.880188\n",
      "2025-03-07 23:30: Train Epoch 131: 20/23 Loss: 703.880188\n",
      "2025-03-07 23:30: **********Train Epoch 131: averaged Loss: 158.373377\n",
      "2025-03-07 23:30: **********Train Epoch 131: averaged Loss: 158.373377\n",
      "2025-03-07 23:30: **********Val Epoch 131: average Loss: 3182.867798\n",
      "2025-03-07 23:30: **********Val Epoch 131: average Loss: 3182.867798\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 132: 0/23 Loss: 1.699964\n",
      "2025-03-07 23:30: Train Epoch 132: 0/23 Loss: 1.699964\n",
      "2025-03-07 23:30: Train Epoch 132: 20/23 Loss: 680.349182\n",
      "2025-03-07 23:30: Train Epoch 132: 20/23 Loss: 680.349182\n",
      "2025-03-07 23:30: **********Train Epoch 132: averaged Loss: 158.098897\n",
      "2025-03-07 23:30: **********Train Epoch 132: averaged Loss: 158.098897\n",
      "2025-03-07 23:30: **********Val Epoch 132: average Loss: 3181.614502\n",
      "2025-03-07 23:30: **********Val Epoch 132: average Loss: 3181.614502\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 133: 0/23 Loss: 7.360323\n",
      "2025-03-07 23:30: Train Epoch 133: 0/23 Loss: 7.360323\n",
      "2025-03-07 23:30: Train Epoch 133: 20/23 Loss: 689.510498\n",
      "2025-03-07 23:30: Train Epoch 133: 20/23 Loss: 689.510498\n",
      "2025-03-07 23:30: **********Train Epoch 133: averaged Loss: 157.426750\n",
      "2025-03-07 23:30: **********Train Epoch 133: averaged Loss: 157.426750\n",
      "2025-03-07 23:30: **********Val Epoch 133: average Loss: 3175.044189\n",
      "2025-03-07 23:30: **********Val Epoch 133: average Loss: 3175.044189\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 134: 0/23 Loss: 2.059763\n",
      "2025-03-07 23:30: Train Epoch 134: 0/23 Loss: 2.059763\n",
      "2025-03-07 23:30: Train Epoch 134: 20/23 Loss: 663.676758\n",
      "2025-03-07 23:30: Train Epoch 134: 20/23 Loss: 663.676758\n",
      "2025-03-07 23:30: **********Train Epoch 134: averaged Loss: 155.540673\n",
      "2025-03-07 23:30: **********Train Epoch 134: averaged Loss: 155.540673\n",
      "2025-03-07 23:30: **********Val Epoch 134: average Loss: 3171.868774\n",
      "2025-03-07 23:30: **********Val Epoch 134: average Loss: 3171.868774\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 135: 0/23 Loss: 4.606175\n",
      "2025-03-07 23:30: Train Epoch 135: 0/23 Loss: 4.606175\n",
      "2025-03-07 23:30: Train Epoch 135: 20/23 Loss: 666.230225\n",
      "2025-03-07 23:30: Train Epoch 135: 20/23 Loss: 666.230225\n",
      "2025-03-07 23:30: **********Train Epoch 135: averaged Loss: 155.177928\n",
      "2025-03-07 23:30: **********Train Epoch 135: averaged Loss: 155.177928\n",
      "2025-03-07 23:30: **********Val Epoch 135: average Loss: 3169.070312\n",
      "2025-03-07 23:30: **********Val Epoch 135: average Loss: 3169.070312\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 136: 0/23 Loss: 2.149165\n",
      "2025-03-07 23:30: Train Epoch 136: 0/23 Loss: 2.149165\n",
      "2025-03-07 23:30: Train Epoch 136: 20/23 Loss: 672.285522\n",
      "2025-03-07 23:30: Train Epoch 136: 20/23 Loss: 672.285522\n",
      "2025-03-07 23:30: **********Train Epoch 136: averaged Loss: 155.697060\n",
      "2025-03-07 23:30: **********Train Epoch 136: averaged Loss: 155.697060\n",
      "2025-03-07 23:30: **********Val Epoch 136: average Loss: 3141.638306\n",
      "2025-03-07 23:30: **********Val Epoch 136: average Loss: 3141.638306\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 137: 0/23 Loss: 6.331451\n",
      "2025-03-07 23:30: Train Epoch 137: 0/23 Loss: 6.331451\n",
      "2025-03-07 23:30: Train Epoch 137: 20/23 Loss: 663.103577\n",
      "2025-03-07 23:30: Train Epoch 137: 20/23 Loss: 663.103577\n",
      "2025-03-07 23:30: **********Train Epoch 137: averaged Loss: 152.303480\n",
      "2025-03-07 23:30: **********Train Epoch 137: averaged Loss: 152.303480\n",
      "2025-03-07 23:30: **********Val Epoch 137: average Loss: 3154.319092\n",
      "2025-03-07 23:30: **********Val Epoch 137: average Loss: 3154.319092\n",
      "2025-03-07 23:30: Train Epoch 138: 0/23 Loss: 0.537439\n",
      "2025-03-07 23:30: Train Epoch 138: 0/23 Loss: 0.537439\n",
      "2025-03-07 23:30: Train Epoch 138: 20/23 Loss: 647.008057\n",
      "2025-03-07 23:30: Train Epoch 138: 20/23 Loss: 647.008057\n",
      "2025-03-07 23:30: **********Train Epoch 138: averaged Loss: 152.874737\n",
      "2025-03-07 23:30: **********Train Epoch 138: averaged Loss: 152.874737\n",
      "2025-03-07 23:30: **********Val Epoch 138: average Loss: 3142.015625\n",
      "2025-03-07 23:30: **********Val Epoch 138: average Loss: 3142.015625\n",
      "2025-03-07 23:30: Train Epoch 139: 0/23 Loss: 6.120904\n",
      "2025-03-07 23:30: Train Epoch 139: 0/23 Loss: 6.120904\n",
      "2025-03-07 23:30: Train Epoch 139: 20/23 Loss: 645.583862\n",
      "2025-03-07 23:30: Train Epoch 139: 20/23 Loss: 645.583862\n",
      "2025-03-07 23:30: **********Train Epoch 139: averaged Loss: 149.663187\n",
      "2025-03-07 23:30: **********Train Epoch 139: averaged Loss: 149.663187\n",
      "2025-03-07 23:30: **********Val Epoch 139: average Loss: 3134.781982\n",
      "2025-03-07 23:30: **********Val Epoch 139: average Loss: 3134.781982\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 140: 0/23 Loss: 2.902883\n",
      "2025-03-07 23:30: Train Epoch 140: 0/23 Loss: 2.902883\n",
      "2025-03-07 23:30: Train Epoch 140: 20/23 Loss: 631.149841\n",
      "2025-03-07 23:30: Train Epoch 140: 20/23 Loss: 631.149841\n",
      "2025-03-07 23:30: **********Train Epoch 140: averaged Loss: 151.010494\n",
      "2025-03-07 23:30: **********Train Epoch 140: averaged Loss: 151.010494\n",
      "2025-03-07 23:30: **********Val Epoch 140: average Loss: 3148.658447\n",
      "2025-03-07 23:30: **********Val Epoch 140: average Loss: 3148.658447\n",
      "2025-03-07 23:30: Train Epoch 141: 0/23 Loss: 10.324293\n",
      "2025-03-07 23:30: Train Epoch 141: 0/23 Loss: 10.324293\n",
      "2025-03-07 23:30: Train Epoch 141: 20/23 Loss: 610.221863\n",
      "2025-03-07 23:30: Train Epoch 141: 20/23 Loss: 610.221863\n",
      "2025-03-07 23:30: **********Train Epoch 141: averaged Loss: 154.469081\n",
      "2025-03-07 23:30: **********Train Epoch 141: averaged Loss: 154.469081\n",
      "2025-03-07 23:30: **********Val Epoch 141: average Loss: 3128.054199\n",
      "2025-03-07 23:30: **********Val Epoch 141: average Loss: 3128.054199\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 142: 0/23 Loss: 0.447791\n",
      "2025-03-07 23:30: Train Epoch 142: 0/23 Loss: 0.447791\n",
      "2025-03-07 23:30: Train Epoch 142: 20/23 Loss: 621.220215\n",
      "2025-03-07 23:30: Train Epoch 142: 20/23 Loss: 621.220215\n",
      "2025-03-07 23:30: **********Train Epoch 142: averaged Loss: 149.104756\n",
      "2025-03-07 23:30: **********Train Epoch 142: averaged Loss: 149.104756\n",
      "2025-03-07 23:30: **********Val Epoch 142: average Loss: 3127.698242\n",
      "2025-03-07 23:30: **********Val Epoch 142: average Loss: 3127.698242\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 143: 0/23 Loss: 6.565258\n",
      "2025-03-07 23:30: Train Epoch 143: 0/23 Loss: 6.565258\n",
      "2025-03-07 23:30: Train Epoch 143: 20/23 Loss: 601.820251\n",
      "2025-03-07 23:30: Train Epoch 143: 20/23 Loss: 601.820251\n",
      "2025-03-07 23:30: **********Train Epoch 143: averaged Loss: 152.015836\n",
      "2025-03-07 23:30: **********Train Epoch 143: averaged Loss: 152.015836\n",
      "2025-03-07 23:30: **********Val Epoch 143: average Loss: 3108.974976\n",
      "2025-03-07 23:30: **********Val Epoch 143: average Loss: 3108.974976\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 144: 0/23 Loss: 7.253473\n",
      "2025-03-07 23:30: Train Epoch 144: 0/23 Loss: 7.253473\n",
      "2025-03-07 23:30: Train Epoch 144: 20/23 Loss: 620.542114\n",
      "2025-03-07 23:30: Train Epoch 144: 20/23 Loss: 620.542114\n",
      "2025-03-07 23:30: **********Train Epoch 144: averaged Loss: 144.711697\n",
      "2025-03-07 23:30: **********Train Epoch 144: averaged Loss: 144.711697\n",
      "2025-03-07 23:30: **********Val Epoch 144: average Loss: 3104.242920\n",
      "2025-03-07 23:30: **********Val Epoch 144: average Loss: 3104.242920\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 145: 0/23 Loss: 7.020292\n",
      "2025-03-07 23:30: Train Epoch 145: 0/23 Loss: 7.020292\n",
      "2025-03-07 23:30: Train Epoch 145: 20/23 Loss: 613.686279\n",
      "2025-03-07 23:30: Train Epoch 145: 20/23 Loss: 613.686279\n",
      "2025-03-07 23:30: **********Train Epoch 145: averaged Loss: 143.043879\n",
      "2025-03-07 23:30: **********Train Epoch 145: averaged Loss: 143.043879\n",
      "2025-03-07 23:30: **********Val Epoch 145: average Loss: 3092.648682\n",
      "2025-03-07 23:30: **********Val Epoch 145: average Loss: 3092.648682\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 146: 0/23 Loss: 3.853560\n",
      "2025-03-07 23:30: Train Epoch 146: 0/23 Loss: 3.853560\n",
      "2025-03-07 23:30: Train Epoch 146: 20/23 Loss: 606.145813\n",
      "2025-03-07 23:30: Train Epoch 146: 20/23 Loss: 606.145813\n",
      "2025-03-07 23:30: **********Train Epoch 146: averaged Loss: 142.290574\n",
      "2025-03-07 23:30: **********Train Epoch 146: averaged Loss: 142.290574\n",
      "2025-03-07 23:30: **********Val Epoch 146: average Loss: 3085.767090\n",
      "2025-03-07 23:30: **********Val Epoch 146: average Loss: 3085.767090\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 147: 0/23 Loss: 2.697842\n",
      "2025-03-07 23:30: Train Epoch 147: 0/23 Loss: 2.697842\n",
      "2025-03-07 23:30: Train Epoch 147: 20/23 Loss: 592.718628\n",
      "2025-03-07 23:30: Train Epoch 147: 20/23 Loss: 592.718628\n",
      "2025-03-07 23:30: **********Train Epoch 147: averaged Loss: 139.539390\n",
      "2025-03-07 23:30: **********Train Epoch 147: averaged Loss: 139.539390\n",
      "2025-03-07 23:30: **********Val Epoch 147: average Loss: 3101.488525\n",
      "2025-03-07 23:30: **********Val Epoch 147: average Loss: 3101.488525\n",
      "2025-03-07 23:30: Train Epoch 148: 0/23 Loss: 3.834972\n",
      "2025-03-07 23:30: Train Epoch 148: 0/23 Loss: 3.834972\n",
      "2025-03-07 23:30: Train Epoch 148: 20/23 Loss: 586.881287\n",
      "2025-03-07 23:30: Train Epoch 148: 20/23 Loss: 586.881287\n",
      "2025-03-07 23:30: **********Train Epoch 148: averaged Loss: 139.578879\n",
      "2025-03-07 23:30: **********Train Epoch 148: averaged Loss: 139.578879\n",
      "2025-03-07 23:30: **********Val Epoch 148: average Loss: 3085.577026\n",
      "2025-03-07 23:30: **********Val Epoch 148: average Loss: 3085.577026\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 149: 0/23 Loss: 4.980006\n",
      "2025-03-07 23:30: Train Epoch 149: 0/23 Loss: 4.980006\n",
      "2025-03-07 23:30: Train Epoch 149: 20/23 Loss: 588.305481\n",
      "2025-03-07 23:30: Train Epoch 149: 20/23 Loss: 588.305481\n",
      "2025-03-07 23:30: **********Train Epoch 149: averaged Loss: 145.735125\n",
      "2025-03-07 23:30: **********Train Epoch 149: averaged Loss: 145.735125\n",
      "2025-03-07 23:30: **********Val Epoch 149: average Loss: 3075.171509\n",
      "2025-03-07 23:30: **********Val Epoch 149: average Loss: 3075.171509\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 150: 0/23 Loss: 5.904107\n",
      "2025-03-07 23:30: Train Epoch 150: 0/23 Loss: 5.904107\n",
      "2025-03-07 23:30: Train Epoch 150: 20/23 Loss: 566.390930\n",
      "2025-03-07 23:30: Train Epoch 150: 20/23 Loss: 566.390930\n",
      "2025-03-07 23:30: **********Train Epoch 150: averaged Loss: 149.474604\n",
      "2025-03-07 23:30: **********Train Epoch 150: averaged Loss: 149.474604\n",
      "2025-03-07 23:30: **********Val Epoch 150: average Loss: 3077.848022\n",
      "2025-03-07 23:30: **********Val Epoch 150: average Loss: 3077.848022\n",
      "2025-03-07 23:30: Train Epoch 151: 0/23 Loss: 1.360312\n",
      "2025-03-07 23:30: Train Epoch 151: 0/23 Loss: 1.360312\n",
      "2025-03-07 23:30: Train Epoch 151: 20/23 Loss: 555.823242\n",
      "2025-03-07 23:30: Train Epoch 151: 20/23 Loss: 555.823242\n",
      "2025-03-07 23:30: **********Train Epoch 151: averaged Loss: 135.704192\n",
      "2025-03-07 23:30: **********Train Epoch 151: averaged Loss: 135.704192\n",
      "2025-03-07 23:30: **********Val Epoch 151: average Loss: 3052.623779\n",
      "2025-03-07 23:30: **********Val Epoch 151: average Loss: 3052.623779\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 152: 0/23 Loss: 21.428593\n",
      "2025-03-07 23:30: Train Epoch 152: 0/23 Loss: 21.428593\n",
      "2025-03-07 23:30: Train Epoch 152: 20/23 Loss: 565.498413\n",
      "2025-03-07 23:30: Train Epoch 152: 20/23 Loss: 565.498413\n",
      "2025-03-07 23:30: **********Train Epoch 152: averaged Loss: 137.141302\n",
      "2025-03-07 23:30: **********Train Epoch 152: averaged Loss: 137.141302\n",
      "2025-03-07 23:30: **********Val Epoch 152: average Loss: 3066.571777\n",
      "2025-03-07 23:30: **********Val Epoch 152: average Loss: 3066.571777\n",
      "2025-03-07 23:30: Train Epoch 153: 0/23 Loss: 1.038484\n",
      "2025-03-07 23:30: Train Epoch 153: 0/23 Loss: 1.038484\n",
      "2025-03-07 23:30: Train Epoch 153: 20/23 Loss: 564.995117\n",
      "2025-03-07 23:30: Train Epoch 153: 20/23 Loss: 564.995117\n",
      "2025-03-07 23:30: **********Train Epoch 153: averaged Loss: 138.052598\n",
      "2025-03-07 23:30: **********Train Epoch 153: averaged Loss: 138.052598\n",
      "2025-03-07 23:30: **********Val Epoch 153: average Loss: 3035.501709\n",
      "2025-03-07 23:30: **********Val Epoch 153: average Loss: 3035.501709\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 154: 0/23 Loss: 7.674538\n",
      "2025-03-07 23:30: Train Epoch 154: 0/23 Loss: 7.674538\n",
      "2025-03-07 23:30: Train Epoch 154: 20/23 Loss: 573.548340\n",
      "2025-03-07 23:30: Train Epoch 154: 20/23 Loss: 573.548340\n",
      "2025-03-07 23:30: **********Train Epoch 154: averaged Loss: 149.923974\n",
      "2025-03-07 23:30: **********Train Epoch 154: averaged Loss: 149.923974\n",
      "2025-03-07 23:30: **********Val Epoch 154: average Loss: 3032.251587\n",
      "2025-03-07 23:30: **********Val Epoch 154: average Loss: 3032.251587\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 155: 0/23 Loss: 1.997990\n",
      "2025-03-07 23:30: Train Epoch 155: 0/23 Loss: 1.997990\n",
      "2025-03-07 23:30: Train Epoch 155: 20/23 Loss: 530.169128\n",
      "2025-03-07 23:30: Train Epoch 155: 20/23 Loss: 530.169128\n",
      "2025-03-07 23:30: **********Train Epoch 155: averaged Loss: 131.699239\n",
      "2025-03-07 23:30: **********Train Epoch 155: averaged Loss: 131.699239\n",
      "2025-03-07 23:30: **********Val Epoch 155: average Loss: 3025.244263\n",
      "2025-03-07 23:30: **********Val Epoch 155: average Loss: 3025.244263\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 156: 0/23 Loss: 14.677439\n",
      "2025-03-07 23:30: Train Epoch 156: 0/23 Loss: 14.677439\n",
      "2025-03-07 23:30: Train Epoch 156: 20/23 Loss: 530.321289\n",
      "2025-03-07 23:30: Train Epoch 156: 20/23 Loss: 530.321289\n",
      "2025-03-07 23:30: **********Train Epoch 156: averaged Loss: 131.212190\n",
      "2025-03-07 23:30: **********Train Epoch 156: averaged Loss: 131.212190\n",
      "2025-03-07 23:30: **********Val Epoch 156: average Loss: 3029.392700\n",
      "2025-03-07 23:30: **********Val Epoch 156: average Loss: 3029.392700\n",
      "2025-03-07 23:30: Train Epoch 157: 0/23 Loss: 1.894224\n",
      "2025-03-07 23:30: Train Epoch 157: 0/23 Loss: 1.894224\n",
      "2025-03-07 23:30: Train Epoch 157: 20/23 Loss: 543.336121\n",
      "2025-03-07 23:30: Train Epoch 157: 20/23 Loss: 543.336121\n",
      "2025-03-07 23:30: **********Train Epoch 157: averaged Loss: 130.492981\n",
      "2025-03-07 23:30: **********Train Epoch 157: averaged Loss: 130.492981\n",
      "2025-03-07 23:30: **********Val Epoch 157: average Loss: 3020.709961\n",
      "2025-03-07 23:30: **********Val Epoch 157: average Loss: 3020.709961\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 158: 0/23 Loss: 3.167798\n",
      "2025-03-07 23:30: Train Epoch 158: 0/23 Loss: 3.167798\n",
      "2025-03-07 23:30: Train Epoch 158: 20/23 Loss: 528.159424\n",
      "2025-03-07 23:30: Train Epoch 158: 20/23 Loss: 528.159424\n",
      "2025-03-07 23:30: **********Train Epoch 158: averaged Loss: 135.945660\n",
      "2025-03-07 23:30: **********Train Epoch 158: averaged Loss: 135.945660\n",
      "2025-03-07 23:30: **********Val Epoch 158: average Loss: 3023.492432\n",
      "2025-03-07 23:30: **********Val Epoch 158: average Loss: 3023.492432\n",
      "2025-03-07 23:30: Train Epoch 159: 0/23 Loss: 6.032411\n",
      "2025-03-07 23:30: Train Epoch 159: 0/23 Loss: 6.032411\n",
      "2025-03-07 23:30: Train Epoch 159: 20/23 Loss: 514.807373\n",
      "2025-03-07 23:30: Train Epoch 159: 20/23 Loss: 514.807373\n",
      "2025-03-07 23:30: **********Train Epoch 159: averaged Loss: 139.277104\n",
      "2025-03-07 23:30: **********Train Epoch 159: averaged Loss: 139.277104\n",
      "2025-03-07 23:30: **********Val Epoch 159: average Loss: 2998.081421\n",
      "2025-03-07 23:30: **********Val Epoch 159: average Loss: 2998.081421\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: *********************************Current best model saved!\n",
      "2025-03-07 23:30: Train Epoch 160: 0/23 Loss: 1.943573\n",
      "2025-03-07 23:30: Train Epoch 160: 0/23 Loss: 1.943573\n",
      "2025-03-07 23:31: Train Epoch 160: 20/23 Loss: 532.348206\n",
      "2025-03-07 23:31: Train Epoch 160: 20/23 Loss: 532.348206\n",
      "2025-03-07 23:31: **********Train Epoch 160: averaged Loss: 128.021590\n",
      "2025-03-07 23:31: **********Train Epoch 160: averaged Loss: 128.021590\n",
      "2025-03-07 23:31: **********Val Epoch 160: average Loss: 2995.833618\n",
      "2025-03-07 23:31: **********Val Epoch 160: average Loss: 2995.833618\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 161: 0/23 Loss: 5.720297\n",
      "2025-03-07 23:31: Train Epoch 161: 0/23 Loss: 5.720297\n",
      "2025-03-07 23:31: Train Epoch 161: 20/23 Loss: 485.816193\n",
      "2025-03-07 23:31: Train Epoch 161: 20/23 Loss: 485.816193\n",
      "2025-03-07 23:31: **********Train Epoch 161: averaged Loss: 123.445634\n",
      "2025-03-07 23:31: **********Train Epoch 161: averaged Loss: 123.445634\n",
      "2025-03-07 23:31: **********Val Epoch 161: average Loss: 2980.741821\n",
      "2025-03-07 23:31: **********Val Epoch 161: average Loss: 2980.741821\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 162: 0/23 Loss: 1.666649\n",
      "2025-03-07 23:31: Train Epoch 162: 0/23 Loss: 1.666649\n",
      "2025-03-07 23:31: Train Epoch 162: 20/23 Loss: 501.686035\n",
      "2025-03-07 23:31: Train Epoch 162: 20/23 Loss: 501.686035\n",
      "2025-03-07 23:31: **********Train Epoch 162: averaged Loss: 123.248391\n",
      "2025-03-07 23:31: **********Train Epoch 162: averaged Loss: 123.248391\n",
      "2025-03-07 23:31: **********Val Epoch 162: average Loss: 2990.097290\n",
      "2025-03-07 23:31: **********Val Epoch 162: average Loss: 2990.097290\n",
      "2025-03-07 23:31: Train Epoch 163: 0/23 Loss: 3.745620\n",
      "2025-03-07 23:31: Train Epoch 163: 0/23 Loss: 3.745620\n",
      "2025-03-07 23:31: Train Epoch 163: 20/23 Loss: 499.504761\n",
      "2025-03-07 23:31: Train Epoch 163: 20/23 Loss: 499.504761\n",
      "2025-03-07 23:31: **********Train Epoch 163: averaged Loss: 129.070585\n",
      "2025-03-07 23:31: **********Train Epoch 163: averaged Loss: 129.070585\n",
      "2025-03-07 23:31: **********Val Epoch 163: average Loss: 2964.121582\n",
      "2025-03-07 23:31: **********Val Epoch 163: average Loss: 2964.121582\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 164: 0/23 Loss: 6.549903\n",
      "2025-03-07 23:31: Train Epoch 164: 0/23 Loss: 6.549903\n",
      "2025-03-07 23:31: Train Epoch 164: 20/23 Loss: 450.490143\n",
      "2025-03-07 23:31: Train Epoch 164: 20/23 Loss: 450.490143\n",
      "2025-03-07 23:31: **********Train Epoch 164: averaged Loss: 129.999001\n",
      "2025-03-07 23:31: **********Train Epoch 164: averaged Loss: 129.999001\n",
      "2025-03-07 23:31: **********Val Epoch 164: average Loss: 2982.602173\n",
      "2025-03-07 23:31: **********Val Epoch 164: average Loss: 2982.602173\n",
      "2025-03-07 23:31: Train Epoch 165: 0/23 Loss: 4.074786\n",
      "2025-03-07 23:31: Train Epoch 165: 0/23 Loss: 4.074786\n",
      "2025-03-07 23:31: Train Epoch 165: 20/23 Loss: 482.013428\n",
      "2025-03-07 23:31: Train Epoch 165: 20/23 Loss: 482.013428\n",
      "2025-03-07 23:31: **********Train Epoch 165: averaged Loss: 127.858758\n",
      "2025-03-07 23:31: **********Train Epoch 165: averaged Loss: 127.858758\n",
      "2025-03-07 23:31: **********Val Epoch 165: average Loss: 2969.122192\n",
      "2025-03-07 23:31: **********Val Epoch 165: average Loss: 2969.122192\n",
      "2025-03-07 23:31: Train Epoch 166: 0/23 Loss: 2.662676\n",
      "2025-03-07 23:31: Train Epoch 166: 0/23 Loss: 2.662676\n",
      "2025-03-07 23:31: Train Epoch 166: 20/23 Loss: 460.850616\n",
      "2025-03-07 23:31: Train Epoch 166: 20/23 Loss: 460.850616\n",
      "2025-03-07 23:31: **********Train Epoch 166: averaged Loss: 124.550225\n",
      "2025-03-07 23:31: **********Train Epoch 166: averaged Loss: 124.550225\n",
      "2025-03-07 23:31: **********Val Epoch 166: average Loss: 2958.284180\n",
      "2025-03-07 23:31: **********Val Epoch 166: average Loss: 2958.284180\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 167: 0/23 Loss: 1.183568\n",
      "2025-03-07 23:31: Train Epoch 167: 0/23 Loss: 1.183568\n",
      "2025-03-07 23:31: Train Epoch 167: 20/23 Loss: 471.957275\n",
      "2025-03-07 23:31: Train Epoch 167: 20/23 Loss: 471.957275\n",
      "2025-03-07 23:31: **********Train Epoch 167: averaged Loss: 124.622765\n",
      "2025-03-07 23:31: **********Train Epoch 167: averaged Loss: 124.622765\n",
      "2025-03-07 23:31: **********Val Epoch 167: average Loss: 2950.746582\n",
      "2025-03-07 23:31: **********Val Epoch 167: average Loss: 2950.746582\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 168: 0/23 Loss: 4.358589\n",
      "2025-03-07 23:31: Train Epoch 168: 0/23 Loss: 4.358589\n",
      "2025-03-07 23:31: Train Epoch 168: 20/23 Loss: 457.462097\n",
      "2025-03-07 23:31: Train Epoch 168: 20/23 Loss: 457.462097\n",
      "2025-03-07 23:31: **********Train Epoch 168: averaged Loss: 125.813292\n",
      "2025-03-07 23:31: **********Train Epoch 168: averaged Loss: 125.813292\n",
      "2025-03-07 23:31: **********Val Epoch 168: average Loss: 2954.234009\n",
      "2025-03-07 23:31: **********Val Epoch 168: average Loss: 2954.234009\n",
      "2025-03-07 23:31: Train Epoch 169: 0/23 Loss: 3.412616\n",
      "2025-03-07 23:31: Train Epoch 169: 0/23 Loss: 3.412616\n",
      "2025-03-07 23:31: Train Epoch 169: 20/23 Loss: 477.671631\n",
      "2025-03-07 23:31: Train Epoch 169: 20/23 Loss: 477.671631\n",
      "2025-03-07 23:31: **********Train Epoch 169: averaged Loss: 122.951207\n",
      "2025-03-07 23:31: **********Train Epoch 169: averaged Loss: 122.951207\n",
      "2025-03-07 23:31: **********Val Epoch 169: average Loss: 2952.262939\n",
      "2025-03-07 23:31: **********Val Epoch 169: average Loss: 2952.262939\n",
      "2025-03-07 23:31: Train Epoch 170: 0/23 Loss: 3.712264\n",
      "2025-03-07 23:31: Train Epoch 170: 0/23 Loss: 3.712264\n",
      "2025-03-07 23:31: Train Epoch 170: 20/23 Loss: 408.995850\n",
      "2025-03-07 23:31: Train Epoch 170: 20/23 Loss: 408.995850\n",
      "2025-03-07 23:31: **********Train Epoch 170: averaged Loss: 119.352192\n",
      "2025-03-07 23:31: **********Train Epoch 170: averaged Loss: 119.352192\n",
      "2025-03-07 23:31: **********Val Epoch 170: average Loss: 2932.963501\n",
      "2025-03-07 23:31: **********Val Epoch 170: average Loss: 2932.963501\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 171: 0/23 Loss: 3.582821\n",
      "2025-03-07 23:31: Train Epoch 171: 0/23 Loss: 3.582821\n",
      "2025-03-07 23:31: Train Epoch 171: 20/23 Loss: 448.309326\n",
      "2025-03-07 23:31: Train Epoch 171: 20/23 Loss: 448.309326\n",
      "2025-03-07 23:31: **********Train Epoch 171: averaged Loss: 115.049244\n",
      "2025-03-07 23:31: **********Train Epoch 171: averaged Loss: 115.049244\n",
      "2025-03-07 23:31: **********Val Epoch 171: average Loss: 2910.115112\n",
      "2025-03-07 23:31: **********Val Epoch 171: average Loss: 2910.115112\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 172: 0/23 Loss: 11.180532\n",
      "2025-03-07 23:31: Train Epoch 172: 0/23 Loss: 11.180532\n",
      "2025-03-07 23:31: Train Epoch 172: 20/23 Loss: 434.938538\n",
      "2025-03-07 23:31: Train Epoch 172: 20/23 Loss: 434.938538\n",
      "2025-03-07 23:31: **********Train Epoch 172: averaged Loss: 116.736392\n",
      "2025-03-07 23:31: **********Train Epoch 172: averaged Loss: 116.736392\n",
      "2025-03-07 23:31: **********Val Epoch 172: average Loss: 2917.616211\n",
      "2025-03-07 23:31: **********Val Epoch 172: average Loss: 2917.616211\n",
      "2025-03-07 23:31: Train Epoch 173: 0/23 Loss: 2.497854\n",
      "2025-03-07 23:31: Train Epoch 173: 0/23 Loss: 2.497854\n",
      "2025-03-07 23:31: Train Epoch 173: 20/23 Loss: 411.459778\n",
      "2025-03-07 23:31: Train Epoch 173: 20/23 Loss: 411.459778\n",
      "2025-03-07 23:31: **********Train Epoch 173: averaged Loss: 110.336682\n",
      "2025-03-07 23:31: **********Train Epoch 173: averaged Loss: 110.336682\n",
      "2025-03-07 23:31: **********Val Epoch 173: average Loss: 2894.272339\n",
      "2025-03-07 23:31: **********Val Epoch 173: average Loss: 2894.272339\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 174: 0/23 Loss: 8.986902\n",
      "2025-03-07 23:31: Train Epoch 174: 0/23 Loss: 8.986902\n",
      "2025-03-07 23:31: Train Epoch 174: 20/23 Loss: 421.011108\n",
      "2025-03-07 23:31: Train Epoch 174: 20/23 Loss: 421.011108\n",
      "2025-03-07 23:31: **********Train Epoch 174: averaged Loss: 110.135763\n",
      "2025-03-07 23:31: **********Train Epoch 174: averaged Loss: 110.135763\n",
      "2025-03-07 23:31: **********Val Epoch 174: average Loss: 2899.941162\n",
      "2025-03-07 23:31: **********Val Epoch 174: average Loss: 2899.941162\n",
      "2025-03-07 23:31: Train Epoch 175: 0/23 Loss: 2.184104\n",
      "2025-03-07 23:31: Train Epoch 175: 0/23 Loss: 2.184104\n",
      "2025-03-07 23:31: Train Epoch 175: 20/23 Loss: 392.688721\n",
      "2025-03-07 23:31: Train Epoch 175: 20/23 Loss: 392.688721\n",
      "2025-03-07 23:31: **********Train Epoch 175: averaged Loss: 107.350226\n",
      "2025-03-07 23:31: **********Train Epoch 175: averaged Loss: 107.350226\n",
      "2025-03-07 23:31: **********Val Epoch 175: average Loss: 2872.253418\n",
      "2025-03-07 23:31: **********Val Epoch 175: average Loss: 2872.253418\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 176: 0/23 Loss: 1.384722\n",
      "2025-03-07 23:31: Train Epoch 176: 0/23 Loss: 1.384722\n",
      "2025-03-07 23:31: Train Epoch 176: 20/23 Loss: 369.319946\n",
      "2025-03-07 23:31: Train Epoch 176: 20/23 Loss: 369.319946\n",
      "2025-03-07 23:31: **********Train Epoch 176: averaged Loss: 112.967453\n",
      "2025-03-07 23:31: **********Train Epoch 176: averaged Loss: 112.967453\n",
      "2025-03-07 23:31: **********Val Epoch 176: average Loss: 2875.800537\n",
      "2025-03-07 23:31: **********Val Epoch 176: average Loss: 2875.800537\n",
      "2025-03-07 23:31: Train Epoch 177: 0/23 Loss: 0.419997\n",
      "2025-03-07 23:31: Train Epoch 177: 0/23 Loss: 0.419997\n",
      "2025-03-07 23:31: Train Epoch 177: 20/23 Loss: 472.190765\n",
      "2025-03-07 23:31: Train Epoch 177: 20/23 Loss: 472.190765\n",
      "2025-03-07 23:31: **********Train Epoch 177: averaged Loss: 121.125757\n",
      "2025-03-07 23:31: **********Train Epoch 177: averaged Loss: 121.125757\n",
      "2025-03-07 23:31: **********Val Epoch 177: average Loss: 2886.115112\n",
      "2025-03-07 23:31: **********Val Epoch 177: average Loss: 2886.115112\n",
      "2025-03-07 23:31: Train Epoch 178: 0/23 Loss: 7.858829\n",
      "2025-03-07 23:31: Train Epoch 178: 0/23 Loss: 7.858829\n",
      "2025-03-07 23:31: Train Epoch 178: 20/23 Loss: 347.027039\n",
      "2025-03-07 23:31: Train Epoch 178: 20/23 Loss: 347.027039\n",
      "2025-03-07 23:31: **********Train Epoch 178: averaged Loss: 122.847168\n",
      "2025-03-07 23:31: **********Train Epoch 178: averaged Loss: 122.847168\n",
      "2025-03-07 23:31: **********Val Epoch 178: average Loss: 2873.354614\n",
      "2025-03-07 23:31: **********Val Epoch 178: average Loss: 2873.354614\n",
      "2025-03-07 23:31: Train Epoch 179: 0/23 Loss: 1.265992\n",
      "2025-03-07 23:31: Train Epoch 179: 0/23 Loss: 1.265992\n",
      "2025-03-07 23:31: Train Epoch 179: 20/23 Loss: 398.797729\n",
      "2025-03-07 23:31: Train Epoch 179: 20/23 Loss: 398.797729\n",
      "2025-03-07 23:31: **********Train Epoch 179: averaged Loss: 113.873301\n",
      "2025-03-07 23:31: **********Train Epoch 179: averaged Loss: 113.873301\n",
      "2025-03-07 23:31: **********Val Epoch 179: average Loss: 2867.863892\n",
      "2025-03-07 23:31: **********Val Epoch 179: average Loss: 2867.863892\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 180: 0/23 Loss: 3.440487\n",
      "2025-03-07 23:31: Train Epoch 180: 0/23 Loss: 3.440487\n",
      "2025-03-07 23:31: Train Epoch 180: 20/23 Loss: 370.301788\n",
      "2025-03-07 23:31: Train Epoch 180: 20/23 Loss: 370.301788\n",
      "2025-03-07 23:31: **********Train Epoch 180: averaged Loss: 108.426632\n",
      "2025-03-07 23:31: **********Train Epoch 180: averaged Loss: 108.426632\n",
      "2025-03-07 23:31: **********Val Epoch 180: average Loss: 2841.398560\n",
      "2025-03-07 23:31: **********Val Epoch 180: average Loss: 2841.398560\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 181: 0/23 Loss: 3.419030\n",
      "2025-03-07 23:31: Train Epoch 181: 0/23 Loss: 3.419030\n",
      "2025-03-07 23:31: Train Epoch 181: 20/23 Loss: 358.529846\n",
      "2025-03-07 23:31: Train Epoch 181: 20/23 Loss: 358.529846\n",
      "2025-03-07 23:31: **********Train Epoch 181: averaged Loss: 103.614047\n",
      "2025-03-07 23:31: **********Train Epoch 181: averaged Loss: 103.614047\n",
      "2025-03-07 23:31: **********Val Epoch 181: average Loss: 2838.682373\n",
      "2025-03-07 23:31: **********Val Epoch 181: average Loss: 2838.682373\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 182: 0/23 Loss: 6.309003\n",
      "2025-03-07 23:31: Train Epoch 182: 0/23 Loss: 6.309003\n",
      "2025-03-07 23:31: Train Epoch 182: 20/23 Loss: 339.319000\n",
      "2025-03-07 23:31: Train Epoch 182: 20/23 Loss: 339.319000\n",
      "2025-03-07 23:31: **********Train Epoch 182: averaged Loss: 101.972411\n",
      "2025-03-07 23:31: **********Train Epoch 182: averaged Loss: 101.972411\n",
      "2025-03-07 23:31: **********Val Epoch 182: average Loss: 2843.572510\n",
      "2025-03-07 23:31: **********Val Epoch 182: average Loss: 2843.572510\n",
      "2025-03-07 23:31: Train Epoch 183: 0/23 Loss: 2.963990\n",
      "2025-03-07 23:31: Train Epoch 183: 0/23 Loss: 2.963990\n",
      "2025-03-07 23:31: Train Epoch 183: 20/23 Loss: 348.732819\n",
      "2025-03-07 23:31: Train Epoch 183: 20/23 Loss: 348.732819\n",
      "2025-03-07 23:31: **********Train Epoch 183: averaged Loss: 99.459022\n",
      "2025-03-07 23:31: **********Train Epoch 183: averaged Loss: 99.459022\n",
      "2025-03-07 23:31: **********Val Epoch 183: average Loss: 2842.187866\n",
      "2025-03-07 23:31: **********Val Epoch 183: average Loss: 2842.187866\n",
      "2025-03-07 23:31: Train Epoch 184: 0/23 Loss: 7.037003\n",
      "2025-03-07 23:31: Train Epoch 184: 0/23 Loss: 7.037003\n",
      "2025-03-07 23:31: Train Epoch 184: 20/23 Loss: 372.168335\n",
      "2025-03-07 23:31: Train Epoch 184: 20/23 Loss: 372.168335\n",
      "2025-03-07 23:31: **********Train Epoch 184: averaged Loss: 96.277649\n",
      "2025-03-07 23:31: **********Train Epoch 184: averaged Loss: 96.277649\n",
      "2025-03-07 23:31: **********Val Epoch 184: average Loss: 2789.913818\n",
      "2025-03-07 23:31: **********Val Epoch 184: average Loss: 2789.913818\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 185: 0/23 Loss: 3.162960\n",
      "2025-03-07 23:31: Train Epoch 185: 0/23 Loss: 3.162960\n",
      "2025-03-07 23:31: Train Epoch 185: 20/23 Loss: 327.672485\n",
      "2025-03-07 23:31: Train Epoch 185: 20/23 Loss: 327.672485\n",
      "2025-03-07 23:31: **********Train Epoch 185: averaged Loss: 96.492894\n",
      "2025-03-07 23:31: **********Train Epoch 185: averaged Loss: 96.492894\n",
      "2025-03-07 23:31: **********Val Epoch 185: average Loss: 2823.863770\n",
      "2025-03-07 23:31: **********Val Epoch 185: average Loss: 2823.863770\n",
      "2025-03-07 23:31: Train Epoch 186: 0/23 Loss: 7.659038\n",
      "2025-03-07 23:31: Train Epoch 186: 0/23 Loss: 7.659038\n",
      "2025-03-07 23:31: Train Epoch 186: 20/23 Loss: 296.111389\n",
      "2025-03-07 23:31: Train Epoch 186: 20/23 Loss: 296.111389\n",
      "2025-03-07 23:31: **********Train Epoch 186: averaged Loss: 96.195747\n",
      "2025-03-07 23:31: **********Train Epoch 186: averaged Loss: 96.195747\n",
      "2025-03-07 23:31: **********Val Epoch 186: average Loss: 2802.373291\n",
      "2025-03-07 23:31: **********Val Epoch 186: average Loss: 2802.373291\n",
      "2025-03-07 23:31: Train Epoch 187: 0/23 Loss: 0.393676\n",
      "2025-03-07 23:31: Train Epoch 187: 0/23 Loss: 0.393676\n",
      "2025-03-07 23:31: Train Epoch 187: 20/23 Loss: 335.651001\n",
      "2025-03-07 23:31: Train Epoch 187: 20/23 Loss: 335.651001\n",
      "2025-03-07 23:31: **********Train Epoch 187: averaged Loss: 96.083638\n",
      "2025-03-07 23:31: **********Train Epoch 187: averaged Loss: 96.083638\n",
      "2025-03-07 23:31: **********Val Epoch 187: average Loss: 2809.819336\n",
      "2025-03-07 23:31: **********Val Epoch 187: average Loss: 2809.819336\n",
      "2025-03-07 23:31: Train Epoch 188: 0/23 Loss: 9.114934\n",
      "2025-03-07 23:31: Train Epoch 188: 0/23 Loss: 9.114934\n",
      "2025-03-07 23:31: Train Epoch 188: 20/23 Loss: 300.294983\n",
      "2025-03-07 23:31: Train Epoch 188: 20/23 Loss: 300.294983\n",
      "2025-03-07 23:31: **********Train Epoch 188: averaged Loss: 100.521768\n",
      "2025-03-07 23:31: **********Train Epoch 188: averaged Loss: 100.521768\n",
      "2025-03-07 23:31: **********Val Epoch 188: average Loss: 2801.352295\n",
      "2025-03-07 23:31: **********Val Epoch 188: average Loss: 2801.352295\n",
      "2025-03-07 23:31: Train Epoch 189: 0/23 Loss: 3.165482\n",
      "2025-03-07 23:31: Train Epoch 189: 0/23 Loss: 3.165482\n",
      "2025-03-07 23:31: Train Epoch 189: 20/23 Loss: 304.308105\n",
      "2025-03-07 23:31: Train Epoch 189: 20/23 Loss: 304.308105\n",
      "2025-03-07 23:31: **********Train Epoch 189: averaged Loss: 97.303487\n",
      "2025-03-07 23:31: **********Train Epoch 189: averaged Loss: 97.303487\n",
      "2025-03-07 23:31: **********Val Epoch 189: average Loss: 2777.995605\n",
      "2025-03-07 23:31: **********Val Epoch 189: average Loss: 2777.995605\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 190: 0/23 Loss: 6.764575\n",
      "2025-03-07 23:31: Train Epoch 190: 0/23 Loss: 6.764575\n",
      "2025-03-07 23:31: Train Epoch 190: 20/23 Loss: 292.533691\n",
      "2025-03-07 23:31: Train Epoch 190: 20/23 Loss: 292.533691\n",
      "2025-03-07 23:31: **********Train Epoch 190: averaged Loss: 104.895155\n",
      "2025-03-07 23:31: **********Train Epoch 190: averaged Loss: 104.895155\n",
      "2025-03-07 23:31: **********Val Epoch 190: average Loss: 2767.639038\n",
      "2025-03-07 23:31: **********Val Epoch 190: average Loss: 2767.639038\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 191: 0/23 Loss: 1.414532\n",
      "2025-03-07 23:31: Train Epoch 191: 0/23 Loss: 1.414532\n",
      "2025-03-07 23:31: Train Epoch 191: 20/23 Loss: 404.745178\n",
      "2025-03-07 23:31: Train Epoch 191: 20/23 Loss: 404.745178\n",
      "2025-03-07 23:31: **********Train Epoch 191: averaged Loss: 113.524876\n",
      "2025-03-07 23:31: **********Train Epoch 191: averaged Loss: 113.524876\n",
      "2025-03-07 23:31: **********Val Epoch 191: average Loss: 2769.568970\n",
      "2025-03-07 23:31: **********Val Epoch 191: average Loss: 2769.568970\n",
      "2025-03-07 23:31: Train Epoch 192: 0/23 Loss: 20.310743\n",
      "2025-03-07 23:31: Train Epoch 192: 0/23 Loss: 20.310743\n",
      "2025-03-07 23:31: Train Epoch 192: 20/23 Loss: 289.510193\n",
      "2025-03-07 23:31: Train Epoch 192: 20/23 Loss: 289.510193\n",
      "2025-03-07 23:31: **********Train Epoch 192: averaged Loss: 90.687998\n",
      "2025-03-07 23:31: **********Train Epoch 192: averaged Loss: 90.687998\n",
      "2025-03-07 23:31: **********Val Epoch 192: average Loss: 2760.435425\n",
      "2025-03-07 23:31: **********Val Epoch 192: average Loss: 2760.435425\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 193: 0/23 Loss: 6.287259\n",
      "2025-03-07 23:31: Train Epoch 193: 0/23 Loss: 6.287259\n",
      "2025-03-07 23:31: Train Epoch 193: 20/23 Loss: 229.226868\n",
      "2025-03-07 23:31: Train Epoch 193: 20/23 Loss: 229.226868\n",
      "2025-03-07 23:31: **********Train Epoch 193: averaged Loss: 86.985834\n",
      "2025-03-07 23:31: **********Train Epoch 193: averaged Loss: 86.985834\n",
      "2025-03-07 23:31: **********Val Epoch 193: average Loss: 2756.868408\n",
      "2025-03-07 23:31: **********Val Epoch 193: average Loss: 2756.868408\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 194: 0/23 Loss: 1.721740\n",
      "2025-03-07 23:31: Train Epoch 194: 0/23 Loss: 1.721740\n",
      "2025-03-07 23:31: Train Epoch 194: 20/23 Loss: 291.937195\n",
      "2025-03-07 23:31: Train Epoch 194: 20/23 Loss: 291.937195\n",
      "2025-03-07 23:31: **********Train Epoch 194: averaged Loss: 84.512917\n",
      "2025-03-07 23:31: **********Train Epoch 194: averaged Loss: 84.512917\n",
      "2025-03-07 23:31: **********Val Epoch 194: average Loss: 2706.581299\n",
      "2025-03-07 23:31: **********Val Epoch 194: average Loss: 2706.581299\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: *********************************Current best model saved!\n",
      "2025-03-07 23:31: Train Epoch 195: 0/23 Loss: 1.695807\n",
      "2025-03-07 23:31: Train Epoch 195: 0/23 Loss: 1.695807\n",
      "2025-03-07 23:32: Train Epoch 195: 20/23 Loss: 199.184341\n",
      "2025-03-07 23:32: Train Epoch 195: 20/23 Loss: 199.184341\n",
      "2025-03-07 23:32: **********Train Epoch 195: averaged Loss: 93.901129\n",
      "2025-03-07 23:32: **********Train Epoch 195: averaged Loss: 93.901129\n",
      "2025-03-07 23:32: **********Val Epoch 195: average Loss: 2748.757812\n",
      "2025-03-07 23:32: **********Val Epoch 195: average Loss: 2748.757812\n",
      "2025-03-07 23:32: Train Epoch 196: 0/23 Loss: 7.555333\n",
      "2025-03-07 23:32: Train Epoch 196: 0/23 Loss: 7.555333\n",
      "2025-03-07 23:32: Train Epoch 196: 20/23 Loss: 250.166138\n",
      "2025-03-07 23:32: Train Epoch 196: 20/23 Loss: 250.166138\n",
      "2025-03-07 23:32: **********Train Epoch 196: averaged Loss: 85.524672\n",
      "2025-03-07 23:32: **********Train Epoch 196: averaged Loss: 85.524672\n",
      "2025-03-07 23:32: **********Val Epoch 196: average Loss: 2724.890381\n",
      "2025-03-07 23:32: **********Val Epoch 196: average Loss: 2724.890381\n",
      "2025-03-07 23:32: Train Epoch 197: 0/23 Loss: 6.338568\n",
      "2025-03-07 23:32: Train Epoch 197: 0/23 Loss: 6.338568\n",
      "2025-03-07 23:32: Train Epoch 197: 20/23 Loss: 213.588058\n",
      "2025-03-07 23:32: Train Epoch 197: 20/23 Loss: 213.588058\n",
      "2025-03-07 23:32: **********Train Epoch 197: averaged Loss: 84.567642\n",
      "2025-03-07 23:32: **********Train Epoch 197: averaged Loss: 84.567642\n",
      "2025-03-07 23:32: **********Val Epoch 197: average Loss: 2700.629028\n",
      "2025-03-07 23:32: **********Val Epoch 197: average Loss: 2700.629028\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 198: 0/23 Loss: 1.420820\n",
      "2025-03-07 23:32: Train Epoch 198: 0/23 Loss: 1.420820\n",
      "2025-03-07 23:32: Train Epoch 198: 20/23 Loss: 215.513702\n",
      "2025-03-07 23:32: Train Epoch 198: 20/23 Loss: 215.513702\n",
      "2025-03-07 23:32: **********Train Epoch 198: averaged Loss: 76.298242\n",
      "2025-03-07 23:32: **********Train Epoch 198: averaged Loss: 76.298242\n",
      "2025-03-07 23:32: **********Val Epoch 198: average Loss: 2702.419678\n",
      "2025-03-07 23:32: **********Val Epoch 198: average Loss: 2702.419678\n",
      "2025-03-07 23:32: Train Epoch 199: 0/23 Loss: 7.403974\n",
      "2025-03-07 23:32: Train Epoch 199: 0/23 Loss: 7.403974\n",
      "2025-03-07 23:32: Train Epoch 199: 20/23 Loss: 222.941742\n",
      "2025-03-07 23:32: Train Epoch 199: 20/23 Loss: 222.941742\n",
      "2025-03-07 23:32: **********Train Epoch 199: averaged Loss: 78.525182\n",
      "2025-03-07 23:32: **********Train Epoch 199: averaged Loss: 78.525182\n",
      "2025-03-07 23:32: **********Val Epoch 199: average Loss: 2723.171509\n",
      "2025-03-07 23:32: **********Val Epoch 199: average Loss: 2723.171509\n",
      "2025-03-07 23:32: Train Epoch 200: 0/23 Loss: 2.322179\n",
      "2025-03-07 23:32: Train Epoch 200: 0/23 Loss: 2.322179\n",
      "2025-03-07 23:32: Train Epoch 200: 20/23 Loss: 296.093872\n",
      "2025-03-07 23:32: Train Epoch 200: 20/23 Loss: 296.093872\n",
      "2025-03-07 23:32: **********Train Epoch 200: averaged Loss: 83.861775\n",
      "2025-03-07 23:32: **********Train Epoch 200: averaged Loss: 83.861775\n",
      "2025-03-07 23:32: **********Val Epoch 200: average Loss: 2692.674927\n",
      "2025-03-07 23:32: **********Val Epoch 200: average Loss: 2692.674927\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 201: 0/23 Loss: 7.730155\n",
      "2025-03-07 23:32: Train Epoch 201: 0/23 Loss: 7.730155\n",
      "2025-03-07 23:32: Train Epoch 201: 20/23 Loss: 190.780365\n",
      "2025-03-07 23:32: Train Epoch 201: 20/23 Loss: 190.780365\n",
      "2025-03-07 23:32: **********Train Epoch 201: averaged Loss: 99.451832\n",
      "2025-03-07 23:32: **********Train Epoch 201: averaged Loss: 99.451832\n",
      "2025-03-07 23:32: **********Val Epoch 201: average Loss: 2664.552490\n",
      "2025-03-07 23:32: **********Val Epoch 201: average Loss: 2664.552490\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 202: 0/23 Loss: 1.536644\n",
      "2025-03-07 23:32: Train Epoch 202: 0/23 Loss: 1.536644\n",
      "2025-03-07 23:32: Train Epoch 202: 20/23 Loss: 238.737625\n",
      "2025-03-07 23:32: Train Epoch 202: 20/23 Loss: 238.737625\n",
      "2025-03-07 23:32: **********Train Epoch 202: averaged Loss: 79.395719\n",
      "2025-03-07 23:32: **********Train Epoch 202: averaged Loss: 79.395719\n",
      "2025-03-07 23:32: **********Val Epoch 202: average Loss: 2681.568726\n",
      "2025-03-07 23:32: **********Val Epoch 202: average Loss: 2681.568726\n",
      "2025-03-07 23:32: Train Epoch 203: 0/23 Loss: 7.726223\n",
      "2025-03-07 23:32: Train Epoch 203: 0/23 Loss: 7.726223\n",
      "2025-03-07 23:32: Train Epoch 203: 20/23 Loss: 202.882507\n",
      "2025-03-07 23:32: Train Epoch 203: 20/23 Loss: 202.882507\n",
      "2025-03-07 23:32: **********Train Epoch 203: averaged Loss: 84.919948\n",
      "2025-03-07 23:32: **********Train Epoch 203: averaged Loss: 84.919948\n",
      "2025-03-07 23:32: **********Val Epoch 203: average Loss: 2689.811279\n",
      "2025-03-07 23:32: **********Val Epoch 203: average Loss: 2689.811279\n",
      "2025-03-07 23:32: Train Epoch 204: 0/23 Loss: 1.290143\n",
      "2025-03-07 23:32: Train Epoch 204: 0/23 Loss: 1.290143\n",
      "2025-03-07 23:32: Train Epoch 204: 20/23 Loss: 230.421051\n",
      "2025-03-07 23:32: Train Epoch 204: 20/23 Loss: 230.421051\n",
      "2025-03-07 23:32: **********Train Epoch 204: averaged Loss: 78.433573\n",
      "2025-03-07 23:32: **********Train Epoch 204: averaged Loss: 78.433573\n",
      "2025-03-07 23:32: **********Val Epoch 204: average Loss: 2641.580200\n",
      "2025-03-07 23:32: **********Val Epoch 204: average Loss: 2641.580200\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 205: 0/23 Loss: 12.641437\n",
      "2025-03-07 23:32: Train Epoch 205: 0/23 Loss: 12.641437\n",
      "2025-03-07 23:32: Train Epoch 205: 20/23 Loss: 169.304535\n",
      "2025-03-07 23:32: Train Epoch 205: 20/23 Loss: 169.304535\n",
      "2025-03-07 23:32: **********Train Epoch 205: averaged Loss: 71.590023\n",
      "2025-03-07 23:32: **********Train Epoch 205: averaged Loss: 71.590023\n",
      "2025-03-07 23:32: **********Val Epoch 205: average Loss: 2663.425903\n",
      "2025-03-07 23:32: **********Val Epoch 205: average Loss: 2663.425903\n",
      "2025-03-07 23:32: Train Epoch 206: 0/23 Loss: 0.573538\n",
      "2025-03-07 23:32: Train Epoch 206: 0/23 Loss: 0.573538\n",
      "2025-03-07 23:32: Train Epoch 206: 20/23 Loss: 243.996704\n",
      "2025-03-07 23:32: Train Epoch 206: 20/23 Loss: 243.996704\n",
      "2025-03-07 23:32: **********Train Epoch 206: averaged Loss: 75.504129\n",
      "2025-03-07 23:32: **********Train Epoch 206: averaged Loss: 75.504129\n",
      "2025-03-07 23:32: **********Val Epoch 206: average Loss: 2635.597290\n",
      "2025-03-07 23:32: **********Val Epoch 206: average Loss: 2635.597290\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 207: 0/23 Loss: 13.139128\n",
      "2025-03-07 23:32: Train Epoch 207: 0/23 Loss: 13.139128\n",
      "2025-03-07 23:32: Train Epoch 207: 20/23 Loss: 175.073975\n",
      "2025-03-07 23:32: Train Epoch 207: 20/23 Loss: 175.073975\n",
      "2025-03-07 23:32: **********Train Epoch 207: averaged Loss: 99.830772\n",
      "2025-03-07 23:32: **********Train Epoch 207: averaged Loss: 99.830772\n",
      "2025-03-07 23:32: **********Val Epoch 207: average Loss: 2611.900269\n",
      "2025-03-07 23:32: **********Val Epoch 207: average Loss: 2611.900269\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 208: 0/23 Loss: 2.410100\n",
      "2025-03-07 23:32: Train Epoch 208: 0/23 Loss: 2.410100\n",
      "2025-03-07 23:32: Train Epoch 208: 20/23 Loss: 234.507156\n",
      "2025-03-07 23:32: Train Epoch 208: 20/23 Loss: 234.507156\n",
      "2025-03-07 23:32: **********Train Epoch 208: averaged Loss: 81.817327\n",
      "2025-03-07 23:32: **********Train Epoch 208: averaged Loss: 81.817327\n",
      "2025-03-07 23:32: **********Val Epoch 208: average Loss: 2591.112427\n",
      "2025-03-07 23:32: **********Val Epoch 208: average Loss: 2591.112427\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 209: 0/23 Loss: 8.428501\n",
      "2025-03-07 23:32: Train Epoch 209: 0/23 Loss: 8.428501\n",
      "2025-03-07 23:32: Train Epoch 209: 20/23 Loss: 162.245880\n",
      "2025-03-07 23:32: Train Epoch 209: 20/23 Loss: 162.245880\n",
      "2025-03-07 23:32: **********Train Epoch 209: averaged Loss: 84.149683\n",
      "2025-03-07 23:32: **********Train Epoch 209: averaged Loss: 84.149683\n",
      "2025-03-07 23:32: **********Val Epoch 209: average Loss: 2630.264709\n",
      "2025-03-07 23:32: **********Val Epoch 209: average Loss: 2630.264709\n",
      "2025-03-07 23:32: Train Epoch 210: 0/23 Loss: 0.823574\n",
      "2025-03-07 23:32: Train Epoch 210: 0/23 Loss: 0.823574\n",
      "2025-03-07 23:32: Train Epoch 210: 20/23 Loss: 141.484894\n",
      "2025-03-07 23:32: Train Epoch 210: 20/23 Loss: 141.484894\n",
      "2025-03-07 23:32: **********Train Epoch 210: averaged Loss: 75.324791\n",
      "2025-03-07 23:32: **********Train Epoch 210: averaged Loss: 75.324791\n",
      "2025-03-07 23:32: **********Val Epoch 210: average Loss: 2636.532349\n",
      "2025-03-07 23:32: **********Val Epoch 210: average Loss: 2636.532349\n",
      "2025-03-07 23:32: Train Epoch 211: 0/23 Loss: 4.849119\n",
      "2025-03-07 23:32: Train Epoch 211: 0/23 Loss: 4.849119\n",
      "2025-03-07 23:32: Train Epoch 211: 20/23 Loss: 159.201416\n",
      "2025-03-07 23:32: Train Epoch 211: 20/23 Loss: 159.201416\n",
      "2025-03-07 23:32: **********Train Epoch 211: averaged Loss: 71.069746\n",
      "2025-03-07 23:32: **********Train Epoch 211: averaged Loss: 71.069746\n",
      "2025-03-07 23:32: **********Val Epoch 211: average Loss: 2598.196289\n",
      "2025-03-07 23:32: **********Val Epoch 211: average Loss: 2598.196289\n",
      "2025-03-07 23:32: Train Epoch 212: 0/23 Loss: 1.511865\n",
      "2025-03-07 23:32: Train Epoch 212: 0/23 Loss: 1.511865\n",
      "2025-03-07 23:32: Train Epoch 212: 20/23 Loss: 189.195129\n",
      "2025-03-07 23:32: Train Epoch 212: 20/23 Loss: 189.195129\n",
      "2025-03-07 23:32: **********Train Epoch 212: averaged Loss: 68.296048\n",
      "2025-03-07 23:32: **********Train Epoch 212: averaged Loss: 68.296048\n",
      "2025-03-07 23:32: **********Val Epoch 212: average Loss: 2585.581909\n",
      "2025-03-07 23:32: **********Val Epoch 212: average Loss: 2585.581909\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 213: 0/23 Loss: 2.487224\n",
      "2025-03-07 23:32: Train Epoch 213: 0/23 Loss: 2.487224\n",
      "2025-03-07 23:32: Train Epoch 213: 20/23 Loss: 116.284256\n",
      "2025-03-07 23:32: Train Epoch 213: 20/23 Loss: 116.284256\n",
      "2025-03-07 23:32: **********Train Epoch 213: averaged Loss: 67.695019\n",
      "2025-03-07 23:32: **********Train Epoch 213: averaged Loss: 67.695019\n",
      "2025-03-07 23:32: **********Val Epoch 213: average Loss: 2589.233459\n",
      "2025-03-07 23:32: **********Val Epoch 213: average Loss: 2589.233459\n",
      "2025-03-07 23:32: Train Epoch 214: 0/23 Loss: 0.710234\n",
      "2025-03-07 23:32: Train Epoch 214: 0/23 Loss: 0.710234\n",
      "2025-03-07 23:32: Train Epoch 214: 20/23 Loss: 128.393036\n",
      "2025-03-07 23:32: Train Epoch 214: 20/23 Loss: 128.393036\n",
      "2025-03-07 23:32: **********Train Epoch 214: averaged Loss: 64.855627\n",
      "2025-03-07 23:32: **********Train Epoch 214: averaged Loss: 64.855627\n",
      "2025-03-07 23:32: **********Val Epoch 214: average Loss: 2577.851624\n",
      "2025-03-07 23:32: **********Val Epoch 214: average Loss: 2577.851624\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 215: 0/23 Loss: 11.307566\n",
      "2025-03-07 23:32: Train Epoch 215: 0/23 Loss: 11.307566\n",
      "2025-03-07 23:32: Train Epoch 215: 20/23 Loss: 184.740509\n",
      "2025-03-07 23:32: Train Epoch 215: 20/23 Loss: 184.740509\n",
      "2025-03-07 23:32: **********Train Epoch 215: averaged Loss: 64.915646\n",
      "2025-03-07 23:32: **********Train Epoch 215: averaged Loss: 64.915646\n",
      "2025-03-07 23:32: **********Val Epoch 215: average Loss: 2547.813293\n",
      "2025-03-07 23:32: **********Val Epoch 215: average Loss: 2547.813293\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 216: 0/23 Loss: 4.682036\n",
      "2025-03-07 23:32: Train Epoch 216: 0/23 Loss: 4.682036\n",
      "2025-03-07 23:32: Train Epoch 216: 20/23 Loss: 134.748108\n",
      "2025-03-07 23:32: Train Epoch 216: 20/23 Loss: 134.748108\n",
      "2025-03-07 23:32: **********Train Epoch 216: averaged Loss: 68.862447\n",
      "2025-03-07 23:32: **********Train Epoch 216: averaged Loss: 68.862447\n",
      "2025-03-07 23:32: **********Val Epoch 216: average Loss: 2567.927612\n",
      "2025-03-07 23:32: **********Val Epoch 216: average Loss: 2567.927612\n",
      "2025-03-07 23:32: Train Epoch 217: 0/23 Loss: 2.716627\n",
      "2025-03-07 23:32: Train Epoch 217: 0/23 Loss: 2.716627\n",
      "2025-03-07 23:32: Train Epoch 217: 20/23 Loss: 126.984642\n",
      "2025-03-07 23:32: Train Epoch 217: 20/23 Loss: 126.984642\n",
      "2025-03-07 23:32: **********Train Epoch 217: averaged Loss: 59.751498\n",
      "2025-03-07 23:32: **********Train Epoch 217: averaged Loss: 59.751498\n",
      "2025-03-07 23:32: **********Val Epoch 217: average Loss: 2535.651794\n",
      "2025-03-07 23:32: **********Val Epoch 217: average Loss: 2535.651794\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 218: 0/23 Loss: 7.612747\n",
      "2025-03-07 23:32: Train Epoch 218: 0/23 Loss: 7.612747\n",
      "2025-03-07 23:32: Train Epoch 218: 20/23 Loss: 108.430405\n",
      "2025-03-07 23:32: Train Epoch 218: 20/23 Loss: 108.430405\n",
      "2025-03-07 23:32: **********Train Epoch 218: averaged Loss: 74.540686\n",
      "2025-03-07 23:32: **********Train Epoch 218: averaged Loss: 74.540686\n",
      "2025-03-07 23:32: **********Val Epoch 218: average Loss: 2577.768188\n",
      "2025-03-07 23:32: **********Val Epoch 218: average Loss: 2577.768188\n",
      "2025-03-07 23:32: Train Epoch 219: 0/23 Loss: 1.329514\n",
      "2025-03-07 23:32: Train Epoch 219: 0/23 Loss: 1.329514\n",
      "2025-03-07 23:32: Train Epoch 219: 20/23 Loss: 171.114807\n",
      "2025-03-07 23:32: Train Epoch 219: 20/23 Loss: 171.114807\n",
      "2025-03-07 23:32: **********Train Epoch 219: averaged Loss: 64.444232\n",
      "2025-03-07 23:32: **********Train Epoch 219: averaged Loss: 64.444232\n",
      "2025-03-07 23:32: **********Val Epoch 219: average Loss: 2535.696594\n",
      "2025-03-07 23:32: **********Val Epoch 219: average Loss: 2535.696594\n",
      "2025-03-07 23:32: Train Epoch 220: 0/23 Loss: 10.861650\n",
      "2025-03-07 23:32: Train Epoch 220: 0/23 Loss: 10.861650\n",
      "2025-03-07 23:32: Train Epoch 220: 20/23 Loss: 119.344124\n",
      "2025-03-07 23:32: Train Epoch 220: 20/23 Loss: 119.344124\n",
      "2025-03-07 23:32: **********Train Epoch 220: averaged Loss: 66.410595\n",
      "2025-03-07 23:32: **********Train Epoch 220: averaged Loss: 66.410595\n",
      "2025-03-07 23:32: **********Val Epoch 220: average Loss: 2540.526978\n",
      "2025-03-07 23:32: **********Val Epoch 220: average Loss: 2540.526978\n",
      "2025-03-07 23:32: Train Epoch 221: 0/23 Loss: 8.679482\n",
      "2025-03-07 23:32: Train Epoch 221: 0/23 Loss: 8.679482\n",
      "2025-03-07 23:32: Train Epoch 221: 20/23 Loss: 131.257050\n",
      "2025-03-07 23:32: Train Epoch 221: 20/23 Loss: 131.257050\n",
      "2025-03-07 23:32: **********Train Epoch 221: averaged Loss: 60.030099\n",
      "2025-03-07 23:32: **********Train Epoch 221: averaged Loss: 60.030099\n",
      "2025-03-07 23:32: **********Val Epoch 221: average Loss: 2515.909241\n",
      "2025-03-07 23:32: **********Val Epoch 221: average Loss: 2515.909241\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 222: 0/23 Loss: 8.908639\n",
      "2025-03-07 23:32: Train Epoch 222: 0/23 Loss: 8.908639\n",
      "2025-03-07 23:32: Train Epoch 222: 20/23 Loss: 94.355705\n",
      "2025-03-07 23:32: Train Epoch 222: 20/23 Loss: 94.355705\n",
      "2025-03-07 23:32: **********Train Epoch 222: averaged Loss: 56.545255\n",
      "2025-03-07 23:32: **********Train Epoch 222: averaged Loss: 56.545255\n",
      "2025-03-07 23:32: **********Val Epoch 222: average Loss: 2519.414062\n",
      "2025-03-07 23:32: **********Val Epoch 222: average Loss: 2519.414062\n",
      "2025-03-07 23:32: Train Epoch 223: 0/23 Loss: 0.428377\n",
      "2025-03-07 23:32: Train Epoch 223: 0/23 Loss: 0.428377\n",
      "2025-03-07 23:32: Train Epoch 223: 20/23 Loss: 101.221924\n",
      "2025-03-07 23:32: Train Epoch 223: 20/23 Loss: 101.221924\n",
      "2025-03-07 23:32: **********Train Epoch 223: averaged Loss: 74.381453\n",
      "2025-03-07 23:32: **********Train Epoch 223: averaged Loss: 74.381453\n",
      "2025-03-07 23:32: **********Val Epoch 223: average Loss: 2490.127075\n",
      "2025-03-07 23:32: **********Val Epoch 223: average Loss: 2490.127075\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: *********************************Current best model saved!\n",
      "2025-03-07 23:32: Train Epoch 224: 0/23 Loss: 8.161840\n",
      "2025-03-07 23:32: Train Epoch 224: 0/23 Loss: 8.161840\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2025-03-08 00:06: Train Epoch 15: 20/23 Loss: 1098.569336\n",
      "2025-03-08 00:06: Train Epoch 15: 20/23 Loss: 1098.569336\n",
      "2025-03-08 00:06: Train Epoch 15: 20/23 Loss: 1098.569336\n",
      "2025-03-08 00:06: **********Train Epoch 15: averaged Loss: 270.575232\n",
      "2025-03-08 00:06: **********Train Epoch 15: averaged Loss: 270.575232\n",
      "2025-03-08 00:06: **********Train Epoch 15: averaged Loss: 270.575232\n",
      "2025-03-08 00:06: **********Val Epoch 15: average Loss: 3593.353760\n",
      "2025-03-08 00:06: **********Val Epoch 15: average Loss: 3593.353760\n",
      "2025-03-08 00:06: **********Val Epoch 15: average Loss: 3593.353760\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 16: 0/23 Loss: 5.923675\n",
      "2025-03-08 00:06: Train Epoch 16: 0/23 Loss: 5.923675\n",
      "2025-03-08 00:06: Train Epoch 16: 0/23 Loss: 5.923675\n",
      "2025-03-08 00:06: Train Epoch 16: 20/23 Loss: 1098.354736\n",
      "2025-03-08 00:06: Train Epoch 16: 20/23 Loss: 1098.354736\n",
      "2025-03-08 00:06: Train Epoch 16: 20/23 Loss: 1098.354736\n",
      "2025-03-08 00:06: **********Train Epoch 16: averaged Loss: 270.471620\n",
      "2025-03-08 00:06: **********Train Epoch 16: averaged Loss: 270.471620\n",
      "2025-03-08 00:06: **********Train Epoch 16: averaged Loss: 270.471620\n",
      "2025-03-08 00:06: **********Val Epoch 16: average Loss: 3592.456299\n",
      "2025-03-08 00:06: **********Val Epoch 16: average Loss: 3592.456299\n",
      "2025-03-08 00:06: **********Val Epoch 16: average Loss: 3592.456299\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 17: 0/23 Loss: 6.464414\n",
      "2025-03-08 00:06: Train Epoch 17: 0/23 Loss: 6.464414\n",
      "2025-03-08 00:06: Train Epoch 17: 0/23 Loss: 6.464414\n",
      "2025-03-08 00:06: Train Epoch 17: 20/23 Loss: 1097.402954\n",
      "2025-03-08 00:06: Train Epoch 17: 20/23 Loss: 1097.402954\n",
      "2025-03-08 00:06: Train Epoch 17: 20/23 Loss: 1097.402954\n",
      "2025-03-08 00:06: **********Train Epoch 17: averaged Loss: 270.199880\n",
      "2025-03-08 00:06: **********Train Epoch 17: averaged Loss: 270.199880\n",
      "2025-03-08 00:06: **********Train Epoch 17: averaged Loss: 270.199880\n",
      "2025-03-08 00:06: **********Val Epoch 17: average Loss: 3591.938232\n",
      "2025-03-08 00:06: **********Val Epoch 17: average Loss: 3591.938232\n",
      "2025-03-08 00:06: **********Val Epoch 17: average Loss: 3591.938232\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 18: 0/23 Loss: 5.806352\n",
      "2025-03-08 00:06: Train Epoch 18: 0/23 Loss: 5.806352\n",
      "2025-03-08 00:06: Train Epoch 18: 0/23 Loss: 5.806352\n",
      "2025-03-08 00:06: Train Epoch 18: 20/23 Loss: 1097.260498\n",
      "2025-03-08 00:06: Train Epoch 18: 20/23 Loss: 1097.260498\n",
      "2025-03-08 00:06: Train Epoch 18: 20/23 Loss: 1097.260498\n",
      "2025-03-08 00:06: **********Train Epoch 18: averaged Loss: 270.243110\n",
      "2025-03-08 00:06: **********Train Epoch 18: averaged Loss: 270.243110\n",
      "2025-03-08 00:06: **********Train Epoch 18: averaged Loss: 270.243110\n",
      "2025-03-08 00:06: **********Val Epoch 18: average Loss: 3591.228149\n",
      "2025-03-08 00:06: **********Val Epoch 18: average Loss: 3591.228149\n",
      "2025-03-08 00:06: **********Val Epoch 18: average Loss: 3591.228149\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 19: 0/23 Loss: 6.679824\n",
      "2025-03-08 00:06: Train Epoch 19: 0/23 Loss: 6.679824\n",
      "2025-03-08 00:06: Train Epoch 19: 0/23 Loss: 6.679824\n",
      "2025-03-08 00:06: Train Epoch 19: 20/23 Loss: 1096.374023\n",
      "2025-03-08 00:06: Train Epoch 19: 20/23 Loss: 1096.374023\n",
      "2025-03-08 00:06: Train Epoch 19: 20/23 Loss: 1096.374023\n",
      "2025-03-08 00:06: **********Train Epoch 19: averaged Loss: 269.711341\n",
      "2025-03-08 00:06: **********Train Epoch 19: averaged Loss: 269.711341\n",
      "2025-03-08 00:06: **********Train Epoch 19: averaged Loss: 269.711341\n",
      "2025-03-08 00:06: **********Val Epoch 19: average Loss: 3590.937012\n",
      "2025-03-08 00:06: **********Val Epoch 19: average Loss: 3590.937012\n",
      "2025-03-08 00:06: **********Val Epoch 19: average Loss: 3590.937012\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 20: 0/23 Loss: 6.465027\n",
      "2025-03-08 00:06: Train Epoch 20: 0/23 Loss: 6.465027\n",
      "2025-03-08 00:06: Train Epoch 20: 0/23 Loss: 6.465027\n",
      "2025-03-08 00:06: Train Epoch 20: 20/23 Loss: 1095.850464\n",
      "2025-03-08 00:06: Train Epoch 20: 20/23 Loss: 1095.850464\n",
      "2025-03-08 00:06: Train Epoch 20: 20/23 Loss: 1095.850464\n",
      "2025-03-08 00:06: **********Train Epoch 20: averaged Loss: 269.706398\n",
      "2025-03-08 00:06: **********Train Epoch 20: averaged Loss: 269.706398\n",
      "2025-03-08 00:06: **********Train Epoch 20: averaged Loss: 269.706398\n",
      "2025-03-08 00:06: **********Val Epoch 20: average Loss: 3589.792969\n",
      "2025-03-08 00:06: **********Val Epoch 20: average Loss: 3589.792969\n",
      "2025-03-08 00:06: **********Val Epoch 20: average Loss: 3589.792969\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 21: 0/23 Loss: 6.184000\n",
      "2025-03-08 00:06: Train Epoch 21: 0/23 Loss: 6.184000\n",
      "2025-03-08 00:06: Train Epoch 21: 0/23 Loss: 6.184000\n",
      "2025-03-08 00:06: Train Epoch 21: 20/23 Loss: 1095.447510\n",
      "2025-03-08 00:06: Train Epoch 21: 20/23 Loss: 1095.447510\n",
      "2025-03-08 00:06: Train Epoch 21: 20/23 Loss: 1095.447510\n",
      "2025-03-08 00:06: **********Train Epoch 21: averaged Loss: 269.185993\n",
      "2025-03-08 00:06: **********Train Epoch 21: averaged Loss: 269.185993\n",
      "2025-03-08 00:06: **********Train Epoch 21: averaged Loss: 269.185993\n",
      "2025-03-08 00:06: **********Val Epoch 21: average Loss: 3589.422363\n",
      "2025-03-08 00:06: **********Val Epoch 21: average Loss: 3589.422363\n",
      "2025-03-08 00:06: **********Val Epoch 21: average Loss: 3589.422363\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 22: 0/23 Loss: 5.216027\n",
      "2025-03-08 00:06: Train Epoch 22: 0/23 Loss: 5.216027\n",
      "2025-03-08 00:06: Train Epoch 22: 0/23 Loss: 5.216027\n",
      "2025-03-08 00:06: Train Epoch 22: 20/23 Loss: 1095.040039\n",
      "2025-03-08 00:06: Train Epoch 22: 20/23 Loss: 1095.040039\n",
      "2025-03-08 00:06: Train Epoch 22: 20/23 Loss: 1095.040039\n",
      "2025-03-08 00:06: **********Train Epoch 22: averaged Loss: 268.782802\n",
      "2025-03-08 00:06: **********Train Epoch 22: averaged Loss: 268.782802\n",
      "2025-03-08 00:06: **********Train Epoch 22: averaged Loss: 268.782802\n",
      "2025-03-08 00:06: **********Val Epoch 22: average Loss: 3588.560791\n",
      "2025-03-08 00:06: **********Val Epoch 22: average Loss: 3588.560791\n",
      "2025-03-08 00:06: **********Val Epoch 22: average Loss: 3588.560791\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 23: 0/23 Loss: 4.620804\n",
      "2025-03-08 00:06: Train Epoch 23: 0/23 Loss: 4.620804\n",
      "2025-03-08 00:06: Train Epoch 23: 0/23 Loss: 4.620804\n",
      "2025-03-08 00:06: Train Epoch 23: 20/23 Loss: 1094.552002\n",
      "2025-03-08 00:06: Train Epoch 23: 20/23 Loss: 1094.552002\n",
      "2025-03-08 00:06: Train Epoch 23: 20/23 Loss: 1094.552002\n",
      "2025-03-08 00:06: **********Train Epoch 23: averaged Loss: 268.430559\n",
      "2025-03-08 00:06: **********Train Epoch 23: averaged Loss: 268.430559\n",
      "2025-03-08 00:06: **********Train Epoch 23: averaged Loss: 268.430559\n",
      "2025-03-08 00:06: **********Val Epoch 23: average Loss: 3587.878540\n",
      "2025-03-08 00:06: **********Val Epoch 23: average Loss: 3587.878540\n",
      "2025-03-08 00:06: **********Val Epoch 23: average Loss: 3587.878540\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 24: 0/23 Loss: 4.129518\n",
      "2025-03-08 00:06: Train Epoch 24: 0/23 Loss: 4.129518\n",
      "2025-03-08 00:06: Train Epoch 24: 0/23 Loss: 4.129518\n",
      "2025-03-08 00:06: Train Epoch 24: 20/23 Loss: 1093.665283\n",
      "2025-03-08 00:06: Train Epoch 24: 20/23 Loss: 1093.665283\n",
      "2025-03-08 00:06: Train Epoch 24: 20/23 Loss: 1093.665283\n",
      "2025-03-08 00:06: **********Train Epoch 24: averaged Loss: 267.931620\n",
      "2025-03-08 00:06: **********Train Epoch 24: averaged Loss: 267.931620\n",
      "2025-03-08 00:06: **********Train Epoch 24: averaged Loss: 267.931620\n",
      "2025-03-08 00:06: **********Val Epoch 24: average Loss: 3587.715210\n",
      "2025-03-08 00:06: **********Val Epoch 24: average Loss: 3587.715210\n",
      "2025-03-08 00:06: **********Val Epoch 24: average Loss: 3587.715210\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 25: 0/23 Loss: 4.222577\n",
      "2025-03-08 00:06: Train Epoch 25: 0/23 Loss: 4.222577\n",
      "2025-03-08 00:06: Train Epoch 25: 0/23 Loss: 4.222577\n",
      "2025-03-08 00:06: Train Epoch 25: 20/23 Loss: 1093.321045\n",
      "2025-03-08 00:06: Train Epoch 25: 20/23 Loss: 1093.321045\n",
      "2025-03-08 00:06: Train Epoch 25: 20/23 Loss: 1093.321045\n",
      "2025-03-08 00:06: **********Train Epoch 25: averaged Loss: 267.619835\n",
      "2025-03-08 00:06: **********Train Epoch 25: averaged Loss: 267.619835\n",
      "2025-03-08 00:06: **********Train Epoch 25: averaged Loss: 267.619835\n",
      "2025-03-08 00:06: **********Val Epoch 25: average Loss: 3586.751709\n",
      "2025-03-08 00:06: **********Val Epoch 25: average Loss: 3586.751709\n",
      "2025-03-08 00:06: **********Val Epoch 25: average Loss: 3586.751709\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 26: 0/23 Loss: 3.792739\n",
      "2025-03-08 00:06: Train Epoch 26: 0/23 Loss: 3.792739\n",
      "2025-03-08 00:06: Train Epoch 26: 0/23 Loss: 3.792739\n",
      "2025-03-08 00:06: Train Epoch 26: 20/23 Loss: 1091.908447\n",
      "2025-03-08 00:06: Train Epoch 26: 20/23 Loss: 1091.908447\n",
      "2025-03-08 00:06: Train Epoch 26: 20/23 Loss: 1091.908447\n",
      "2025-03-08 00:06: **********Train Epoch 26: averaged Loss: 266.946370\n",
      "2025-03-08 00:06: **********Train Epoch 26: averaged Loss: 266.946370\n",
      "2025-03-08 00:06: **********Train Epoch 26: averaged Loss: 266.946370\n",
      "2025-03-08 00:06: **********Val Epoch 26: average Loss: 3586.138306\n",
      "2025-03-08 00:06: **********Val Epoch 26: average Loss: 3586.138306\n",
      "2025-03-08 00:06: **********Val Epoch 26: average Loss: 3586.138306\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 27: 0/23 Loss: 3.828038\n",
      "2025-03-08 00:06: Train Epoch 27: 0/23 Loss: 3.828038\n",
      "2025-03-08 00:06: Train Epoch 27: 0/23 Loss: 3.828038\n",
      "2025-03-08 00:06: Train Epoch 27: 20/23 Loss: 1091.085083\n",
      "2025-03-08 00:06: Train Epoch 27: 20/23 Loss: 1091.085083\n",
      "2025-03-08 00:06: Train Epoch 27: 20/23 Loss: 1091.085083\n",
      "2025-03-08 00:06: **********Train Epoch 27: averaged Loss: 266.516403\n",
      "2025-03-08 00:06: **********Train Epoch 27: averaged Loss: 266.516403\n",
      "2025-03-08 00:06: **********Train Epoch 27: averaged Loss: 266.516403\n",
      "2025-03-08 00:06: **********Val Epoch 27: average Loss: 3585.081787\n",
      "2025-03-08 00:06: **********Val Epoch 27: average Loss: 3585.081787\n",
      "2025-03-08 00:06: **********Val Epoch 27: average Loss: 3585.081787\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 28: 0/23 Loss: 3.874334\n",
      "2025-03-08 00:06: Train Epoch 28: 0/23 Loss: 3.874334\n",
      "2025-03-08 00:06: Train Epoch 28: 0/23 Loss: 3.874334\n",
      "2025-03-08 00:06: Train Epoch 28: 20/23 Loss: 1090.185303\n",
      "2025-03-08 00:06: Train Epoch 28: 20/23 Loss: 1090.185303\n",
      "2025-03-08 00:06: Train Epoch 28: 20/23 Loss: 1090.185303\n",
      "2025-03-08 00:06: **********Train Epoch 28: averaged Loss: 266.127071\n",
      "2025-03-08 00:06: **********Train Epoch 28: averaged Loss: 266.127071\n",
      "2025-03-08 00:06: **********Train Epoch 28: averaged Loss: 266.127071\n",
      "2025-03-08 00:06: **********Val Epoch 28: average Loss: 3584.564331\n",
      "2025-03-08 00:06: **********Val Epoch 28: average Loss: 3584.564331\n",
      "2025-03-08 00:06: **********Val Epoch 28: average Loss: 3584.564331\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 29: 0/23 Loss: 3.030032\n",
      "2025-03-08 00:06: Train Epoch 29: 0/23 Loss: 3.030032\n",
      "2025-03-08 00:06: Train Epoch 29: 0/23 Loss: 3.030032\n",
      "2025-03-08 00:06: Train Epoch 29: 20/23 Loss: 1090.158691\n",
      "2025-03-08 00:06: Train Epoch 29: 20/23 Loss: 1090.158691\n",
      "2025-03-08 00:06: Train Epoch 29: 20/23 Loss: 1090.158691\n",
      "2025-03-08 00:06: **********Train Epoch 29: averaged Loss: 265.522910\n",
      "2025-03-08 00:06: **********Train Epoch 29: averaged Loss: 265.522910\n",
      "2025-03-08 00:06: **********Train Epoch 29: averaged Loss: 265.522910\n",
      "2025-03-08 00:06: **********Val Epoch 29: average Loss: 3583.339355\n",
      "2025-03-08 00:06: **********Val Epoch 29: average Loss: 3583.339355\n",
      "2025-03-08 00:06: **********Val Epoch 29: average Loss: 3583.339355\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 30: 0/23 Loss: 3.750175\n",
      "2025-03-08 00:06: Train Epoch 30: 0/23 Loss: 3.750175\n",
      "2025-03-08 00:06: Train Epoch 30: 0/23 Loss: 3.750175\n",
      "2025-03-08 00:06: Train Epoch 30: 20/23 Loss: 1089.022339\n",
      "2025-03-08 00:06: Train Epoch 30: 20/23 Loss: 1089.022339\n",
      "2025-03-08 00:06: Train Epoch 30: 20/23 Loss: 1089.022339\n",
      "2025-03-08 00:06: **********Train Epoch 30: averaged Loss: 265.109847\n",
      "2025-03-08 00:06: **********Train Epoch 30: averaged Loss: 265.109847\n",
      "2025-03-08 00:06: **********Train Epoch 30: averaged Loss: 265.109847\n",
      "2025-03-08 00:06: **********Val Epoch 30: average Loss: 3582.406494\n",
      "2025-03-08 00:06: **********Val Epoch 30: average Loss: 3582.406494\n",
      "2025-03-08 00:06: **********Val Epoch 30: average Loss: 3582.406494\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 31: 0/23 Loss: 3.640689\n",
      "2025-03-08 00:06: Train Epoch 31: 0/23 Loss: 3.640689\n",
      "2025-03-08 00:06: Train Epoch 31: 0/23 Loss: 3.640689\n",
      "2025-03-08 00:06: Train Epoch 31: 20/23 Loss: 1088.406738\n",
      "2025-03-08 00:06: Train Epoch 31: 20/23 Loss: 1088.406738\n",
      "2025-03-08 00:06: Train Epoch 31: 20/23 Loss: 1088.406738\n",
      "2025-03-08 00:06: **********Train Epoch 31: averaged Loss: 264.584065\n",
      "2025-03-08 00:06: **********Train Epoch 31: averaged Loss: 264.584065\n",
      "2025-03-08 00:06: **********Train Epoch 31: averaged Loss: 264.584065\n",
      "2025-03-08 00:06: **********Val Epoch 31: average Loss: 3581.490234\n",
      "2025-03-08 00:06: **********Val Epoch 31: average Loss: 3581.490234\n",
      "2025-03-08 00:06: **********Val Epoch 31: average Loss: 3581.490234\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: *********************************Current best model saved!\n",
      "2025-03-08 00:06: Train Epoch 32: 0/23 Loss: 3.237337\n",
      "2025-03-08 00:06: Train Epoch 32: 0/23 Loss: 3.237337\n",
      "2025-03-08 00:06: Train Epoch 32: 0/23 Loss: 3.237337\n",
      "2025-03-08 00:06: Train Epoch 32: 20/23 Loss: 1086.327393\n",
      "2025-03-08 00:06: Train Epoch 32: 20/23 Loss: 1086.327393\n",
      "2025-03-08 00:07: **********Val Epoch 67: average Loss: 3498.812744\n",
      "2025-03-08 00:07: **********Val Epoch 67: average Loss: 3498.812744\n",
      "2025-03-08 00:07: **********Val Epoch 67: average Loss: 3498.812744\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 68: 0/23 Loss: 2.771359\n",
      "2025-03-08 00:07: Train Epoch 68: 0/23 Loss: 2.771359\n",
      "2025-03-08 00:07: Train Epoch 68: 0/23 Loss: 2.771359\n",
      "2025-03-08 00:07: Train Epoch 68: 20/23 Loss: 1000.907471\n",
      "2025-03-08 00:07: Train Epoch 68: 20/23 Loss: 1000.907471\n",
      "2025-03-08 00:07: Train Epoch 68: 20/23 Loss: 1000.907471\n",
      "2025-03-08 00:07: **********Train Epoch 68: averaged Loss: 229.177321\n",
      "2025-03-08 00:07: **********Train Epoch 68: averaged Loss: 229.177321\n",
      "2025-03-08 00:07: **********Train Epoch 68: averaged Loss: 229.177321\n",
      "2025-03-08 00:07: **********Val Epoch 68: average Loss: 3494.978516\n",
      "2025-03-08 00:07: **********Val Epoch 68: average Loss: 3494.978516\n",
      "2025-03-08 00:07: **********Val Epoch 68: average Loss: 3494.978516\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 69: 0/23 Loss: 5.844808\n",
      "2025-03-08 00:07: Train Epoch 69: 0/23 Loss: 5.844808\n",
      "2025-03-08 00:07: Train Epoch 69: 0/23 Loss: 5.844808\n",
      "2025-03-08 00:07: Train Epoch 69: 20/23 Loss: 998.495850\n",
      "2025-03-08 00:07: Train Epoch 69: 20/23 Loss: 998.495850\n",
      "2025-03-08 00:07: Train Epoch 69: 20/23 Loss: 998.495850\n",
      "2025-03-08 00:07: **********Train Epoch 69: averaged Loss: 228.715538\n",
      "2025-03-08 00:07: **********Train Epoch 69: averaged Loss: 228.715538\n",
      "2025-03-08 00:07: **********Train Epoch 69: averaged Loss: 228.715538\n",
      "2025-03-08 00:07: **********Val Epoch 69: average Loss: 3491.520752\n",
      "2025-03-08 00:07: **********Val Epoch 69: average Loss: 3491.520752\n",
      "2025-03-08 00:07: **********Val Epoch 69: average Loss: 3491.520752\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 70: 0/23 Loss: 1.488482\n",
      "2025-03-08 00:07: Train Epoch 70: 0/23 Loss: 1.488482\n",
      "2025-03-08 00:07: Train Epoch 70: 0/23 Loss: 1.488482\n",
      "2025-03-08 00:07: Train Epoch 70: 20/23 Loss: 997.951660\n",
      "2025-03-08 00:07: Train Epoch 70: 20/23 Loss: 997.951660\n",
      "2025-03-08 00:07: Train Epoch 70: 20/23 Loss: 997.951660\n",
      "2025-03-08 00:07: **********Train Epoch 70: averaged Loss: 228.259083\n",
      "2025-03-08 00:07: **********Train Epoch 70: averaged Loss: 228.259083\n",
      "2025-03-08 00:07: **********Train Epoch 70: averaged Loss: 228.259083\n",
      "2025-03-08 00:07: **********Val Epoch 70: average Loss: 3489.828613\n",
      "2025-03-08 00:07: **********Val Epoch 70: average Loss: 3489.828613\n",
      "2025-03-08 00:07: **********Val Epoch 70: average Loss: 3489.828613\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 71: 0/23 Loss: 6.605588\n",
      "2025-03-08 00:07: Train Epoch 71: 0/23 Loss: 6.605588\n",
      "2025-03-08 00:07: Train Epoch 71: 0/23 Loss: 6.605588\n",
      "2025-03-08 00:07: Train Epoch 71: 20/23 Loss: 993.347229\n",
      "2025-03-08 00:07: Train Epoch 71: 20/23 Loss: 993.347229\n",
      "2025-03-08 00:07: Train Epoch 71: 20/23 Loss: 993.347229\n",
      "2025-03-08 00:07: **********Train Epoch 71: averaged Loss: 228.254164\n",
      "2025-03-08 00:07: **********Train Epoch 71: averaged Loss: 228.254164\n",
      "2025-03-08 00:07: **********Train Epoch 71: averaged Loss: 228.254164\n",
      "2025-03-08 00:07: **********Val Epoch 71: average Loss: 3488.942139\n",
      "2025-03-08 00:07: **********Val Epoch 71: average Loss: 3488.942139\n",
      "2025-03-08 00:07: **********Val Epoch 71: average Loss: 3488.942139\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 72: 0/23 Loss: 0.733775\n",
      "2025-03-08 00:07: Train Epoch 72: 0/23 Loss: 0.733775\n",
      "2025-03-08 00:07: Train Epoch 72: 0/23 Loss: 0.733775\n",
      "2025-03-08 00:07: Train Epoch 72: 20/23 Loss: 991.943359\n",
      "2025-03-08 00:07: Train Epoch 72: 20/23 Loss: 991.943359\n",
      "2025-03-08 00:07: Train Epoch 72: 20/23 Loss: 991.943359\n",
      "2025-03-08 00:07: **********Train Epoch 72: averaged Loss: 226.987999\n",
      "2025-03-08 00:07: **********Train Epoch 72: averaged Loss: 226.987999\n",
      "2025-03-08 00:07: **********Train Epoch 72: averaged Loss: 226.987999\n",
      "2025-03-08 00:07: **********Val Epoch 72: average Loss: 3486.037231\n",
      "2025-03-08 00:07: **********Val Epoch 72: average Loss: 3486.037231\n",
      "2025-03-08 00:07: **********Val Epoch 72: average Loss: 3486.037231\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 73: 0/23 Loss: 7.000319\n",
      "2025-03-08 00:07: Train Epoch 73: 0/23 Loss: 7.000319\n",
      "2025-03-08 00:07: Train Epoch 73: 0/23 Loss: 7.000319\n",
      "2025-03-08 00:07: Train Epoch 73: 20/23 Loss: 986.831482\n",
      "2025-03-08 00:07: Train Epoch 73: 20/23 Loss: 986.831482\n",
      "2025-03-08 00:07: Train Epoch 73: 20/23 Loss: 986.831482\n",
      "2025-03-08 00:07: **********Train Epoch 73: averaged Loss: 227.340586\n",
      "2025-03-08 00:07: **********Train Epoch 73: averaged Loss: 227.340586\n",
      "2025-03-08 00:07: **********Train Epoch 73: averaged Loss: 227.340586\n",
      "2025-03-08 00:07: **********Val Epoch 73: average Loss: 3482.360352\n",
      "2025-03-08 00:07: **********Val Epoch 73: average Loss: 3482.360352\n",
      "2025-03-08 00:07: **********Val Epoch 73: average Loss: 3482.360352\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 74: 0/23 Loss: 1.617297\n",
      "2025-03-08 00:07: Train Epoch 74: 0/23 Loss: 1.617297\n",
      "2025-03-08 00:07: Train Epoch 74: 0/23 Loss: 1.617297\n",
      "2025-03-08 00:07: Train Epoch 74: 20/23 Loss: 984.322693\n",
      "2025-03-08 00:07: Train Epoch 74: 20/23 Loss: 984.322693\n",
      "2025-03-08 00:07: Train Epoch 74: 20/23 Loss: 984.322693\n",
      "2025-03-08 00:07: **********Train Epoch 74: averaged Loss: 225.109983\n",
      "2025-03-08 00:07: **********Train Epoch 74: averaged Loss: 225.109983\n",
      "2025-03-08 00:07: **********Train Epoch 74: averaged Loss: 225.109983\n",
      "2025-03-08 00:07: **********Val Epoch 74: average Loss: 3477.926514\n",
      "2025-03-08 00:07: **********Val Epoch 74: average Loss: 3477.926514\n",
      "2025-03-08 00:07: **********Val Epoch 74: average Loss: 3477.926514\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 75: 0/23 Loss: 5.983711\n",
      "2025-03-08 00:07: Train Epoch 75: 0/23 Loss: 5.983711\n",
      "2025-03-08 00:07: Train Epoch 75: 0/23 Loss: 5.983711\n",
      "2025-03-08 00:07: Train Epoch 75: 20/23 Loss: 981.528198\n",
      "2025-03-08 00:07: Train Epoch 75: 20/23 Loss: 981.528198\n",
      "2025-03-08 00:07: Train Epoch 75: 20/23 Loss: 981.528198\n",
      "2025-03-08 00:07: **********Train Epoch 75: averaged Loss: 225.111206\n",
      "2025-03-08 00:07: **********Train Epoch 75: averaged Loss: 225.111206\n",
      "2025-03-08 00:07: **********Train Epoch 75: averaged Loss: 225.111206\n",
      "2025-03-08 00:07: **********Val Epoch 75: average Loss: 3476.569580\n",
      "2025-03-08 00:07: **********Val Epoch 75: average Loss: 3476.569580\n",
      "2025-03-08 00:07: **********Val Epoch 75: average Loss: 3476.569580\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: *********************************Current best model saved!\n",
      "2025-03-08 00:07: Train Epoch 76: 0/23 Loss: 0.769998\n",
      "2025-03-08 00:07: Train Epoch 76: 0/23 Loss: 0.769998\n",
      "2025-03-08 00:07: Train Epoch 76: 0/23 Loss: 0.769998\n",
      "2025-03-08 00:07: Train Epoch 76: 20/23 Loss: 982.470581\n",
      "2025-03-08 00:07: Train Epoch 76: 20/23 Loss: 982.470581\n",
      "2025-03-08 00:07: Train Epoch 76: 20/23 Loss: 982.470581\n",
      "2025-03-08 00:07: **********Train Epoch 76: averaged Loss: 224.600130\n",
      "2025-03-08 00:07: **********Train Epoch 76: averaged Loss: 224.600130\n",
      "2025-03-08 00:07: **********Train Epoch 76: averaged Loss: 224.600130\n",
      "2025-03-08 00:07: **********Val Epoch 76: average Loss: 3478.204468\n",
      "2025-03-08 00:07: **********Val Epoch 76: average Loss: 3478.204468\n",
      "2025-03-08 00:07: **********Val Epoch 76: average Loss: 3478.204468\n",
      "2025-03-08 00:07: Train Epoch 77: 0/23 Loss: 6.241838\n",
      "2025-03-08 00:07: Train Epoch 77: 0/23 Loss: 6.241838\n",
      "2025-03-08 00:07: Train Epoch 77: 0/23 Loss: 6.241838\n",
      "2025-03-08 00:08: Train Epoch 77: 20/23 Loss: 981.817139\n",
      "2025-03-08 00:08: Train Epoch 77: 20/23 Loss: 981.817139\n",
      "2025-03-08 00:08: Train Epoch 77: 20/23 Loss: 981.817139\n",
      "2025-03-08 00:08: **********Train Epoch 77: averaged Loss: 224.142229\n",
      "2025-03-08 00:08: **********Train Epoch 77: averaged Loss: 224.142229\n",
      "2025-03-08 00:08: **********Train Epoch 77: averaged Loss: 224.142229\n",
      "2025-03-08 00:08: **********Val Epoch 77: average Loss: 3474.265869\n",
      "2025-03-08 00:08: **********Val Epoch 77: average Loss: 3474.265869\n",
      "2025-03-08 00:08: **********Val Epoch 77: average Loss: 3474.265869\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 78: 0/23 Loss: 0.484772\n",
      "2025-03-08 00:08: Train Epoch 78: 0/23 Loss: 0.484772\n",
      "2025-03-08 00:08: Train Epoch 78: 0/23 Loss: 0.484772\n",
      "2025-03-08 00:08: Train Epoch 78: 20/23 Loss: 973.230286\n",
      "2025-03-08 00:08: Train Epoch 78: 20/23 Loss: 973.230286\n",
      "2025-03-08 00:08: Train Epoch 78: 20/23 Loss: 973.230286\n",
      "2025-03-08 00:08: **********Train Epoch 78: averaged Loss: 223.070914\n",
      "2025-03-08 00:08: **********Train Epoch 78: averaged Loss: 223.070914\n",
      "2025-03-08 00:08: **********Train Epoch 78: averaged Loss: 223.070914\n",
      "2025-03-08 00:08: **********Val Epoch 78: average Loss: 3471.786743\n",
      "2025-03-08 00:08: **********Val Epoch 78: average Loss: 3471.786743\n",
      "2025-03-08 00:08: **********Val Epoch 78: average Loss: 3471.786743\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 79: 0/23 Loss: 6.412423\n",
      "2025-03-08 00:08: Train Epoch 79: 0/23 Loss: 6.412423\n",
      "2025-03-08 00:08: Train Epoch 79: 0/23 Loss: 6.412423\n",
      "2025-03-08 00:08: Train Epoch 79: 20/23 Loss: 972.137573\n",
      "2025-03-08 00:08: Train Epoch 79: 20/23 Loss: 972.137573\n",
      "2025-03-08 00:08: Train Epoch 79: 20/23 Loss: 972.137573\n",
      "2025-03-08 00:08: **********Train Epoch 79: averaged Loss: 223.038736\n",
      "2025-03-08 00:08: **********Train Epoch 79: averaged Loss: 223.038736\n",
      "2025-03-08 00:08: **********Train Epoch 79: averaged Loss: 223.038736\n",
      "2025-03-08 00:08: **********Val Epoch 79: average Loss: 3470.050049\n",
      "2025-03-08 00:08: **********Val Epoch 79: average Loss: 3470.050049\n",
      "2025-03-08 00:08: **********Val Epoch 79: average Loss: 3470.050049\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 80: 0/23 Loss: 0.480096\n",
      "2025-03-08 00:08: Train Epoch 80: 0/23 Loss: 0.480096\n",
      "2025-03-08 00:08: Train Epoch 80: 0/23 Loss: 0.480096\n",
      "2025-03-08 00:08: Train Epoch 80: 20/23 Loss: 968.259338\n",
      "2025-03-08 00:08: Train Epoch 80: 20/23 Loss: 968.259338\n",
      "2025-03-08 00:08: Train Epoch 80: 20/23 Loss: 968.259338\n",
      "2025-03-08 00:08: **********Train Epoch 80: averaged Loss: 222.159768\n",
      "2025-03-08 00:08: **********Train Epoch 80: averaged Loss: 222.159768\n",
      "2025-03-08 00:08: **********Train Epoch 80: averaged Loss: 222.159768\n",
      "2025-03-08 00:08: **********Val Epoch 80: average Loss: 3463.259521\n",
      "2025-03-08 00:08: **********Val Epoch 80: average Loss: 3463.259521\n",
      "2025-03-08 00:08: **********Val Epoch 80: average Loss: 3463.259521\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 81: 0/23 Loss: 5.163934\n",
      "2025-03-08 00:08: Train Epoch 81: 0/23 Loss: 5.163934\n",
      "2025-03-08 00:08: Train Epoch 81: 0/23 Loss: 5.163934\n",
      "2025-03-08 00:08: Train Epoch 81: 20/23 Loss: 965.811707\n",
      "2025-03-08 00:08: Train Epoch 81: 20/23 Loss: 965.811707\n",
      "2025-03-08 00:08: Train Epoch 81: 20/23 Loss: 965.811707\n",
      "2025-03-08 00:08: **********Train Epoch 81: averaged Loss: 220.963174\n",
      "2025-03-08 00:08: **********Train Epoch 81: averaged Loss: 220.963174\n",
      "2025-03-08 00:08: **********Train Epoch 81: averaged Loss: 220.963174\n",
      "2025-03-08 00:08: **********Val Epoch 81: average Loss: 3458.544922\n",
      "2025-03-08 00:08: **********Val Epoch 81: average Loss: 3458.544922\n",
      "2025-03-08 00:08: **********Val Epoch 81: average Loss: 3458.544922\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 82: 0/23 Loss: 0.509265\n",
      "2025-03-08 00:08: Train Epoch 82: 0/23 Loss: 0.509265\n",
      "2025-03-08 00:08: Train Epoch 82: 0/23 Loss: 0.509265\n",
      "2025-03-08 00:08: Train Epoch 82: 20/23 Loss: 961.564880\n",
      "2025-03-08 00:08: Train Epoch 82: 20/23 Loss: 961.564880\n",
      "2025-03-08 00:08: Train Epoch 82: 20/23 Loss: 961.564880\n",
      "2025-03-08 00:08: **********Train Epoch 82: averaged Loss: 219.884559\n",
      "2025-03-08 00:08: **********Train Epoch 82: averaged Loss: 219.884559\n",
      "2025-03-08 00:08: **********Train Epoch 82: averaged Loss: 219.884559\n",
      "2025-03-08 00:08: **********Val Epoch 82: average Loss: 3457.840576\n",
      "2025-03-08 00:08: **********Val Epoch 82: average Loss: 3457.840576\n",
      "2025-03-08 00:08: **********Val Epoch 82: average Loss: 3457.840576\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 83: 0/23 Loss: 5.509648\n",
      "2025-03-08 00:08: Train Epoch 83: 0/23 Loss: 5.509648\n",
      "2025-03-08 00:08: Train Epoch 83: 0/23 Loss: 5.509648\n",
      "2025-03-08 00:08: Train Epoch 83: 20/23 Loss: 959.025391\n",
      "2025-03-08 00:08: Train Epoch 83: 20/23 Loss: 959.025391\n",
      "2025-03-08 00:08: Train Epoch 83: 20/23 Loss: 959.025391\n",
      "2025-03-08 00:08: **********Train Epoch 83: averaged Loss: 219.026552\n",
      "2025-03-08 00:08: **********Train Epoch 83: averaged Loss: 219.026552\n",
      "2025-03-08 00:08: **********Train Epoch 83: averaged Loss: 219.026552\n",
      "2025-03-08 00:08: **********Val Epoch 83: average Loss: 3457.683350\n",
      "2025-03-08 00:08: **********Val Epoch 83: average Loss: 3457.683350\n",
      "2025-03-08 00:08: **********Val Epoch 83: average Loss: 3457.683350\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 84: 0/23 Loss: 3.000278\n",
      "2025-03-08 00:08: Train Epoch 84: 0/23 Loss: 3.000278\n",
      "2025-03-08 00:08: Train Epoch 84: 0/23 Loss: 3.000278\n",
      "2025-03-08 00:08: Train Epoch 84: 20/23 Loss: 958.214661\n",
      "2025-03-08 00:08: Train Epoch 84: 20/23 Loss: 958.214661\n",
      "2025-03-08 00:08: Train Epoch 84: 20/23 Loss: 958.214661\n",
      "2025-03-08 00:08: **********Train Epoch 84: averaged Loss: 219.803140\n",
      "2025-03-08 00:08: **********Train Epoch 84: averaged Loss: 219.803140\n",
      "2025-03-08 00:08: **********Train Epoch 84: averaged Loss: 219.803140\n",
      "2025-03-08 00:08: **********Val Epoch 84: average Loss: 3450.679932\n",
      "2025-03-08 00:08: **********Val Epoch 84: average Loss: 3450.679932\n",
      "2025-03-08 00:08: **********Val Epoch 84: average Loss: 3450.679932\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 85: 0/23 Loss: 5.670869\n",
      "2025-03-08 00:08: Train Epoch 85: 0/23 Loss: 5.670869\n",
      "2025-03-08 00:08: Train Epoch 85: 0/23 Loss: 5.670869\n",
      "2025-03-08 00:08: Train Epoch 85: 20/23 Loss: 952.539612\n",
      "2025-03-08 00:08: Train Epoch 85: 20/23 Loss: 952.539612\n",
      "2025-03-08 00:08: Train Epoch 85: 20/23 Loss: 952.539612\n",
      "2025-03-08 00:08: **********Train Epoch 85: averaged Loss: 218.872390\n",
      "2025-03-08 00:08: **********Train Epoch 85: averaged Loss: 218.872390\n",
      "2025-03-08 00:08: **********Train Epoch 85: averaged Loss: 218.872390\n",
      "2025-03-08 00:08: **********Val Epoch 85: average Loss: 3448.726562\n",
      "2025-03-08 00:08: **********Val Epoch 85: average Loss: 3448.726562\n",
      "2025-03-08 00:08: **********Val Epoch 85: average Loss: 3448.726562\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 86: 0/23 Loss: 0.484999\n",
      "2025-03-08 00:08: Train Epoch 86: 0/23 Loss: 0.484999\n",
      "2025-03-08 00:08: Train Epoch 86: 0/23 Loss: 0.484999\n",
      "2025-03-08 00:08: Train Epoch 86: 20/23 Loss: 942.827637\n",
      "2025-03-08 00:08: Train Epoch 86: 20/23 Loss: 942.827637\n",
      "2025-03-08 00:08: Train Epoch 86: 20/23 Loss: 942.827637\n",
      "2025-03-08 00:08: **********Train Epoch 86: averaged Loss: 218.364919\n",
      "2025-03-08 00:08: **********Train Epoch 86: averaged Loss: 218.364919\n",
      "2025-03-08 00:08: **********Train Epoch 86: averaged Loss: 218.364919\n",
      "2025-03-08 00:08: **********Val Epoch 86: average Loss: 3441.774048\n",
      "2025-03-08 00:08: **********Val Epoch 86: average Loss: 3441.774048\n",
      "2025-03-08 00:08: **********Val Epoch 86: average Loss: 3441.774048\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 87: 0/23 Loss: 5.205170\n",
      "2025-03-08 00:08: Train Epoch 87: 0/23 Loss: 5.205170\n",
      "2025-03-08 00:08: Train Epoch 87: 0/23 Loss: 5.205170\n",
      "2025-03-08 00:08: Train Epoch 87: 20/23 Loss: 946.394653\n",
      "2025-03-08 00:08: Train Epoch 87: 20/23 Loss: 946.394653\n",
      "2025-03-08 00:08: Train Epoch 87: 20/23 Loss: 946.394653\n",
      "2025-03-08 00:08: **********Train Epoch 87: averaged Loss: 216.371733\n",
      "2025-03-08 00:08: **********Train Epoch 87: averaged Loss: 216.371733\n",
      "2025-03-08 00:08: **********Train Epoch 87: averaged Loss: 216.371733\n",
      "2025-03-08 00:08: **********Val Epoch 87: average Loss: 3441.995361\n",
      "2025-03-08 00:08: **********Val Epoch 87: average Loss: 3441.995361\n",
      "2025-03-08 00:08: **********Val Epoch 87: average Loss: 3441.995361\n",
      "2025-03-08 00:08: Train Epoch 88: 0/23 Loss: 0.519231\n",
      "2025-03-08 00:08: Train Epoch 88: 0/23 Loss: 0.519231\n",
      "2025-03-08 00:08: Train Epoch 88: 0/23 Loss: 0.519231\n",
      "2025-03-08 00:08: Train Epoch 88: 20/23 Loss: 943.772888\n",
      "2025-03-08 00:08: Train Epoch 88: 20/23 Loss: 943.772888\n",
      "2025-03-08 00:08: Train Epoch 88: 20/23 Loss: 943.772888\n",
      "2025-03-08 00:08: **********Train Epoch 88: averaged Loss: 216.583560\n",
      "2025-03-08 00:08: **********Train Epoch 88: averaged Loss: 216.583560\n",
      "2025-03-08 00:08: **********Train Epoch 88: averaged Loss: 216.583560\n",
      "2025-03-08 00:08: **********Val Epoch 88: average Loss: 3439.127075\n",
      "2025-03-08 00:08: **********Val Epoch 88: average Loss: 3439.127075\n",
      "2025-03-08 00:08: **********Val Epoch 88: average Loss: 3439.127075\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 89: 0/23 Loss: 5.933022\n",
      "2025-03-08 00:08: Train Epoch 89: 0/23 Loss: 5.933022\n",
      "2025-03-08 00:08: Train Epoch 89: 0/23 Loss: 5.933022\n",
      "2025-03-08 00:08: Train Epoch 89: 20/23 Loss: 939.745728\n",
      "2025-03-08 00:08: Train Epoch 89: 20/23 Loss: 939.745728\n",
      "2025-03-08 00:08: Train Epoch 89: 20/23 Loss: 939.745728\n",
      "2025-03-08 00:08: **********Train Epoch 89: averaged Loss: 216.001321\n",
      "2025-03-08 00:08: **********Train Epoch 89: averaged Loss: 216.001321\n",
      "2025-03-08 00:08: **********Train Epoch 89: averaged Loss: 216.001321\n",
      "2025-03-08 00:08: **********Val Epoch 89: average Loss: 3431.935791\n",
      "2025-03-08 00:08: **********Val Epoch 89: average Loss: 3431.935791\n",
      "2025-03-08 00:08: **********Val Epoch 89: average Loss: 3431.935791\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 90: 0/23 Loss: 0.565615\n",
      "2025-03-08 00:08: Train Epoch 90: 0/23 Loss: 0.565615\n",
      "2025-03-08 00:08: Train Epoch 90: 0/23 Loss: 0.565615\n",
      "2025-03-08 00:08: Train Epoch 90: 20/23 Loss: 933.919312\n",
      "2025-03-08 00:08: Train Epoch 90: 20/23 Loss: 933.919312\n",
      "2025-03-08 00:08: Train Epoch 90: 20/23 Loss: 933.919312\n",
      "2025-03-08 00:08: **********Train Epoch 90: averaged Loss: 214.074249\n",
      "2025-03-08 00:08: **********Train Epoch 90: averaged Loss: 214.074249\n",
      "2025-03-08 00:08: **********Train Epoch 90: averaged Loss: 214.074249\n",
      "2025-03-08 00:08: **********Val Epoch 90: average Loss: 3432.432007\n",
      "2025-03-08 00:08: **********Val Epoch 90: average Loss: 3432.432007\n",
      "2025-03-08 00:08: **********Val Epoch 90: average Loss: 3432.432007\n",
      "2025-03-08 00:08: Train Epoch 91: 0/23 Loss: 4.631896\n",
      "2025-03-08 00:08: Train Epoch 91: 0/23 Loss: 4.631896\n",
      "2025-03-08 00:08: Train Epoch 91: 0/23 Loss: 4.631896\n",
      "2025-03-08 00:08: Train Epoch 91: 20/23 Loss: 934.544800\n",
      "2025-03-08 00:08: Train Epoch 91: 20/23 Loss: 934.544800\n",
      "2025-03-08 00:08: Train Epoch 91: 20/23 Loss: 934.544800\n",
      "2025-03-08 00:08: **********Train Epoch 91: averaged Loss: 212.383902\n",
      "2025-03-08 00:08: **********Train Epoch 91: averaged Loss: 212.383902\n",
      "2025-03-08 00:08: **********Train Epoch 91: averaged Loss: 212.383902\n",
      "2025-03-08 00:08: **********Val Epoch 91: average Loss: 3427.036865\n",
      "2025-03-08 00:08: **********Val Epoch 91: average Loss: 3427.036865\n",
      "2025-03-08 00:08: **********Val Epoch 91: average Loss: 3427.036865\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 92: 0/23 Loss: 0.611722\n",
      "2025-03-08 00:08: Train Epoch 92: 0/23 Loss: 0.611722\n",
      "2025-03-08 00:08: Train Epoch 92: 0/23 Loss: 0.611722\n",
      "2025-03-08 00:08: Train Epoch 92: 20/23 Loss: 929.938293\n",
      "2025-03-08 00:08: Train Epoch 92: 20/23 Loss: 929.938293\n",
      "2025-03-08 00:08: Train Epoch 92: 20/23 Loss: 929.938293\n",
      "2025-03-08 00:08: **********Train Epoch 92: averaged Loss: 211.501145\n",
      "2025-03-08 00:08: **********Train Epoch 92: averaged Loss: 211.501145\n",
      "2025-03-08 00:08: **********Train Epoch 92: averaged Loss: 211.501145\n",
      "2025-03-08 00:08: **********Val Epoch 92: average Loss: 3423.584717\n",
      "2025-03-08 00:08: **********Val Epoch 92: average Loss: 3423.584717\n",
      "2025-03-08 00:08: **********Val Epoch 92: average Loss: 3423.584717\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 93: 0/23 Loss: 3.357443\n",
      "2025-03-08 00:08: Train Epoch 93: 0/23 Loss: 3.357443\n",
      "2025-03-08 00:08: Train Epoch 93: 0/23 Loss: 3.357443\n",
      "2025-03-08 00:08: Train Epoch 93: 20/23 Loss: 919.575684\n",
      "2025-03-08 00:08: Train Epoch 93: 20/23 Loss: 919.575684\n",
      "2025-03-08 00:08: Train Epoch 93: 20/23 Loss: 919.575684\n",
      "2025-03-08 00:08: **********Train Epoch 93: averaged Loss: 211.254354\n",
      "2025-03-08 00:08: **********Train Epoch 93: averaged Loss: 211.254354\n",
      "2025-03-08 00:08: **********Train Epoch 93: averaged Loss: 211.254354\n",
      "2025-03-08 00:08: **********Val Epoch 93: average Loss: 3417.345581\n",
      "2025-03-08 00:08: **********Val Epoch 93: average Loss: 3417.345581\n",
      "2025-03-08 00:08: **********Val Epoch 93: average Loss: 3417.345581\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 94: 0/23 Loss: 1.922003\n",
      "2025-03-08 00:08: Train Epoch 94: 0/23 Loss: 1.922003\n",
      "2025-03-08 00:08: Train Epoch 94: 0/23 Loss: 1.922003\n",
      "2025-03-08 00:08: Train Epoch 94: 20/23 Loss: 917.196716\n",
      "2025-03-08 00:08: Train Epoch 94: 20/23 Loss: 917.196716\n",
      "2025-03-08 00:08: Train Epoch 94: 20/23 Loss: 917.196716\n",
      "2025-03-08 00:08: **********Train Epoch 94: averaged Loss: 211.726836\n",
      "2025-03-08 00:08: **********Train Epoch 94: averaged Loss: 211.726836\n",
      "2025-03-08 00:08: **********Train Epoch 94: averaged Loss: 211.726836\n",
      "2025-03-08 00:08: **********Val Epoch 94: average Loss: 3415.155029\n",
      "2025-03-08 00:08: **********Val Epoch 94: average Loss: 3415.155029\n",
      "2025-03-08 00:08: **********Val Epoch 94: average Loss: 3415.155029\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 95: 0/23 Loss: 5.320559\n",
      "2025-03-08 00:08: Train Epoch 95: 0/23 Loss: 5.320559\n",
      "2025-03-08 00:08: Train Epoch 95: 0/23 Loss: 5.320559\n",
      "2025-03-08 00:08: Train Epoch 95: 20/23 Loss: 915.974915\n",
      "2025-03-08 00:08: Train Epoch 95: 20/23 Loss: 915.974915\n",
      "2025-03-08 00:08: Train Epoch 95: 20/23 Loss: 915.974915\n",
      "2025-03-08 00:08: **********Train Epoch 95: averaged Loss: 211.168992\n",
      "2025-03-08 00:08: **********Train Epoch 95: averaged Loss: 211.168992\n",
      "2025-03-08 00:08: **********Train Epoch 95: averaged Loss: 211.168992\n",
      "2025-03-08 00:08: **********Val Epoch 95: average Loss: 3411.490356\n",
      "2025-03-08 00:08: **********Val Epoch 95: average Loss: 3411.490356\n",
      "2025-03-08 00:08: **********Val Epoch 95: average Loss: 3411.490356\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 96: 0/23 Loss: 1.540304\n",
      "2025-03-08 00:08: Train Epoch 96: 0/23 Loss: 1.540304\n",
      "2025-03-08 00:08: Train Epoch 96: 0/23 Loss: 1.540304\n",
      "2025-03-08 00:08: Train Epoch 96: 20/23 Loss: 908.998535\n",
      "2025-03-08 00:08: Train Epoch 96: 20/23 Loss: 908.998535\n",
      "2025-03-08 00:08: Train Epoch 96: 20/23 Loss: 908.998535\n",
      "2025-03-08 00:08: **********Train Epoch 96: averaged Loss: 208.744160\n",
      "2025-03-08 00:08: **********Train Epoch 96: averaged Loss: 208.744160\n",
      "2025-03-08 00:08: **********Train Epoch 96: averaged Loss: 208.744160\n",
      "2025-03-08 00:08: **********Val Epoch 96: average Loss: 3407.090210\n",
      "2025-03-08 00:08: **********Val Epoch 96: average Loss: 3407.090210\n",
      "2025-03-08 00:08: **********Val Epoch 96: average Loss: 3407.090210\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 97: 0/23 Loss: 6.182536\n",
      "2025-03-08 00:08: Train Epoch 97: 0/23 Loss: 6.182536\n",
      "2025-03-08 00:08: Train Epoch 97: 0/23 Loss: 6.182536\n",
      "2025-03-08 00:08: Train Epoch 97: 20/23 Loss: 906.714722\n",
      "2025-03-08 00:08: Train Epoch 97: 20/23 Loss: 906.714722\n",
      "2025-03-08 00:08: Train Epoch 97: 20/23 Loss: 906.714722\n",
      "2025-03-08 00:08: **********Train Epoch 97: averaged Loss: 208.932409\n",
      "2025-03-08 00:08: **********Train Epoch 97: averaged Loss: 208.932409\n",
      "2025-03-08 00:08: **********Train Epoch 97: averaged Loss: 208.932409\n",
      "2025-03-08 00:08: **********Val Epoch 97: average Loss: 3405.958984\n",
      "2025-03-08 00:08: **********Val Epoch 97: average Loss: 3405.958984\n",
      "2025-03-08 00:08: **********Val Epoch 97: average Loss: 3405.958984\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 98: 0/23 Loss: 0.519349\n",
      "2025-03-08 00:08: Train Epoch 98: 0/23 Loss: 0.519349\n",
      "2025-03-08 00:08: Train Epoch 98: 0/23 Loss: 0.519349\n",
      "2025-03-08 00:08: Train Epoch 98: 20/23 Loss: 902.238159\n",
      "2025-03-08 00:08: Train Epoch 98: 20/23 Loss: 902.238159\n",
      "2025-03-08 00:08: Train Epoch 98: 20/23 Loss: 902.238159\n",
      "2025-03-08 00:08: **********Train Epoch 98: averaged Loss: 206.282914\n",
      "2025-03-08 00:08: **********Train Epoch 98: averaged Loss: 206.282914\n",
      "2025-03-08 00:08: **********Train Epoch 98: averaged Loss: 206.282914\n",
      "2025-03-08 00:08: **********Val Epoch 98: average Loss: 3401.768433\n",
      "2025-03-08 00:08: **********Val Epoch 98: average Loss: 3401.768433\n",
      "2025-03-08 00:08: **********Val Epoch 98: average Loss: 3401.768433\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 99: 0/23 Loss: 5.381313\n",
      "2025-03-08 00:08: Train Epoch 99: 0/23 Loss: 5.381313\n",
      "2025-03-08 00:08: Train Epoch 99: 0/23 Loss: 5.381313\n",
      "2025-03-08 00:08: Train Epoch 99: 20/23 Loss: 899.382996\n",
      "2025-03-08 00:08: Train Epoch 99: 20/23 Loss: 899.382996\n",
      "2025-03-08 00:08: Train Epoch 99: 20/23 Loss: 899.382996\n",
      "2025-03-08 00:08: **********Train Epoch 99: averaged Loss: 206.223503\n",
      "2025-03-08 00:08: **********Train Epoch 99: averaged Loss: 206.223503\n",
      "2025-03-08 00:08: **********Train Epoch 99: averaged Loss: 206.223503\n",
      "2025-03-08 00:08: **********Val Epoch 99: average Loss: 3399.875854\n",
      "2025-03-08 00:08: **********Val Epoch 99: average Loss: 3399.875854\n",
      "2025-03-08 00:08: **********Val Epoch 99: average Loss: 3399.875854\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 100: 0/23 Loss: 2.864614\n",
      "2025-03-08 00:08: Train Epoch 100: 0/23 Loss: 2.864614\n",
      "2025-03-08 00:08: Train Epoch 100: 0/23 Loss: 2.864614\n",
      "2025-03-08 00:08: Train Epoch 100: 20/23 Loss: 896.384277\n",
      "2025-03-08 00:08: Train Epoch 100: 20/23 Loss: 896.384277\n",
      "2025-03-08 00:08: Train Epoch 100: 20/23 Loss: 896.384277\n",
      "2025-03-08 00:08: **********Train Epoch 100: averaged Loss: 205.353112\n",
      "2025-03-08 00:08: **********Train Epoch 100: averaged Loss: 205.353112\n",
      "2025-03-08 00:08: **********Train Epoch 100: averaged Loss: 205.353112\n",
      "2025-03-08 00:08: **********Val Epoch 100: average Loss: 3389.592651\n",
      "2025-03-08 00:08: **********Val Epoch 100: average Loss: 3389.592651\n",
      "2025-03-08 00:08: **********Val Epoch 100: average Loss: 3389.592651\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 101: 0/23 Loss: 5.456435\n",
      "2025-03-08 00:08: Train Epoch 101: 0/23 Loss: 5.456435\n",
      "2025-03-08 00:08: Train Epoch 101: 0/23 Loss: 5.456435\n",
      "2025-03-08 00:08: Train Epoch 101: 20/23 Loss: 888.386841\n",
      "2025-03-08 00:08: Train Epoch 101: 20/23 Loss: 888.386841\n",
      "2025-03-08 00:08: Train Epoch 101: 20/23 Loss: 888.386841\n",
      "2025-03-08 00:08: **********Train Epoch 101: averaged Loss: 204.852966\n",
      "2025-03-08 00:08: **********Train Epoch 101: averaged Loss: 204.852966\n",
      "2025-03-08 00:08: **********Train Epoch 101: averaged Loss: 204.852966\n",
      "2025-03-08 00:08: **********Val Epoch 101: average Loss: 3389.563965\n",
      "2025-03-08 00:08: **********Val Epoch 101: average Loss: 3389.563965\n",
      "2025-03-08 00:08: **********Val Epoch 101: average Loss: 3389.563965\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 102: 0/23 Loss: 2.459146\n",
      "2025-03-08 00:08: Train Epoch 102: 0/23 Loss: 2.459146\n",
      "2025-03-08 00:08: Train Epoch 102: 0/23 Loss: 2.459146\n",
      "2025-03-08 00:08: Train Epoch 102: 20/23 Loss: 893.889404\n",
      "2025-03-08 00:08: Train Epoch 102: 20/23 Loss: 893.889404\n",
      "2025-03-08 00:08: Train Epoch 102: 20/23 Loss: 893.889404\n",
      "2025-03-08 00:08: **********Train Epoch 102: averaged Loss: 204.306739\n",
      "2025-03-08 00:08: **********Train Epoch 102: averaged Loss: 204.306739\n",
      "2025-03-08 00:08: **********Train Epoch 102: averaged Loss: 204.306739\n",
      "2025-03-08 00:08: **********Val Epoch 102: average Loss: 3378.720215\n",
      "2025-03-08 00:08: **********Val Epoch 102: average Loss: 3378.720215\n",
      "2025-03-08 00:08: **********Val Epoch 102: average Loss: 3378.720215\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 103: 0/23 Loss: 5.678082\n",
      "2025-03-08 00:08: Train Epoch 103: 0/23 Loss: 5.678082\n",
      "2025-03-08 00:08: Train Epoch 103: 0/23 Loss: 5.678082\n",
      "2025-03-08 00:08: Train Epoch 103: 20/23 Loss: 878.942139\n",
      "2025-03-08 00:08: Train Epoch 103: 20/23 Loss: 878.942139\n",
      "2025-03-08 00:08: Train Epoch 103: 20/23 Loss: 878.942139\n",
      "2025-03-08 00:08: **********Train Epoch 103: averaged Loss: 203.234550\n",
      "2025-03-08 00:08: **********Train Epoch 103: averaged Loss: 203.234550\n",
      "2025-03-08 00:08: **********Train Epoch 103: averaged Loss: 203.234550\n",
      "2025-03-08 00:08: **********Val Epoch 103: average Loss: 3379.233398\n",
      "2025-03-08 00:08: **********Val Epoch 103: average Loss: 3379.233398\n",
      "2025-03-08 00:08: **********Val Epoch 103: average Loss: 3379.233398\n",
      "2025-03-08 00:08: Train Epoch 104: 0/23 Loss: 0.522255\n",
      "2025-03-08 00:08: Train Epoch 104: 0/23 Loss: 0.522255\n",
      "2025-03-08 00:08: Train Epoch 104: 0/23 Loss: 0.522255\n",
      "2025-03-08 00:08: Train Epoch 104: 20/23 Loss: 881.758179\n",
      "2025-03-08 00:08: Train Epoch 104: 20/23 Loss: 881.758179\n",
      "2025-03-08 00:08: Train Epoch 104: 20/23 Loss: 881.758179\n",
      "2025-03-08 00:08: **********Train Epoch 104: averaged Loss: 200.580492\n",
      "2025-03-08 00:08: **********Train Epoch 104: averaged Loss: 200.580492\n",
      "2025-03-08 00:08: **********Train Epoch 104: averaged Loss: 200.580492\n",
      "2025-03-08 00:08: **********Val Epoch 104: average Loss: 3372.238525\n",
      "2025-03-08 00:08: **********Val Epoch 104: average Loss: 3372.238525\n",
      "2025-03-08 00:08: **********Val Epoch 104: average Loss: 3372.238525\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 105: 0/23 Loss: 3.398158\n",
      "2025-03-08 00:08: Train Epoch 105: 0/23 Loss: 3.398158\n",
      "2025-03-08 00:08: Train Epoch 105: 0/23 Loss: 3.398158\n",
      "2025-03-08 00:08: Train Epoch 105: 20/23 Loss: 864.967529\n",
      "2025-03-08 00:08: Train Epoch 105: 20/23 Loss: 864.967529\n",
      "2025-03-08 00:08: Train Epoch 105: 20/23 Loss: 864.967529\n",
      "2025-03-08 00:08: **********Train Epoch 105: averaged Loss: 199.309622\n",
      "2025-03-08 00:08: **********Train Epoch 105: averaged Loss: 199.309622\n",
      "2025-03-08 00:08: **********Train Epoch 105: averaged Loss: 199.309622\n",
      "2025-03-08 00:08: **********Val Epoch 105: average Loss: 3372.438721\n",
      "2025-03-08 00:08: **********Val Epoch 105: average Loss: 3372.438721\n",
      "2025-03-08 00:08: **********Val Epoch 105: average Loss: 3372.438721\n",
      "2025-03-08 00:08: Train Epoch 106: 0/23 Loss: 2.370275\n",
      "2025-03-08 00:08: Train Epoch 106: 0/23 Loss: 2.370275\n",
      "2025-03-08 00:08: Train Epoch 106: 0/23 Loss: 2.370275\n",
      "2025-03-08 00:08: Train Epoch 106: 20/23 Loss: 872.416016\n",
      "2025-03-08 00:08: Train Epoch 106: 20/23 Loss: 872.416016\n",
      "2025-03-08 00:08: Train Epoch 106: 20/23 Loss: 872.416016\n",
      "2025-03-08 00:08: **********Train Epoch 106: averaged Loss: 199.921770\n",
      "2025-03-08 00:08: **********Train Epoch 106: averaged Loss: 199.921770\n",
      "2025-03-08 00:08: **********Train Epoch 106: averaged Loss: 199.921770\n",
      "2025-03-08 00:08: **********Val Epoch 106: average Loss: 3366.863159\n",
      "2025-03-08 00:08: **********Val Epoch 106: average Loss: 3366.863159\n",
      "2025-03-08 00:08: **********Val Epoch 106: average Loss: 3366.863159\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 107: 0/23 Loss: 4.254029\n",
      "2025-03-08 00:08: Train Epoch 107: 0/23 Loss: 4.254029\n",
      "2025-03-08 00:08: Train Epoch 107: 0/23 Loss: 4.254029\n",
      "2025-03-08 00:08: Train Epoch 107: 20/23 Loss: 855.541077\n",
      "2025-03-08 00:08: Train Epoch 107: 20/23 Loss: 855.541077\n",
      "2025-03-08 00:08: Train Epoch 107: 20/23 Loss: 855.541077\n",
      "2025-03-08 00:08: **********Train Epoch 107: averaged Loss: 198.572799\n",
      "2025-03-08 00:08: **********Train Epoch 107: averaged Loss: 198.572799\n",
      "2025-03-08 00:08: **********Train Epoch 107: averaged Loss: 198.572799\n",
      "2025-03-08 00:08: **********Val Epoch 107: average Loss: 3362.248657\n",
      "2025-03-08 00:08: **********Val Epoch 107: average Loss: 3362.248657\n",
      "2025-03-08 00:08: **********Val Epoch 107: average Loss: 3362.248657\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: *********************************Current best model saved!\n",
      "2025-03-08 00:08: Train Epoch 108: 0/23 Loss: 2.486669\n",
      "2025-03-08 00:08: Train Epoch 108: 0/23 Loss: 2.486669\n",
      "2025-03-08 00:08: Train Epoch 108: 0/23 Loss: 2.486669\n",
      "2025-03-08 00:09: Train Epoch 108: 20/23 Loss: 862.273560\n",
      "2025-03-08 00:09: Train Epoch 108: 20/23 Loss: 862.273560\n",
      "2025-03-08 00:09: Train Epoch 108: 20/23 Loss: 862.273560\n",
      "2025-03-08 00:09: **********Train Epoch 108: averaged Loss: 198.758164\n",
      "2025-03-08 00:09: **********Train Epoch 108: averaged Loss: 198.758164\n",
      "2025-03-08 00:09: **********Train Epoch 108: averaged Loss: 198.758164\n",
      "2025-03-08 00:09: **********Val Epoch 108: average Loss: 3362.431152\n",
      "2025-03-08 00:09: **********Val Epoch 108: average Loss: 3362.431152\n",
      "2025-03-08 00:09: **********Val Epoch 108: average Loss: 3362.431152\n",
      "2025-03-08 00:09: Train Epoch 109: 0/23 Loss: 5.469444\n",
      "2025-03-08 00:09: Train Epoch 109: 0/23 Loss: 5.469444\n",
      "2025-03-08 00:09: Train Epoch 109: 0/23 Loss: 5.469444\n",
      "2025-03-08 00:09: Train Epoch 109: 20/23 Loss: 855.557617\n",
      "2025-03-08 00:09: Train Epoch 109: 20/23 Loss: 855.557617\n",
      "2025-03-08 00:09: Train Epoch 109: 20/23 Loss: 855.557617\n",
      "2025-03-08 00:09: **********Train Epoch 109: averaged Loss: 196.130366\n",
      "2025-03-08 00:09: **********Train Epoch 109: averaged Loss: 196.130366\n",
      "2025-03-08 00:09: **********Train Epoch 109: averaged Loss: 196.130366\n",
      "2025-03-08 00:09: **********Val Epoch 109: average Loss: 3355.054443\n",
      "2025-03-08 00:09: **********Val Epoch 109: average Loss: 3355.054443\n",
      "2025-03-08 00:09: **********Val Epoch 109: average Loss: 3355.054443\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 110: 0/23 Loss: 0.865395\n",
      "2025-03-08 00:09: Train Epoch 110: 0/23 Loss: 0.865395\n",
      "2025-03-08 00:09: Train Epoch 110: 0/23 Loss: 0.865395\n",
      "2025-03-08 00:09: Train Epoch 110: 20/23 Loss: 849.702637\n",
      "2025-03-08 00:09: Train Epoch 110: 20/23 Loss: 849.702637\n",
      "2025-03-08 00:09: Train Epoch 110: 20/23 Loss: 849.702637\n",
      "2025-03-08 00:09: **********Train Epoch 110: averaged Loss: 194.011020\n",
      "2025-03-08 00:09: **********Train Epoch 110: averaged Loss: 194.011020\n",
      "2025-03-08 00:09: **********Train Epoch 110: averaged Loss: 194.011020\n",
      "2025-03-08 00:09: **********Val Epoch 110: average Loss: 3345.395996\n",
      "2025-03-08 00:09: **********Val Epoch 110: average Loss: 3345.395996\n",
      "2025-03-08 00:09: **********Val Epoch 110: average Loss: 3345.395996\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 111: 0/23 Loss: 3.685916\n",
      "2025-03-08 00:09: Train Epoch 111: 0/23 Loss: 3.685916\n",
      "2025-03-08 00:09: Train Epoch 111: 0/23 Loss: 3.685916\n",
      "2025-03-08 00:09: Train Epoch 111: 20/23 Loss: 845.456238\n",
      "2025-03-08 00:09: Train Epoch 111: 20/23 Loss: 845.456238\n",
      "2025-03-08 00:09: Train Epoch 111: 20/23 Loss: 845.456238\n",
      "2025-03-08 00:09: **********Train Epoch 111: averaged Loss: 193.982735\n",
      "2025-03-08 00:09: **********Train Epoch 111: averaged Loss: 193.982735\n",
      "2025-03-08 00:09: **********Train Epoch 111: averaged Loss: 193.982735\n",
      "2025-03-08 00:09: **********Val Epoch 111: average Loss: 3341.314575\n",
      "2025-03-08 00:09: **********Val Epoch 111: average Loss: 3341.314575\n",
      "2025-03-08 00:09: **********Val Epoch 111: average Loss: 3341.314575\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 112: 0/23 Loss: 1.882794\n",
      "2025-03-08 00:09: Train Epoch 112: 0/23 Loss: 1.882794\n",
      "2025-03-08 00:09: Train Epoch 112: 0/23 Loss: 1.882794\n",
      "2025-03-08 00:09: Train Epoch 112: 20/23 Loss: 839.753052\n",
      "2025-03-08 00:09: Train Epoch 112: 20/23 Loss: 839.753052\n",
      "2025-03-08 00:09: Train Epoch 112: 20/23 Loss: 839.753052\n",
      "2025-03-08 00:09: **********Train Epoch 112: averaged Loss: 191.476826\n",
      "2025-03-08 00:09: **********Train Epoch 112: averaged Loss: 191.476826\n",
      "2025-03-08 00:09: **********Train Epoch 112: averaged Loss: 191.476826\n",
      "2025-03-08 00:09: **********Val Epoch 112: average Loss: 3334.588257\n",
      "2025-03-08 00:09: **********Val Epoch 112: average Loss: 3334.588257\n",
      "2025-03-08 00:09: **********Val Epoch 112: average Loss: 3334.588257\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 113: 0/23 Loss: 6.745610\n",
      "2025-03-08 00:09: Train Epoch 113: 0/23 Loss: 6.745610\n",
      "2025-03-08 00:09: Train Epoch 113: 0/23 Loss: 6.745610\n",
      "2025-03-08 00:09: Train Epoch 113: 20/23 Loss: 836.321960\n",
      "2025-03-08 00:09: Train Epoch 113: 20/23 Loss: 836.321960\n",
      "2025-03-08 00:09: Train Epoch 113: 20/23 Loss: 836.321960\n",
      "2025-03-08 00:09: **********Train Epoch 113: averaged Loss: 191.597777\n",
      "2025-03-08 00:09: **********Train Epoch 113: averaged Loss: 191.597777\n",
      "2025-03-08 00:09: **********Train Epoch 113: averaged Loss: 191.597777\n",
      "2025-03-08 00:09: **********Val Epoch 113: average Loss: 3327.078979\n",
      "2025-03-08 00:09: **********Val Epoch 113: average Loss: 3327.078979\n",
      "2025-03-08 00:09: **********Val Epoch 113: average Loss: 3327.078979\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 114: 0/23 Loss: 2.431161\n",
      "2025-03-08 00:09: Train Epoch 114: 0/23 Loss: 2.431161\n",
      "2025-03-08 00:09: Train Epoch 114: 0/23 Loss: 2.431161\n",
      "2025-03-08 00:09: Train Epoch 114: 20/23 Loss: 825.121094\n",
      "2025-03-08 00:09: Train Epoch 114: 20/23 Loss: 825.121094\n",
      "2025-03-08 00:09: Train Epoch 114: 20/23 Loss: 825.121094\n",
      "2025-03-08 00:09: **********Train Epoch 114: averaged Loss: 190.744573\n",
      "2025-03-08 00:09: **********Train Epoch 114: averaged Loss: 190.744573\n",
      "2025-03-08 00:09: **********Train Epoch 114: averaged Loss: 190.744573\n",
      "2025-03-08 00:09: **********Val Epoch 114: average Loss: 3333.250977\n",
      "2025-03-08 00:09: **********Val Epoch 114: average Loss: 3333.250977\n",
      "2025-03-08 00:09: **********Val Epoch 114: average Loss: 3333.250977\n",
      "2025-03-08 00:09: Train Epoch 115: 0/23 Loss: 5.785122\n",
      "2025-03-08 00:09: Train Epoch 115: 0/23 Loss: 5.785122\n",
      "2025-03-08 00:09: Train Epoch 115: 0/23 Loss: 5.785122\n",
      "2025-03-08 00:09: Train Epoch 115: 20/23 Loss: 818.682190\n",
      "2025-03-08 00:09: Train Epoch 115: 20/23 Loss: 818.682190\n",
      "2025-03-08 00:09: Train Epoch 115: 20/23 Loss: 818.682190\n",
      "2025-03-08 00:09: **********Train Epoch 115: averaged Loss: 190.279229\n",
      "2025-03-08 00:09: **********Train Epoch 115: averaged Loss: 190.279229\n",
      "2025-03-08 00:09: **********Train Epoch 115: averaged Loss: 190.279229\n",
      "2025-03-08 00:09: **********Val Epoch 115: average Loss: 3319.784912\n",
      "2025-03-08 00:09: **********Val Epoch 115: average Loss: 3319.784912\n",
      "2025-03-08 00:09: **********Val Epoch 115: average Loss: 3319.784912\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 116: 0/23 Loss: 4.493993\n",
      "2025-03-08 00:09: Train Epoch 116: 0/23 Loss: 4.493993\n",
      "2025-03-08 00:09: Train Epoch 116: 0/23 Loss: 4.493993\n",
      "2025-03-08 00:09: Train Epoch 116: 20/23 Loss: 816.054260\n",
      "2025-03-08 00:09: Train Epoch 116: 20/23 Loss: 816.054260\n",
      "2025-03-08 00:09: Train Epoch 116: 20/23 Loss: 816.054260\n",
      "2025-03-08 00:09: **********Train Epoch 116: averaged Loss: 188.354659\n",
      "2025-03-08 00:09: **********Train Epoch 116: averaged Loss: 188.354659\n",
      "2025-03-08 00:09: **********Train Epoch 116: averaged Loss: 188.354659\n",
      "2025-03-08 00:09: **********Val Epoch 116: average Loss: 3315.633667\n",
      "2025-03-08 00:09: **********Val Epoch 116: average Loss: 3315.633667\n",
      "2025-03-08 00:09: **********Val Epoch 116: average Loss: 3315.633667\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 117: 0/23 Loss: 8.072259\n",
      "2025-03-08 00:09: Train Epoch 117: 0/23 Loss: 8.072259\n",
      "2025-03-08 00:09: Train Epoch 117: 0/23 Loss: 8.072259\n",
      "2025-03-08 00:09: Train Epoch 117: 20/23 Loss: 807.766907\n",
      "2025-03-08 00:09: Train Epoch 117: 20/23 Loss: 807.766907\n",
      "2025-03-08 00:09: Train Epoch 117: 20/23 Loss: 807.766907\n",
      "2025-03-08 00:09: **********Train Epoch 117: averaged Loss: 187.946817\n",
      "2025-03-08 00:09: **********Train Epoch 117: averaged Loss: 187.946817\n",
      "2025-03-08 00:09: **********Train Epoch 117: averaged Loss: 187.946817\n",
      "2025-03-08 00:09: **********Val Epoch 117: average Loss: 3311.190430\n",
      "2025-03-08 00:09: **********Val Epoch 117: average Loss: 3311.190430\n",
      "2025-03-08 00:09: **********Val Epoch 117: average Loss: 3311.190430\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 118: 0/23 Loss: 0.592228\n",
      "2025-03-08 00:09: Train Epoch 118: 0/23 Loss: 0.592228\n",
      "2025-03-08 00:09: Train Epoch 118: 0/23 Loss: 0.592228\n",
      "2025-03-08 00:09: Train Epoch 118: 20/23 Loss: 807.446289\n",
      "2025-03-08 00:09: Train Epoch 118: 20/23 Loss: 807.446289\n",
      "2025-03-08 00:09: Train Epoch 118: 20/23 Loss: 807.446289\n",
      "2025-03-08 00:09: **********Train Epoch 118: averaged Loss: 184.486895\n",
      "2025-03-08 00:09: **********Train Epoch 118: averaged Loss: 184.486895\n",
      "2025-03-08 00:09: **********Train Epoch 118: averaged Loss: 184.486895\n",
      "2025-03-08 00:09: **********Val Epoch 118: average Loss: 3307.908081\n",
      "2025-03-08 00:09: **********Val Epoch 118: average Loss: 3307.908081\n",
      "2025-03-08 00:09: **********Val Epoch 118: average Loss: 3307.908081\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 119: 0/23 Loss: 5.407753\n",
      "2025-03-08 00:09: Train Epoch 119: 0/23 Loss: 5.407753\n",
      "2025-03-08 00:09: Train Epoch 119: 0/23 Loss: 5.407753\n",
      "2025-03-08 00:09: Train Epoch 119: 20/23 Loss: 803.128845\n",
      "2025-03-08 00:09: Train Epoch 119: 20/23 Loss: 803.128845\n",
      "2025-03-08 00:09: Train Epoch 119: 20/23 Loss: 803.128845\n",
      "2025-03-08 00:09: **********Train Epoch 119: averaged Loss: 185.316025\n",
      "2025-03-08 00:09: **********Train Epoch 119: averaged Loss: 185.316025\n",
      "2025-03-08 00:09: **********Train Epoch 119: averaged Loss: 185.316025\n",
      "2025-03-08 00:09: **********Val Epoch 119: average Loss: 3296.449341\n",
      "2025-03-08 00:09: **********Val Epoch 119: average Loss: 3296.449341\n",
      "2025-03-08 00:09: **********Val Epoch 119: average Loss: 3296.449341\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: *********************************Current best model saved!\n",
      "2025-03-08 00:09: Train Epoch 120: 0/23 Loss: 1.034401\n",
      "2025-03-08 00:09: Train Epoch 120: 0/23 Loss: 1.034401\n",
      "2025-03-08 00:09: Train Epoch 120: 0/23 Loss: 1.034401\n",
      "2025-03-08 00:09: Train Epoch 120: 20/23 Loss: 793.823303\n",
      "2025-03-08 00:09: Train Epoch 120: 20/23 Loss: 793.823303\n",
      "2025-03-08 00:09: Train Epoch 120: 20/23 Loss: 793.823303\n",
      "2025-03-08 00:09: **********Train Epoch 120: averaged Loss: 180.847330\n",
      "2025-03-08 00:09: **********Train Epoch 120: averaged Loss: 180.847330\n",
      "2025-03-08 00:09: **********Train Epoch 120: averaged Loss: 180.847330\n",
      "2025-03-08 00:09: **********Val Epoch 120: average Loss: 3298.010254\n",
      "2025-03-08 00:09: **********Val Epoch 120: average Loss: 3298.010254\n",
      "2025-03-08 00:09: **********Val Epoch 120: average Loss: 3298.010254\n",
      "2025-03-08 00:09: Train Epoch 121: 0/23 Loss: 6.549289\n",
      "2025-03-08 00:09: Train Epoch 121: 0/23 Loss: 6.549289\n",
      "2025-03-08 00:09: Train Epoch 121: 0/23 Loss: 6.549289\n",
      "2025-03-08 00:09: Train Epoch 121: 20/23 Loss: 778.087646\n",
      "2025-03-08 00:09: Train Epoch 121: 20/23 Loss: 778.087646\n",
      "2025-03-08 00:09: Train Epoch 121: 20/23 Loss: 778.087646\n",
      "2025-03-08 00:09: **********Train Epoch 121: averaged Loss: 180.598218\n",
      "2025-03-08 00:11: Train Epoch 200: 20/23 Loss: 307.000916\n",
      "2025-03-08 00:11: Train Epoch 200: 20/23 Loss: 307.000916\n",
      "2025-03-08 00:11: Train Epoch 200: 20/23 Loss: 307.000916\n",
      "2025-03-08 00:11: **********Train Epoch 200: averaged Loss: 87.879071\n",
      "2025-03-08 00:11: **********Train Epoch 200: averaged Loss: 87.879071\n",
      "2025-03-08 00:11: **********Train Epoch 200: averaged Loss: 87.879071\n",
      "2025-03-08 00:11: **********Val Epoch 200: average Loss: 2729.599731\n",
      "2025-03-08 00:11: **********Val Epoch 200: average Loss: 2729.599731\n",
      "2025-03-08 00:11: **********Val Epoch 200: average Loss: 2729.599731\n",
      "2025-03-08 00:11: Train Epoch 201: 0/23 Loss: 12.077839\n",
      "2025-03-08 00:11: Train Epoch 201: 0/23 Loss: 12.077839\n",
      "2025-03-08 00:11: Train Epoch 201: 0/23 Loss: 12.077839\n",
      "2025-03-08 00:11: Train Epoch 201: 20/23 Loss: 208.986038\n",
      "2025-03-08 00:11: Train Epoch 201: 20/23 Loss: 208.986038\n",
      "2025-03-08 00:11: Train Epoch 201: 20/23 Loss: 208.986038\n",
      "2025-03-08 00:11: **********Train Epoch 201: averaged Loss: 95.668871\n",
      "2025-03-08 00:11: **********Train Epoch 201: averaged Loss: 95.668871\n",
      "2025-03-08 00:11: **********Train Epoch 201: averaged Loss: 95.668871\n",
      "2025-03-08 00:11: **********Val Epoch 201: average Loss: 2707.823975\n",
      "2025-03-08 00:11: **********Val Epoch 201: average Loss: 2707.823975\n",
      "2025-03-08 00:11: **********Val Epoch 201: average Loss: 2707.823975\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: Train Epoch 202: 0/23 Loss: 4.312634\n",
      "2025-03-08 00:11: Train Epoch 202: 0/23 Loss: 4.312634\n",
      "2025-03-08 00:11: Train Epoch 202: 0/23 Loss: 4.312634\n",
      "2025-03-08 00:11: Train Epoch 202: 20/23 Loss: 252.867798\n",
      "2025-03-08 00:11: Train Epoch 202: 20/23 Loss: 252.867798\n",
      "2025-03-08 00:11: Train Epoch 202: 20/23 Loss: 252.867798\n",
      "2025-03-08 00:11: **********Train Epoch 202: averaged Loss: 80.896212\n",
      "2025-03-08 00:11: **********Train Epoch 202: averaged Loss: 80.896212\n",
      "2025-03-08 00:11: **********Train Epoch 202: averaged Loss: 80.896212\n",
      "2025-03-08 00:11: **********Val Epoch 202: average Loss: 2705.700623\n",
      "2025-03-08 00:11: **********Val Epoch 202: average Loss: 2705.700623\n",
      "2025-03-08 00:11: **********Val Epoch 202: average Loss: 2705.700623\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: Train Epoch 203: 0/23 Loss: 16.511002\n",
      "2025-03-08 00:11: Train Epoch 203: 0/23 Loss: 16.511002\n",
      "2025-03-08 00:11: Train Epoch 203: 0/23 Loss: 16.511002\n",
      "2025-03-08 00:11: Train Epoch 203: 20/23 Loss: 199.488495\n",
      "2025-03-08 00:11: Train Epoch 203: 20/23 Loss: 199.488495\n",
      "2025-03-08 00:11: Train Epoch 203: 20/23 Loss: 199.488495\n",
      "2025-03-08 00:11: **********Train Epoch 203: averaged Loss: 83.814174\n",
      "2025-03-08 00:11: **********Train Epoch 203: averaged Loss: 83.814174\n",
      "2025-03-08 00:11: **********Train Epoch 203: averaged Loss: 83.814174\n",
      "2025-03-08 00:11: **********Val Epoch 203: average Loss: 2697.743164\n",
      "2025-03-08 00:11: **********Val Epoch 203: average Loss: 2697.743164\n",
      "2025-03-08 00:11: **********Val Epoch 203: average Loss: 2697.743164\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: Train Epoch 204: 0/23 Loss: 1.442667\n",
      "2025-03-08 00:11: Train Epoch 204: 0/23 Loss: 1.442667\n",
      "2025-03-08 00:11: Train Epoch 204: 0/23 Loss: 1.442667\n",
      "2025-03-08 00:11: Train Epoch 204: 20/23 Loss: 252.995117\n",
      "2025-03-08 00:11: Train Epoch 204: 20/23 Loss: 252.995117\n",
      "2025-03-08 00:11: Train Epoch 204: 20/23 Loss: 252.995117\n",
      "2025-03-08 00:11: **********Train Epoch 204: averaged Loss: 79.139826\n",
      "2025-03-08 00:11: **********Train Epoch 204: averaged Loss: 79.139826\n",
      "2025-03-08 00:11: **********Train Epoch 204: averaged Loss: 79.139826\n",
      "2025-03-08 00:11: **********Val Epoch 204: average Loss: 2683.596008\n",
      "2025-03-08 00:11: **********Val Epoch 204: average Loss: 2683.596008\n",
      "2025-03-08 00:11: **********Val Epoch 204: average Loss: 2683.596008\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: Train Epoch 205: 0/23 Loss: 15.149274\n",
      "2025-03-08 00:11: Train Epoch 205: 0/23 Loss: 15.149274\n",
      "2025-03-08 00:11: Train Epoch 205: 0/23 Loss: 15.149274\n",
      "2025-03-08 00:11: Train Epoch 205: 20/23 Loss: 174.668808\n",
      "2025-03-08 00:11: Train Epoch 205: 20/23 Loss: 174.668808\n",
      "2025-03-08 00:11: Train Epoch 205: 20/23 Loss: 174.668808\n",
      "2025-03-08 00:11: **********Train Epoch 205: averaged Loss: 93.758899\n",
      "2025-03-08 00:11: **********Train Epoch 205: averaged Loss: 93.758899\n",
      "2025-03-08 00:11: **********Train Epoch 205: averaged Loss: 93.758899\n",
      "2025-03-08 00:11: **********Val Epoch 205: average Loss: 2665.437683\n",
      "2025-03-08 00:11: **********Val Epoch 205: average Loss: 2665.437683\n",
      "2025-03-08 00:11: **********Val Epoch 205: average Loss: 2665.437683\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: *********************************Current best model saved!\n",
      "2025-03-08 00:11: Train Epoch 206: 0/23 Loss: 4.728144\n",
      "2025-03-08 00:11: Train Epoch 206: 0/23 Loss: 4.728144\n",
      "2025-03-08 00:11: Train Epoch 206: 0/23 Loss: 4.728144\n",
      "2025-03-08 00:12: Train Epoch 206: 20/23 Loss: 228.187851\n",
      "2025-03-08 00:12: Train Epoch 206: 20/23 Loss: 228.187851\n",
      "2025-03-08 00:12: Train Epoch 206: 20/23 Loss: 228.187851\n",
      "2025-03-08 00:12: **********Train Epoch 206: averaged Loss: 74.201087\n",
      "2025-03-08 00:12: **********Train Epoch 206: averaged Loss: 74.201087\n",
      "2025-03-08 00:12: **********Train Epoch 206: averaged Loss: 74.201087\n",
      "2025-03-08 00:12: **********Val Epoch 206: average Loss: 2635.031799\n",
      "2025-03-08 00:12: **********Val Epoch 206: average Loss: 2635.031799\n",
      "2025-03-08 00:12: **********Val Epoch 206: average Loss: 2635.031799\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 207: 0/23 Loss: 11.518688\n",
      "2025-03-08 00:12: Train Epoch 207: 0/23 Loss: 11.518688\n",
      "2025-03-08 00:12: Train Epoch 207: 0/23 Loss: 11.518688\n",
      "2025-03-08 00:12: Train Epoch 207: 20/23 Loss: 154.314407\n",
      "2025-03-08 00:12: Train Epoch 207: 20/23 Loss: 154.314407\n",
      "2025-03-08 00:12: Train Epoch 207: 20/23 Loss: 154.314407\n",
      "2025-03-08 00:12: **********Train Epoch 207: averaged Loss: 77.971480\n",
      "2025-03-08 00:12: **********Train Epoch 207: averaged Loss: 77.971480\n",
      "2025-03-08 00:12: **********Train Epoch 207: averaged Loss: 77.971480\n",
      "2025-03-08 00:12: **********Val Epoch 207: average Loss: 2671.798462\n",
      "2025-03-08 00:12: **********Val Epoch 207: average Loss: 2671.798462\n",
      "2025-03-08 00:12: **********Val Epoch 207: average Loss: 2671.798462\n",
      "2025-03-08 00:12: Train Epoch 208: 0/23 Loss: 1.140313\n",
      "2025-03-08 00:12: Train Epoch 208: 0/23 Loss: 1.140313\n",
      "2025-03-08 00:12: Train Epoch 208: 0/23 Loss: 1.140313\n",
      "2025-03-08 00:12: Train Epoch 208: 20/23 Loss: 246.306534\n",
      "2025-03-08 00:12: Train Epoch 208: 20/23 Loss: 246.306534\n",
      "2025-03-08 00:12: Train Epoch 208: 20/23 Loss: 246.306534\n",
      "2025-03-08 00:12: **********Train Epoch 208: averaged Loss: 77.205098\n",
      "2025-03-08 00:12: **********Train Epoch 208: averaged Loss: 77.205098\n",
      "2025-03-08 00:12: **********Train Epoch 208: averaged Loss: 77.205098\n",
      "2025-03-08 00:12: **********Val Epoch 208: average Loss: 2643.960022\n",
      "2025-03-08 00:12: **********Val Epoch 208: average Loss: 2643.960022\n",
      "2025-03-08 00:12: **********Val Epoch 208: average Loss: 2643.960022\n",
      "2025-03-08 00:12: Train Epoch 209: 0/23 Loss: 7.690265\n",
      "2025-03-08 00:12: Train Epoch 209: 0/23 Loss: 7.690265\n",
      "2025-03-08 00:12: Train Epoch 209: 0/23 Loss: 7.690265\n",
      "2025-03-08 00:12: Train Epoch 209: 20/23 Loss: 152.257706\n",
      "2025-03-08 00:12: Train Epoch 209: 20/23 Loss: 152.257706\n",
      "2025-03-08 00:12: Train Epoch 209: 20/23 Loss: 152.257706\n",
      "2025-03-08 00:12: **********Train Epoch 209: averaged Loss: 92.899143\n",
      "2025-03-08 00:12: **********Train Epoch 209: averaged Loss: 92.899143\n",
      "2025-03-08 00:12: **********Train Epoch 209: averaged Loss: 92.899143\n",
      "2025-03-08 00:12: **********Val Epoch 209: average Loss: 2642.604431\n",
      "2025-03-08 00:12: **********Val Epoch 209: average Loss: 2642.604431\n",
      "2025-03-08 00:12: **********Val Epoch 209: average Loss: 2642.604431\n",
      "2025-03-08 00:12: Train Epoch 210: 0/23 Loss: 3.923552\n",
      "2025-03-08 00:12: Train Epoch 210: 0/23 Loss: 3.923552\n",
      "2025-03-08 00:12: Train Epoch 210: 0/23 Loss: 3.923552\n",
      "2025-03-08 00:12: Train Epoch 210: 20/23 Loss: 257.609039\n",
      "2025-03-08 00:12: Train Epoch 210: 20/23 Loss: 257.609039\n",
      "2025-03-08 00:12: Train Epoch 210: 20/23 Loss: 257.609039\n",
      "2025-03-08 00:12: **********Train Epoch 210: averaged Loss: 78.275343\n",
      "2025-03-08 00:12: **********Train Epoch 210: averaged Loss: 78.275343\n",
      "2025-03-08 00:12: **********Train Epoch 210: averaged Loss: 78.275343\n",
      "2025-03-08 00:12: **********Val Epoch 210: average Loss: 2623.084717\n",
      "2025-03-08 00:12: **********Val Epoch 210: average Loss: 2623.084717\n",
      "2025-03-08 00:12: **********Val Epoch 210: average Loss: 2623.084717\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 211: 0/23 Loss: 14.756609\n",
      "2025-03-08 00:12: Train Epoch 211: 0/23 Loss: 14.756609\n",
      "2025-03-08 00:12: Train Epoch 211: 0/23 Loss: 14.756609\n",
      "2025-03-08 00:12: Train Epoch 211: 20/23 Loss: 148.304489\n",
      "2025-03-08 00:12: Train Epoch 211: 20/23 Loss: 148.304489\n",
      "2025-03-08 00:12: Train Epoch 211: 20/23 Loss: 148.304489\n",
      "2025-03-08 00:12: **********Train Epoch 211: averaged Loss: 82.783502\n",
      "2025-03-08 00:12: **********Train Epoch 211: averaged Loss: 82.783502\n",
      "2025-03-08 00:12: **********Train Epoch 211: averaged Loss: 82.783502\n",
      "2025-03-08 00:12: **********Val Epoch 211: average Loss: 2617.297974\n",
      "2025-03-08 00:12: **********Val Epoch 211: average Loss: 2617.297974\n",
      "2025-03-08 00:12: **********Val Epoch 211: average Loss: 2617.297974\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 212: 0/23 Loss: 1.606304\n",
      "2025-03-08 00:12: Train Epoch 212: 0/23 Loss: 1.606304\n",
      "2025-03-08 00:12: Train Epoch 212: 0/23 Loss: 1.606304\n",
      "2025-03-08 00:12: Train Epoch 212: 20/23 Loss: 197.045700\n",
      "2025-03-08 00:12: Train Epoch 212: 20/23 Loss: 197.045700\n",
      "2025-03-08 00:12: Train Epoch 212: 20/23 Loss: 197.045700\n",
      "2025-03-08 00:12: **********Train Epoch 212: averaged Loss: 69.973523\n",
      "2025-03-08 00:12: **********Train Epoch 212: averaged Loss: 69.973523\n",
      "2025-03-08 00:12: **********Train Epoch 212: averaged Loss: 69.973523\n",
      "2025-03-08 00:12: **********Val Epoch 212: average Loss: 2611.560303\n",
      "2025-03-08 00:12: **********Val Epoch 212: average Loss: 2611.560303\n",
      "2025-03-08 00:12: **********Val Epoch 212: average Loss: 2611.560303\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 213: 0/23 Loss: 12.292247\n",
      "2025-03-08 00:12: Train Epoch 213: 0/23 Loss: 12.292247\n",
      "2025-03-08 00:12: Train Epoch 213: 0/23 Loss: 12.292247\n",
      "2025-03-08 00:12: Train Epoch 213: 20/23 Loss: 164.495163\n",
      "2025-03-08 00:12: Train Epoch 213: 20/23 Loss: 164.495163\n",
      "2025-03-08 00:12: Train Epoch 213: 20/23 Loss: 164.495163\n",
      "2025-03-08 00:12: **********Train Epoch 213: averaged Loss: 67.770565\n",
      "2025-03-08 00:12: **********Train Epoch 213: averaged Loss: 67.770565\n",
      "2025-03-08 00:12: **********Train Epoch 213: averaged Loss: 67.770565\n",
      "2025-03-08 00:12: **********Val Epoch 213: average Loss: 2599.272827\n",
      "2025-03-08 00:12: **********Val Epoch 213: average Loss: 2599.272827\n",
      "2025-03-08 00:12: **********Val Epoch 213: average Loss: 2599.272827\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 214: 0/23 Loss: 5.019464\n",
      "2025-03-08 00:12: Train Epoch 214: 0/23 Loss: 5.019464\n",
      "2025-03-08 00:12: Train Epoch 214: 0/23 Loss: 5.019464\n",
      "2025-03-08 00:12: Train Epoch 214: 20/23 Loss: 142.492462\n",
      "2025-03-08 00:12: Train Epoch 214: 20/23 Loss: 142.492462\n",
      "2025-03-08 00:12: Train Epoch 214: 20/23 Loss: 142.492462\n",
      "2025-03-08 00:12: **********Train Epoch 214: averaged Loss: 69.241055\n",
      "2025-03-08 00:12: **********Train Epoch 214: averaged Loss: 69.241055\n",
      "2025-03-08 00:12: **********Train Epoch 214: averaged Loss: 69.241055\n",
      "2025-03-08 00:12: **********Val Epoch 214: average Loss: 2603.562378\n",
      "2025-03-08 00:12: **********Val Epoch 214: average Loss: 2603.562378\n",
      "2025-03-08 00:12: **********Val Epoch 214: average Loss: 2603.562378\n",
      "2025-03-08 00:12: Train Epoch 215: 0/23 Loss: 6.185781\n",
      "2025-03-08 00:12: Train Epoch 215: 0/23 Loss: 6.185781\n",
      "2025-03-08 00:12: Train Epoch 215: 0/23 Loss: 6.185781\n",
      "2025-03-08 00:12: Train Epoch 215: 20/23 Loss: 208.193283\n",
      "2025-03-08 00:12: Train Epoch 215: 20/23 Loss: 208.193283\n",
      "2025-03-08 00:12: Train Epoch 215: 20/23 Loss: 208.193283\n",
      "2025-03-08 00:12: **********Train Epoch 215: averaged Loss: 68.800498\n",
      "2025-03-08 00:12: **********Train Epoch 215: averaged Loss: 68.800498\n",
      "2025-03-08 00:12: **********Train Epoch 215: averaged Loss: 68.800498\n",
      "2025-03-08 00:12: **********Val Epoch 215: average Loss: 2606.116150\n",
      "2025-03-08 00:12: **********Val Epoch 215: average Loss: 2606.116150\n",
      "2025-03-08 00:12: **********Val Epoch 215: average Loss: 2606.116150\n",
      "2025-03-08 00:12: Train Epoch 216: 0/23 Loss: 5.996640\n",
      "2025-03-08 00:12: Train Epoch 216: 0/23 Loss: 5.996640\n",
      "2025-03-08 00:12: Train Epoch 216: 0/23 Loss: 5.996640\n",
      "2025-03-08 00:12: Train Epoch 216: 20/23 Loss: 127.254654\n",
      "2025-03-08 00:12: Train Epoch 216: 20/23 Loss: 127.254654\n",
      "2025-03-08 00:12: Train Epoch 216: 20/23 Loss: 127.254654\n",
      "2025-03-08 00:12: **********Train Epoch 216: averaged Loss: 76.884381\n",
      "2025-03-08 00:12: **********Train Epoch 216: averaged Loss: 76.884381\n",
      "2025-03-08 00:12: **********Train Epoch 216: averaged Loss: 76.884381\n",
      "2025-03-08 00:12: **********Val Epoch 216: average Loss: 2578.644470\n",
      "2025-03-08 00:12: **********Val Epoch 216: average Loss: 2578.644470\n",
      "2025-03-08 00:12: **********Val Epoch 216: average Loss: 2578.644470\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 217: 0/23 Loss: 3.150383\n",
      "2025-03-08 00:12: Train Epoch 217: 0/23 Loss: 3.150383\n",
      "2025-03-08 00:12: Train Epoch 217: 0/23 Loss: 3.150383\n",
      "2025-03-08 00:12: Train Epoch 217: 20/23 Loss: 128.732239\n",
      "2025-03-08 00:12: Train Epoch 217: 20/23 Loss: 128.732239\n",
      "2025-03-08 00:12: Train Epoch 217: 20/23 Loss: 128.732239\n",
      "2025-03-08 00:12: **********Train Epoch 217: averaged Loss: 63.190954\n",
      "2025-03-08 00:12: **********Train Epoch 217: averaged Loss: 63.190954\n",
      "2025-03-08 00:12: **********Train Epoch 217: averaged Loss: 63.190954\n",
      "2025-03-08 00:12: **********Val Epoch 217: average Loss: 2584.609436\n",
      "2025-03-08 00:12: **********Val Epoch 217: average Loss: 2584.609436\n",
      "2025-03-08 00:12: **********Val Epoch 217: average Loss: 2584.609436\n",
      "2025-03-08 00:12: Train Epoch 218: 0/23 Loss: 13.695429\n",
      "2025-03-08 00:12: Train Epoch 218: 0/23 Loss: 13.695429\n",
      "2025-03-08 00:12: Train Epoch 218: 0/23 Loss: 13.695429\n",
      "2025-03-08 00:12: Train Epoch 218: 20/23 Loss: 170.272034\n",
      "2025-03-08 00:12: Train Epoch 218: 20/23 Loss: 170.272034\n",
      "2025-03-08 00:12: Train Epoch 218: 20/23 Loss: 170.272034\n",
      "2025-03-08 00:12: **********Train Epoch 218: averaged Loss: 66.578730\n",
      "2025-03-08 00:12: **********Train Epoch 218: averaged Loss: 66.578730\n",
      "2025-03-08 00:12: **********Train Epoch 218: averaged Loss: 66.578730\n",
      "2025-03-08 00:12: **********Val Epoch 218: average Loss: 2562.750549\n",
      "2025-03-08 00:12: **********Val Epoch 218: average Loss: 2562.750549\n",
      "2025-03-08 00:12: **********Val Epoch 218: average Loss: 2562.750549\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 219: 0/23 Loss: 1.812646\n",
      "2025-03-08 00:12: Train Epoch 219: 0/23 Loss: 1.812646\n",
      "2025-03-08 00:12: Train Epoch 219: 0/23 Loss: 1.812646\n",
      "2025-03-08 00:12: Train Epoch 219: 20/23 Loss: 121.923683\n",
      "2025-03-08 00:12: Train Epoch 219: 20/23 Loss: 121.923683\n",
      "2025-03-08 00:12: Train Epoch 219: 20/23 Loss: 121.923683\n",
      "2025-03-08 00:12: **********Train Epoch 219: averaged Loss: 78.493324\n",
      "2025-03-08 00:12: **********Train Epoch 219: averaged Loss: 78.493324\n",
      "2025-03-08 00:12: **********Train Epoch 219: averaged Loss: 78.493324\n",
      "2025-03-08 00:12: **********Val Epoch 219: average Loss: 2562.998169\n",
      "2025-03-08 00:12: **********Val Epoch 219: average Loss: 2562.998169\n",
      "2025-03-08 00:12: **********Val Epoch 219: average Loss: 2562.998169\n",
      "2025-03-08 00:12: Train Epoch 220: 0/23 Loss: 5.007075\n",
      "2025-03-08 00:12: Train Epoch 220: 0/23 Loss: 5.007075\n",
      "2025-03-08 00:12: Train Epoch 220: 0/23 Loss: 5.007075\n",
      "2025-03-08 00:12: Train Epoch 220: 20/23 Loss: 119.467590\n",
      "2025-03-08 00:12: Train Epoch 220: 20/23 Loss: 119.467590\n",
      "2025-03-08 00:12: Train Epoch 220: 20/23 Loss: 119.467590\n",
      "2025-03-08 00:12: **********Train Epoch 220: averaged Loss: 62.039454\n",
      "2025-03-08 00:12: **********Train Epoch 220: averaged Loss: 62.039454\n",
      "2025-03-08 00:12: **********Train Epoch 220: averaged Loss: 62.039454\n",
      "2025-03-08 00:12: **********Val Epoch 220: average Loss: 2569.523376\n",
      "2025-03-08 00:12: **********Val Epoch 220: average Loss: 2569.523376\n",
      "2025-03-08 00:12: **********Val Epoch 220: average Loss: 2569.523376\n",
      "2025-03-08 00:12: Train Epoch 221: 0/23 Loss: 3.751473\n",
      "2025-03-08 00:12: Train Epoch 221: 0/23 Loss: 3.751473\n",
      "2025-03-08 00:12: Train Epoch 221: 0/23 Loss: 3.751473\n",
      "2025-03-08 00:12: Train Epoch 221: 20/23 Loss: 148.394119\n",
      "2025-03-08 00:12: Train Epoch 221: 20/23 Loss: 148.394119\n",
      "2025-03-08 00:12: Train Epoch 221: 20/23 Loss: 148.394119\n",
      "2025-03-08 00:12: **********Train Epoch 221: averaged Loss: 60.593301\n",
      "2025-03-08 00:12: **********Train Epoch 221: averaged Loss: 60.593301\n",
      "2025-03-08 00:12: **********Train Epoch 221: averaged Loss: 60.593301\n",
      "2025-03-08 00:12: **********Val Epoch 221: average Loss: 2544.903931\n",
      "2025-03-08 00:12: **********Val Epoch 221: average Loss: 2544.903931\n",
      "2025-03-08 00:12: **********Val Epoch 221: average Loss: 2544.903931\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 222: 0/23 Loss: 2.572314\n",
      "2025-03-08 00:12: Train Epoch 222: 0/23 Loss: 2.572314\n",
      "2025-03-08 00:12: Train Epoch 222: 0/23 Loss: 2.572314\n",
      "2025-03-08 00:12: Train Epoch 222: 20/23 Loss: 104.933380\n",
      "2025-03-08 00:12: Train Epoch 222: 20/23 Loss: 104.933380\n",
      "2025-03-08 00:12: Train Epoch 222: 20/23 Loss: 104.933380\n",
      "2025-03-08 00:12: **********Train Epoch 222: averaged Loss: 72.700021\n",
      "2025-03-08 00:12: **********Train Epoch 222: averaged Loss: 72.700021\n",
      "2025-03-08 00:12: **********Train Epoch 222: averaged Loss: 72.700021\n",
      "2025-03-08 00:12: **********Val Epoch 222: average Loss: 2528.109924\n",
      "2025-03-08 00:12: **********Val Epoch 222: average Loss: 2528.109924\n",
      "2025-03-08 00:12: **********Val Epoch 222: average Loss: 2528.109924\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 223: 0/23 Loss: 4.457613\n",
      "2025-03-08 00:12: Train Epoch 223: 0/23 Loss: 4.457613\n",
      "2025-03-08 00:12: Train Epoch 223: 0/23 Loss: 4.457613\n",
      "2025-03-08 00:12: Train Epoch 223: 20/23 Loss: 102.967583\n",
      "2025-03-08 00:12: Train Epoch 223: 20/23 Loss: 102.967583\n",
      "2025-03-08 00:12: Train Epoch 223: 20/23 Loss: 102.967583\n",
      "2025-03-08 00:12: **********Train Epoch 223: averaged Loss: 59.234176\n",
      "2025-03-08 00:12: **********Train Epoch 223: averaged Loss: 59.234176\n",
      "2025-03-08 00:12: **********Train Epoch 223: averaged Loss: 59.234176\n",
      "2025-03-08 00:12: **********Val Epoch 223: average Loss: 2522.860718\n",
      "2025-03-08 00:12: **********Val Epoch 223: average Loss: 2522.860718\n",
      "2025-03-08 00:12: **********Val Epoch 223: average Loss: 2522.860718\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 224: 0/23 Loss: 11.543242\n",
      "2025-03-08 00:12: Train Epoch 224: 0/23 Loss: 11.543242\n",
      "2025-03-08 00:12: Train Epoch 224: 0/23 Loss: 11.543242\n",
      "2025-03-08 00:12: Train Epoch 224: 20/23 Loss: 106.578300\n",
      "2025-03-08 00:12: Train Epoch 224: 20/23 Loss: 106.578300\n",
      "2025-03-08 00:12: Train Epoch 224: 20/23 Loss: 106.578300\n",
      "2025-03-08 00:12: **********Train Epoch 224: averaged Loss: 56.101366\n",
      "2025-03-08 00:12: **********Train Epoch 224: averaged Loss: 56.101366\n",
      "2025-03-08 00:12: **********Train Epoch 224: averaged Loss: 56.101366\n",
      "2025-03-08 00:12: **********Val Epoch 224: average Loss: 2523.627075\n",
      "2025-03-08 00:12: **********Val Epoch 224: average Loss: 2523.627075\n",
      "2025-03-08 00:12: **********Val Epoch 224: average Loss: 2523.627075\n",
      "2025-03-08 00:12: Train Epoch 225: 0/23 Loss: 0.695606\n",
      "2025-03-08 00:12: Train Epoch 225: 0/23 Loss: 0.695606\n",
      "2025-03-08 00:12: Train Epoch 225: 0/23 Loss: 0.695606\n",
      "2025-03-08 00:12: Train Epoch 225: 20/23 Loss: 138.865906\n",
      "2025-03-08 00:12: Train Epoch 225: 20/23 Loss: 138.865906\n",
      "2025-03-08 00:12: Train Epoch 225: 20/23 Loss: 138.865906\n",
      "2025-03-08 00:12: **********Train Epoch 225: averaged Loss: 56.982084\n",
      "2025-03-08 00:12: **********Train Epoch 225: averaged Loss: 56.982084\n",
      "2025-03-08 00:12: **********Train Epoch 225: averaged Loss: 56.982084\n",
      "2025-03-08 00:12: **********Val Epoch 225: average Loss: 2522.363403\n",
      "2025-03-08 00:12: **********Val Epoch 225: average Loss: 2522.363403\n",
      "2025-03-08 00:12: **********Val Epoch 225: average Loss: 2522.363403\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 226: 0/23 Loss: 7.458898\n",
      "2025-03-08 00:12: Train Epoch 226: 0/23 Loss: 7.458898\n",
      "2025-03-08 00:12: Train Epoch 226: 0/23 Loss: 7.458898\n",
      "2025-03-08 00:12: Train Epoch 226: 20/23 Loss: 93.785446\n",
      "2025-03-08 00:12: Train Epoch 226: 20/23 Loss: 93.785446\n",
      "2025-03-08 00:12: Train Epoch 226: 20/23 Loss: 93.785446\n",
      "2025-03-08 00:12: **********Train Epoch 226: averaged Loss: 78.577600\n",
      "2025-03-08 00:12: **********Train Epoch 226: averaged Loss: 78.577600\n",
      "2025-03-08 00:12: **********Train Epoch 226: averaged Loss: 78.577600\n",
      "2025-03-08 00:12: **********Val Epoch 226: average Loss: 2521.911682\n",
      "2025-03-08 00:12: **********Val Epoch 226: average Loss: 2521.911682\n",
      "2025-03-08 00:12: **********Val Epoch 226: average Loss: 2521.911682\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 227: 0/23 Loss: 1.148593\n",
      "2025-03-08 00:12: Train Epoch 227: 0/23 Loss: 1.148593\n",
      "2025-03-08 00:12: Train Epoch 227: 0/23 Loss: 1.148593\n",
      "2025-03-08 00:12: Train Epoch 227: 20/23 Loss: 177.771500\n",
      "2025-03-08 00:12: Train Epoch 227: 20/23 Loss: 177.771500\n",
      "2025-03-08 00:12: Train Epoch 227: 20/23 Loss: 177.771500\n",
      "2025-03-08 00:12: **********Train Epoch 227: averaged Loss: 65.790270\n",
      "2025-03-08 00:12: **********Train Epoch 227: averaged Loss: 65.790270\n",
      "2025-03-08 00:12: **********Train Epoch 227: averaged Loss: 65.790270\n",
      "2025-03-08 00:12: **********Val Epoch 227: average Loss: 2498.522095\n",
      "2025-03-08 00:12: **********Val Epoch 227: average Loss: 2498.522095\n",
      "2025-03-08 00:12: **********Val Epoch 227: average Loss: 2498.522095\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 228: 0/23 Loss: 13.350475\n",
      "2025-03-08 00:12: Train Epoch 228: 0/23 Loss: 13.350475\n",
      "2025-03-08 00:12: Train Epoch 228: 0/23 Loss: 13.350475\n",
      "2025-03-08 00:12: Train Epoch 228: 20/23 Loss: 95.893875\n",
      "2025-03-08 00:12: Train Epoch 228: 20/23 Loss: 95.893875\n",
      "2025-03-08 00:12: Train Epoch 228: 20/23 Loss: 95.893875\n",
      "2025-03-08 00:12: **********Train Epoch 228: averaged Loss: 76.530684\n",
      "2025-03-08 00:12: **********Train Epoch 228: averaged Loss: 76.530684\n",
      "2025-03-08 00:12: **********Train Epoch 228: averaged Loss: 76.530684\n",
      "2025-03-08 00:12: **********Val Epoch 228: average Loss: 2490.673584\n",
      "2025-03-08 00:12: **********Val Epoch 228: average Loss: 2490.673584\n",
      "2025-03-08 00:12: **********Val Epoch 228: average Loss: 2490.673584\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 229: 0/23 Loss: 6.351068\n",
      "2025-03-08 00:12: Train Epoch 229: 0/23 Loss: 6.351068\n",
      "2025-03-08 00:12: Train Epoch 229: 0/23 Loss: 6.351068\n",
      "2025-03-08 00:12: Train Epoch 229: 20/23 Loss: 99.574516\n",
      "2025-03-08 00:12: Train Epoch 229: 20/23 Loss: 99.574516\n",
      "2025-03-08 00:12: Train Epoch 229: 20/23 Loss: 99.574516\n",
      "2025-03-08 00:12: **********Train Epoch 229: averaged Loss: 58.029767\n",
      "2025-03-08 00:12: **********Train Epoch 229: averaged Loss: 58.029767\n",
      "2025-03-08 00:12: **********Train Epoch 229: averaged Loss: 58.029767\n",
      "2025-03-08 00:12: **********Val Epoch 229: average Loss: 2491.908447\n",
      "2025-03-08 00:12: **********Val Epoch 229: average Loss: 2491.908447\n",
      "2025-03-08 00:12: **********Val Epoch 229: average Loss: 2491.908447\n",
      "2025-03-08 00:12: Train Epoch 230: 0/23 Loss: 12.900522\n",
      "2025-03-08 00:12: Train Epoch 230: 0/23 Loss: 12.900522\n",
      "2025-03-08 00:12: Train Epoch 230: 0/23 Loss: 12.900522\n",
      "2025-03-08 00:12: Train Epoch 230: 20/23 Loss: 127.376984\n",
      "2025-03-08 00:12: Train Epoch 230: 20/23 Loss: 127.376984\n",
      "2025-03-08 00:12: Train Epoch 230: 20/23 Loss: 127.376984\n",
      "2025-03-08 00:12: **********Train Epoch 230: averaged Loss: 54.465166\n",
      "2025-03-08 00:12: **********Train Epoch 230: averaged Loss: 54.465166\n",
      "2025-03-08 00:12: **********Train Epoch 230: averaged Loss: 54.465166\n",
      "2025-03-08 00:12: **********Val Epoch 230: average Loss: 2490.632874\n",
      "2025-03-08 00:12: **********Val Epoch 230: average Loss: 2490.632874\n",
      "2025-03-08 00:12: **********Val Epoch 230: average Loss: 2490.632874\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 231: 0/23 Loss: 0.786872\n",
      "2025-03-08 00:12: Train Epoch 231: 0/23 Loss: 0.786872\n",
      "2025-03-08 00:12: Train Epoch 231: 0/23 Loss: 0.786872\n",
      "2025-03-08 00:12: Train Epoch 231: 20/23 Loss: 87.855217\n",
      "2025-03-08 00:12: Train Epoch 231: 20/23 Loss: 87.855217\n",
      "2025-03-08 00:12: Train Epoch 231: 20/23 Loss: 87.855217\n",
      "2025-03-08 00:12: **********Train Epoch 231: averaged Loss: 63.163671\n",
      "2025-03-08 00:12: **********Train Epoch 231: averaged Loss: 63.163671\n",
      "2025-03-08 00:12: **********Train Epoch 231: averaged Loss: 63.163671\n",
      "2025-03-08 00:12: **********Val Epoch 231: average Loss: 2462.725037\n",
      "2025-03-08 00:12: **********Val Epoch 231: average Loss: 2462.725037\n",
      "2025-03-08 00:12: **********Val Epoch 231: average Loss: 2462.725037\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 232: 0/23 Loss: 7.027452\n",
      "2025-03-08 00:12: Train Epoch 232: 0/23 Loss: 7.027452\n",
      "2025-03-08 00:12: Train Epoch 232: 0/23 Loss: 7.027452\n",
      "2025-03-08 00:12: Train Epoch 232: 20/23 Loss: 77.874855\n",
      "2025-03-08 00:12: Train Epoch 232: 20/23 Loss: 77.874855\n",
      "2025-03-08 00:12: Train Epoch 232: 20/23 Loss: 77.874855\n",
      "2025-03-08 00:12: **********Train Epoch 232: averaged Loss: 53.628516\n",
      "2025-03-08 00:12: **********Train Epoch 232: averaged Loss: 53.628516\n",
      "2025-03-08 00:12: **********Train Epoch 232: averaged Loss: 53.628516\n",
      "2025-03-08 00:12: **********Val Epoch 232: average Loss: 2454.983582\n",
      "2025-03-08 00:12: **********Val Epoch 232: average Loss: 2454.983582\n",
      "2025-03-08 00:12: **********Val Epoch 232: average Loss: 2454.983582\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 233: 0/23 Loss: 9.900578\n",
      "2025-03-08 00:12: Train Epoch 233: 0/23 Loss: 9.900578\n",
      "2025-03-08 00:12: Train Epoch 233: 0/23 Loss: 9.900578\n",
      "2025-03-08 00:12: Train Epoch 233: 20/23 Loss: 82.454453\n",
      "2025-03-08 00:12: Train Epoch 233: 20/23 Loss: 82.454453\n",
      "2025-03-08 00:12: Train Epoch 233: 20/23 Loss: 82.454453\n",
      "2025-03-08 00:12: **********Train Epoch 233: averaged Loss: 51.592007\n",
      "2025-03-08 00:12: **********Train Epoch 233: averaged Loss: 51.592007\n",
      "2025-03-08 00:12: **********Train Epoch 233: averaged Loss: 51.592007\n",
      "2025-03-08 00:12: **********Val Epoch 233: average Loss: 2464.031311\n",
      "2025-03-08 00:12: **********Val Epoch 233: average Loss: 2464.031311\n",
      "2025-03-08 00:12: **********Val Epoch 233: average Loss: 2464.031311\n",
      "2025-03-08 00:12: Train Epoch 234: 0/23 Loss: 1.634214\n",
      "2025-03-08 00:12: Train Epoch 234: 0/23 Loss: 1.634214\n",
      "2025-03-08 00:12: Train Epoch 234: 0/23 Loss: 1.634214\n",
      "2025-03-08 00:12: Train Epoch 234: 20/23 Loss: 132.161041\n",
      "2025-03-08 00:12: Train Epoch 234: 20/23 Loss: 132.161041\n",
      "2025-03-08 00:12: Train Epoch 234: 20/23 Loss: 132.161041\n",
      "2025-03-08 00:12: **********Train Epoch 234: averaged Loss: 53.065809\n",
      "2025-03-08 00:12: **********Train Epoch 234: averaged Loss: 53.065809\n",
      "2025-03-08 00:12: **********Train Epoch 234: averaged Loss: 53.065809\n",
      "2025-03-08 00:12: **********Val Epoch 234: average Loss: 2452.909241\n",
      "2025-03-08 00:12: **********Val Epoch 234: average Loss: 2452.909241\n",
      "2025-03-08 00:12: **********Val Epoch 234: average Loss: 2452.909241\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 235: 0/23 Loss: 11.175897\n",
      "2025-03-08 00:12: Train Epoch 235: 0/23 Loss: 11.175897\n",
      "2025-03-08 00:12: Train Epoch 235: 0/23 Loss: 11.175897\n",
      "2025-03-08 00:12: Train Epoch 235: 20/23 Loss: 92.260124\n",
      "2025-03-08 00:12: Train Epoch 235: 20/23 Loss: 92.260124\n",
      "2025-03-08 00:12: Train Epoch 235: 20/23 Loss: 92.260124\n",
      "2025-03-08 00:12: **********Train Epoch 235: averaged Loss: 83.555252\n",
      "2025-03-08 00:12: **********Train Epoch 235: averaged Loss: 83.555252\n",
      "2025-03-08 00:12: **********Train Epoch 235: averaged Loss: 83.555252\n",
      "2025-03-08 00:12: **********Val Epoch 235: average Loss: 2440.591187\n",
      "2025-03-08 00:12: **********Val Epoch 235: average Loss: 2440.591187\n",
      "2025-03-08 00:12: **********Val Epoch 235: average Loss: 2440.591187\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 236: 0/23 Loss: 4.250218\n",
      "2025-03-08 00:12: Train Epoch 236: 0/23 Loss: 4.250218\n",
      "2025-03-08 00:12: Train Epoch 236: 0/23 Loss: 4.250218\n",
      "2025-03-08 00:12: Train Epoch 236: 20/23 Loss: 79.038483\n",
      "2025-03-08 00:12: Train Epoch 236: 20/23 Loss: 79.038483\n",
      "2025-03-08 00:12: Train Epoch 236: 20/23 Loss: 79.038483\n",
      "2025-03-08 00:12: **********Train Epoch 236: averaged Loss: 51.571502\n",
      "2025-03-08 00:12: **********Train Epoch 236: averaged Loss: 51.571502\n",
      "2025-03-08 00:12: **********Train Epoch 236: averaged Loss: 51.571502\n",
      "2025-03-08 00:12: **********Val Epoch 236: average Loss: 2452.465210\n",
      "2025-03-08 00:12: **********Val Epoch 236: average Loss: 2452.465210\n",
      "2025-03-08 00:12: **********Val Epoch 236: average Loss: 2452.465210\n",
      "2025-03-08 00:12: Train Epoch 237: 0/23 Loss: 12.934823\n",
      "2025-03-08 00:12: Train Epoch 237: 0/23 Loss: 12.934823\n",
      "2025-03-08 00:12: Train Epoch 237: 0/23 Loss: 12.934823\n",
      "2025-03-08 00:12: Train Epoch 237: 20/23 Loss: 88.853920\n",
      "2025-03-08 00:12: Train Epoch 237: 20/23 Loss: 88.853920\n",
      "2025-03-08 00:12: Train Epoch 237: 20/23 Loss: 88.853920\n",
      "2025-03-08 00:12: **********Train Epoch 237: averaged Loss: 50.380750\n",
      "2025-03-08 00:12: **********Train Epoch 237: averaged Loss: 50.380750\n",
      "2025-03-08 00:12: **********Train Epoch 237: averaged Loss: 50.380750\n",
      "2025-03-08 00:12: **********Val Epoch 237: average Loss: 2434.748413\n",
      "2025-03-08 00:12: **********Val Epoch 237: average Loss: 2434.748413\n",
      "2025-03-08 00:12: **********Val Epoch 237: average Loss: 2434.748413\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 238: 0/23 Loss: 4.375995\n",
      "2025-03-08 00:12: Train Epoch 238: 0/23 Loss: 4.375995\n",
      "2025-03-08 00:12: Train Epoch 238: 0/23 Loss: 4.375995\n",
      "2025-03-08 00:12: Train Epoch 238: 20/23 Loss: 90.261826\n",
      "2025-03-08 00:12: Train Epoch 238: 20/23 Loss: 90.261826\n",
      "2025-03-08 00:12: Train Epoch 238: 20/23 Loss: 90.261826\n",
      "2025-03-08 00:12: **********Train Epoch 238: averaged Loss: 48.142689\n",
      "2025-03-08 00:12: **********Train Epoch 238: averaged Loss: 48.142689\n",
      "2025-03-08 00:12: **********Train Epoch 238: averaged Loss: 48.142689\n",
      "2025-03-08 00:12: **********Val Epoch 238: average Loss: 2429.652466\n",
      "2025-03-08 00:12: **********Val Epoch 238: average Loss: 2429.652466\n",
      "2025-03-08 00:12: **********Val Epoch 238: average Loss: 2429.652466\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: *********************************Current best model saved!\n",
      "2025-03-08 00:12: Train Epoch 239: 0/23 Loss: 13.559856\n",
      "2025-03-08 00:12: Train Epoch 239: 0/23 Loss: 13.559856\n",
      "2025-03-08 00:12: Train Epoch 239: 0/23 Loss: 13.559856\n",
      "2025-03-08 00:13: Train Epoch 239: 20/23 Loss: 85.244934\n",
      "2025-03-08 00:13: Train Epoch 239: 20/23 Loss: 85.244934\n",
      "2025-03-08 00:13: Train Epoch 239: 20/23 Loss: 85.244934\n",
      "2025-03-08 00:13: **********Train Epoch 239: averaged Loss: 59.270532\n",
      "2025-03-08 00:13: **********Train Epoch 239: averaged Loss: 59.270532\n",
      "2025-03-08 00:13: **********Train Epoch 239: averaged Loss: 59.270532\n",
      "2025-03-08 00:13: **********Val Epoch 239: average Loss: 2437.198608\n",
      "2025-03-08 00:13: **********Val Epoch 239: average Loss: 2437.198608\n",
      "2025-03-08 00:13: **********Val Epoch 239: average Loss: 2437.198608\n",
      "2025-03-08 00:13: Train Epoch 240: 0/23 Loss: 5.476041\n",
      "2025-03-08 00:13: Train Epoch 240: 0/23 Loss: 5.476041\n",
      "2025-03-08 00:13: Train Epoch 240: 0/23 Loss: 5.476041\n",
      "2025-03-08 00:13: Train Epoch 240: 20/23 Loss: 112.001884\n",
      "2025-03-08 00:13: Train Epoch 240: 20/23 Loss: 112.001884\n",
      "2025-03-08 00:13: Train Epoch 240: 20/23 Loss: 112.001884\n",
      "2025-03-08 00:13: **********Train Epoch 240: averaged Loss: 53.365831\n",
      "2025-03-08 00:13: **********Train Epoch 240: averaged Loss: 53.365831\n",
      "2025-03-08 00:13: **********Train Epoch 240: averaged Loss: 53.365831\n",
      "2025-03-08 00:13: **********Val Epoch 240: average Loss: 2416.069458\n",
      "2025-03-08 00:13: **********Val Epoch 240: average Loss: 2416.069458\n",
      "2025-03-08 00:13: **********Val Epoch 240: average Loss: 2416.069458\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 241: 0/23 Loss: 15.939507\n",
      "2025-03-08 00:13: Train Epoch 241: 0/23 Loss: 15.939507\n",
      "2025-03-08 00:13: Train Epoch 241: 0/23 Loss: 15.939507\n",
      "2025-03-08 00:13: Train Epoch 241: 20/23 Loss: 85.394691\n",
      "2025-03-08 00:13: Train Epoch 241: 20/23 Loss: 85.394691\n",
      "2025-03-08 00:13: Train Epoch 241: 20/23 Loss: 85.394691\n",
      "2025-03-08 00:13: **********Train Epoch 241: averaged Loss: 75.453214\n",
      "2025-03-08 00:13: **********Train Epoch 241: averaged Loss: 75.453214\n",
      "2025-03-08 00:13: **********Train Epoch 241: averaged Loss: 75.453214\n",
      "2025-03-08 00:13: **********Val Epoch 241: average Loss: 2423.063171\n",
      "2025-03-08 00:13: **********Val Epoch 241: average Loss: 2423.063171\n",
      "2025-03-08 00:13: **********Val Epoch 241: average Loss: 2423.063171\n",
      "2025-03-08 00:13: Train Epoch 242: 0/23 Loss: 3.219279\n",
      "2025-03-08 00:13: Train Epoch 242: 0/23 Loss: 3.219279\n",
      "2025-03-08 00:13: Train Epoch 242: 0/23 Loss: 3.219279\n",
      "2025-03-08 00:13: Train Epoch 242: 20/23 Loss: 73.232803\n",
      "2025-03-08 00:13: Train Epoch 242: 20/23 Loss: 73.232803\n",
      "2025-03-08 00:13: Train Epoch 242: 20/23 Loss: 73.232803\n",
      "2025-03-08 00:13: **********Train Epoch 242: averaged Loss: 51.409959\n",
      "2025-03-08 00:13: **********Train Epoch 242: averaged Loss: 51.409959\n",
      "2025-03-08 00:13: **********Train Epoch 242: averaged Loss: 51.409959\n",
      "2025-03-08 00:13: **********Val Epoch 242: average Loss: 2409.022217\n",
      "2025-03-08 00:13: **********Val Epoch 242: average Loss: 2409.022217\n",
      "2025-03-08 00:13: **********Val Epoch 242: average Loss: 2409.022217\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 243: 0/23 Loss: 12.485500\n",
      "2025-03-08 00:13: Train Epoch 243: 0/23 Loss: 12.485500\n",
      "2025-03-08 00:13: Train Epoch 243: 0/23 Loss: 12.485500\n",
      "2025-03-08 00:13: Train Epoch 243: 20/23 Loss: 81.657837\n",
      "2025-03-08 00:13: Train Epoch 243: 20/23 Loss: 81.657837\n",
      "2025-03-08 00:13: Train Epoch 243: 20/23 Loss: 81.657837\n",
      "2025-03-08 00:13: **********Train Epoch 243: averaged Loss: 47.468098\n",
      "2025-03-08 00:13: **********Train Epoch 243: averaged Loss: 47.468098\n",
      "2025-03-08 00:13: **********Train Epoch 243: averaged Loss: 47.468098\n",
      "2025-03-08 00:13: **********Val Epoch 243: average Loss: 2413.994141\n",
      "2025-03-08 00:13: **********Val Epoch 243: average Loss: 2413.994141\n",
      "2025-03-08 00:13: **********Val Epoch 243: average Loss: 2413.994141\n",
      "2025-03-08 00:13: Train Epoch 244: 0/23 Loss: 0.700073\n",
      "2025-03-08 00:13: Train Epoch 244: 0/23 Loss: 0.700073\n",
      "2025-03-08 00:13: Train Epoch 244: 0/23 Loss: 0.700073\n",
      "2025-03-08 00:13: Train Epoch 244: 20/23 Loss: 79.521828\n",
      "2025-03-08 00:13: Train Epoch 244: 20/23 Loss: 79.521828\n",
      "2025-03-08 00:13: Train Epoch 244: 20/23 Loss: 79.521828\n",
      "2025-03-08 00:13: **********Train Epoch 244: averaged Loss: 47.963186\n",
      "2025-03-08 00:13: **********Train Epoch 244: averaged Loss: 47.963186\n",
      "2025-03-08 00:13: **********Train Epoch 244: averaged Loss: 47.963186\n",
      "2025-03-08 00:13: **********Val Epoch 244: average Loss: 2418.058044\n",
      "2025-03-08 00:13: **********Val Epoch 244: average Loss: 2418.058044\n",
      "2025-03-08 00:13: **********Val Epoch 244: average Loss: 2418.058044\n",
      "2025-03-08 00:13: Train Epoch 245: 0/23 Loss: 2.291165\n",
      "2025-03-08 00:13: Train Epoch 245: 0/23 Loss: 2.291165\n",
      "2025-03-08 00:13: Train Epoch 245: 0/23 Loss: 2.291165\n",
      "2025-03-08 00:13: Train Epoch 245: 20/23 Loss: 210.125366\n",
      "2025-03-08 00:13: Train Epoch 245: 20/23 Loss: 210.125366\n",
      "2025-03-08 00:13: Train Epoch 245: 20/23 Loss: 210.125366\n",
      "2025-03-08 00:13: **********Train Epoch 245: averaged Loss: 58.861586\n",
      "2025-03-08 00:13: **********Train Epoch 245: averaged Loss: 58.861586\n",
      "2025-03-08 00:13: **********Train Epoch 245: averaged Loss: 58.861586\n",
      "2025-03-08 00:13: **********Val Epoch 245: average Loss: 2398.527649\n",
      "2025-03-08 00:13: **********Val Epoch 245: average Loss: 2398.527649\n",
      "2025-03-08 00:13: **********Val Epoch 245: average Loss: 2398.527649\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 246: 0/23 Loss: 7.909436\n",
      "2025-03-08 00:13: Train Epoch 246: 0/23 Loss: 7.909436\n",
      "2025-03-08 00:13: Train Epoch 246: 0/23 Loss: 7.909436\n",
      "2025-03-08 00:13: Train Epoch 246: 20/23 Loss: 108.110443\n",
      "2025-03-08 00:13: Train Epoch 246: 20/23 Loss: 108.110443\n",
      "2025-03-08 00:13: Train Epoch 246: 20/23 Loss: 108.110443\n",
      "2025-03-08 00:13: **********Train Epoch 246: averaged Loss: 63.914595\n",
      "2025-03-08 00:13: **********Train Epoch 246: averaged Loss: 63.914595\n",
      "2025-03-08 00:13: **********Train Epoch 246: averaged Loss: 63.914595\n",
      "2025-03-08 00:13: **********Val Epoch 246: average Loss: 2409.141296\n",
      "2025-03-08 00:13: **********Val Epoch 246: average Loss: 2409.141296\n",
      "2025-03-08 00:13: **********Val Epoch 246: average Loss: 2409.141296\n",
      "2025-03-08 00:13: Train Epoch 247: 0/23 Loss: 1.380386\n",
      "2025-03-08 00:13: Train Epoch 247: 0/23 Loss: 1.380386\n",
      "2025-03-08 00:13: Train Epoch 247: 0/23 Loss: 1.380386\n",
      "2025-03-08 00:13: Train Epoch 247: 20/23 Loss: 78.531937\n",
      "2025-03-08 00:13: Train Epoch 247: 20/23 Loss: 78.531937\n",
      "2025-03-08 00:13: Train Epoch 247: 20/23 Loss: 78.531937\n",
      "2025-03-08 00:13: **********Train Epoch 247: averaged Loss: 46.321616\n",
      "2025-03-08 00:13: **********Train Epoch 247: averaged Loss: 46.321616\n",
      "2025-03-08 00:13: **********Train Epoch 247: averaged Loss: 46.321616\n",
      "2025-03-08 00:13: **********Val Epoch 247: average Loss: 2373.980713\n",
      "2025-03-08 00:13: **********Val Epoch 247: average Loss: 2373.980713\n",
      "2025-03-08 00:13: **********Val Epoch 247: average Loss: 2373.980713\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 248: 0/23 Loss: 12.918119\n",
      "2025-03-08 00:13: Train Epoch 248: 0/23 Loss: 12.918119\n",
      "2025-03-08 00:13: Train Epoch 248: 0/23 Loss: 12.918119\n",
      "2025-03-08 00:13: Train Epoch 248: 20/23 Loss: 104.797173\n",
      "2025-03-08 00:13: Train Epoch 248: 20/23 Loss: 104.797173\n",
      "2025-03-08 00:13: Train Epoch 248: 20/23 Loss: 104.797173\n",
      "2025-03-08 00:13: **********Train Epoch 248: averaged Loss: 47.133425\n",
      "2025-03-08 00:13: **********Train Epoch 248: averaged Loss: 47.133425\n",
      "2025-03-08 00:13: **********Train Epoch 248: averaged Loss: 47.133425\n",
      "2025-03-08 00:13: **********Val Epoch 248: average Loss: 2380.529114\n",
      "2025-03-08 00:13: **********Val Epoch 248: average Loss: 2380.529114\n",
      "2025-03-08 00:13: **********Val Epoch 248: average Loss: 2380.529114\n",
      "2025-03-08 00:13: Train Epoch 249: 0/23 Loss: 1.903811\n",
      "2025-03-08 00:13: Train Epoch 249: 0/23 Loss: 1.903811\n",
      "2025-03-08 00:13: Train Epoch 249: 0/23 Loss: 1.903811\n",
      "2025-03-08 00:13: Train Epoch 249: 20/23 Loss: 76.879852\n",
      "2025-03-08 00:13: Train Epoch 249: 20/23 Loss: 76.879852\n",
      "2025-03-08 00:13: Train Epoch 249: 20/23 Loss: 76.879852\n",
      "2025-03-08 00:13: **********Train Epoch 249: averaged Loss: 53.450537\n",
      "2025-03-08 00:13: **********Train Epoch 249: averaged Loss: 53.450537\n",
      "2025-03-08 00:13: **********Train Epoch 249: averaged Loss: 53.450537\n",
      "2025-03-08 00:13: **********Val Epoch 249: average Loss: 2367.425659\n",
      "2025-03-08 00:13: **********Val Epoch 249: average Loss: 2367.425659\n",
      "2025-03-08 00:13: **********Val Epoch 249: average Loss: 2367.425659\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 250: 0/23 Loss: 7.964902\n",
      "2025-03-08 00:13: Train Epoch 250: 0/23 Loss: 7.964902\n",
      "2025-03-08 00:13: Train Epoch 250: 0/23 Loss: 7.964902\n",
      "2025-03-08 00:13: Train Epoch 250: 20/23 Loss: 81.478043\n",
      "2025-03-08 00:13: Train Epoch 250: 20/23 Loss: 81.478043\n",
      "2025-03-08 00:13: Train Epoch 250: 20/23 Loss: 81.478043\n",
      "2025-03-08 00:13: **********Train Epoch 250: averaged Loss: 45.705873\n",
      "2025-03-08 00:13: **********Train Epoch 250: averaged Loss: 45.705873\n",
      "2025-03-08 00:13: **********Train Epoch 250: averaged Loss: 45.705873\n",
      "2025-03-08 00:13: **********Val Epoch 250: average Loss: 2399.882263\n",
      "2025-03-08 00:13: **********Val Epoch 250: average Loss: 2399.882263\n",
      "2025-03-08 00:13: **********Val Epoch 250: average Loss: 2399.882263\n",
      "2025-03-08 00:13: Train Epoch 251: 0/23 Loss: 8.674816\n",
      "2025-03-08 00:13: Train Epoch 251: 0/23 Loss: 8.674816\n",
      "2025-03-08 00:13: Train Epoch 251: 0/23 Loss: 8.674816\n",
      "2025-03-08 00:13: Train Epoch 251: 20/23 Loss: 73.963371\n",
      "2025-03-08 00:13: Train Epoch 251: 20/23 Loss: 73.963371\n",
      "2025-03-08 00:13: Train Epoch 251: 20/23 Loss: 73.963371\n",
      "2025-03-08 00:13: **********Train Epoch 251: averaged Loss: 47.896068\n",
      "2025-03-08 00:13: **********Train Epoch 251: averaged Loss: 47.896068\n",
      "2025-03-08 00:13: **********Train Epoch 251: averaged Loss: 47.896068\n",
      "2025-03-08 00:13: **********Val Epoch 251: average Loss: 2361.338379\n",
      "2025-03-08 00:13: **********Val Epoch 251: average Loss: 2361.338379\n",
      "2025-03-08 00:13: **********Val Epoch 251: average Loss: 2361.338379\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 252: 0/23 Loss: 3.444095\n",
      "2025-03-08 00:13: Train Epoch 252: 0/23 Loss: 3.444095\n",
      "2025-03-08 00:13: Train Epoch 252: 0/23 Loss: 3.444095\n",
      "2025-03-08 00:13: Train Epoch 252: 20/23 Loss: 138.793015\n",
      "2025-03-08 00:13: Train Epoch 252: 20/23 Loss: 138.793015\n",
      "2025-03-08 00:13: Train Epoch 252: 20/23 Loss: 138.793015\n",
      "2025-03-08 00:13: **********Train Epoch 252: averaged Loss: 51.568878\n",
      "2025-03-08 00:13: **********Train Epoch 252: averaged Loss: 51.568878\n",
      "2025-03-08 00:13: **********Train Epoch 252: averaged Loss: 51.568878\n",
      "2025-03-08 00:13: **********Val Epoch 252: average Loss: 2350.763306\n",
      "2025-03-08 00:13: **********Val Epoch 252: average Loss: 2350.763306\n",
      "2025-03-08 00:13: **********Val Epoch 252: average Loss: 2350.763306\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 253: 0/23 Loss: 12.083567\n",
      "2025-03-08 00:13: Train Epoch 253: 0/23 Loss: 12.083567\n",
      "2025-03-08 00:13: Train Epoch 253: 0/23 Loss: 12.083567\n",
      "2025-03-08 00:13: Train Epoch 253: 20/23 Loss: 90.054810\n",
      "2025-03-08 00:13: Train Epoch 253: 20/23 Loss: 90.054810\n",
      "2025-03-08 00:13: Train Epoch 253: 20/23 Loss: 90.054810\n",
      "2025-03-08 00:13: **********Train Epoch 253: averaged Loss: 67.879791\n",
      "2025-03-08 00:13: **********Train Epoch 253: averaged Loss: 67.879791\n",
      "2025-03-08 00:13: **********Train Epoch 253: averaged Loss: 67.879791\n",
      "2025-03-08 00:13: **********Val Epoch 253: average Loss: 2366.798584\n",
      "2025-03-08 00:13: **********Val Epoch 253: average Loss: 2366.798584\n",
      "2025-03-08 00:13: **********Val Epoch 253: average Loss: 2366.798584\n",
      "2025-03-08 00:13: Train Epoch 254: 0/23 Loss: 2.685823\n",
      "2025-03-08 00:13: Train Epoch 254: 0/23 Loss: 2.685823\n",
      "2025-03-08 00:13: Train Epoch 254: 0/23 Loss: 2.685823\n",
      "2025-03-08 00:13: Train Epoch 254: 20/23 Loss: 80.583633\n",
      "2025-03-08 00:13: Train Epoch 254: 20/23 Loss: 80.583633\n",
      "2025-03-08 00:13: Train Epoch 254: 20/23 Loss: 80.583633\n",
      "2025-03-08 00:13: **********Train Epoch 254: averaged Loss: 48.036022\n",
      "2025-03-08 00:13: **********Train Epoch 254: averaged Loss: 48.036022\n",
      "2025-03-08 00:13: **********Train Epoch 254: averaged Loss: 48.036022\n",
      "2025-03-08 00:13: **********Val Epoch 254: average Loss: 2367.989868\n",
      "2025-03-08 00:13: **********Val Epoch 254: average Loss: 2367.989868\n",
      "2025-03-08 00:13: **********Val Epoch 254: average Loss: 2367.989868\n",
      "2025-03-08 00:13: Train Epoch 255: 0/23 Loss: 15.746864\n",
      "2025-03-08 00:13: Train Epoch 255: 0/23 Loss: 15.746864\n",
      "2025-03-08 00:13: Train Epoch 255: 0/23 Loss: 15.746864\n",
      "2025-03-08 00:13: Train Epoch 255: 20/23 Loss: 85.221970\n",
      "2025-03-08 00:13: Train Epoch 255: 20/23 Loss: 85.221970\n",
      "2025-03-08 00:13: Train Epoch 255: 20/23 Loss: 85.221970\n",
      "2025-03-08 00:13: **********Train Epoch 255: averaged Loss: 45.294059\n",
      "2025-03-08 00:13: **********Train Epoch 255: averaged Loss: 45.294059\n",
      "2025-03-08 00:13: **********Train Epoch 255: averaged Loss: 45.294059\n",
      "2025-03-08 00:13: **********Val Epoch 255: average Loss: 2352.242188\n",
      "2025-03-08 00:13: **********Val Epoch 255: average Loss: 2352.242188\n",
      "2025-03-08 00:13: **********Val Epoch 255: average Loss: 2352.242188\n",
      "2025-03-08 00:13: Train Epoch 256: 0/23 Loss: 2.585356\n",
      "2025-03-08 00:13: Train Epoch 256: 0/23 Loss: 2.585356\n",
      "2025-03-08 00:13: Train Epoch 256: 0/23 Loss: 2.585356\n",
      "2025-03-08 00:13: Train Epoch 256: 20/23 Loss: 80.778915\n",
      "2025-03-08 00:13: Train Epoch 256: 20/23 Loss: 80.778915\n",
      "2025-03-08 00:13: Train Epoch 256: 20/23 Loss: 80.778915\n",
      "2025-03-08 00:13: **********Train Epoch 256: averaged Loss: 42.428323\n",
      "2025-03-08 00:13: **********Train Epoch 256: averaged Loss: 42.428323\n",
      "2025-03-08 00:13: **********Train Epoch 256: averaged Loss: 42.428323\n",
      "2025-03-08 00:13: **********Val Epoch 256: average Loss: 2357.371826\n",
      "2025-03-08 00:13: **********Val Epoch 256: average Loss: 2357.371826\n",
      "2025-03-08 00:13: **********Val Epoch 256: average Loss: 2357.371826\n",
      "2025-03-08 00:13: Train Epoch 257: 0/23 Loss: 13.699762\n",
      "2025-03-08 00:13: Train Epoch 257: 0/23 Loss: 13.699762\n",
      "2025-03-08 00:13: Train Epoch 257: 0/23 Loss: 13.699762\n",
      "2025-03-08 00:13: Train Epoch 257: 20/23 Loss: 153.329376\n",
      "2025-03-08 00:13: Train Epoch 257: 20/23 Loss: 153.329376\n",
      "2025-03-08 00:13: Train Epoch 257: 20/23 Loss: 153.329376\n",
      "2025-03-08 00:13: **********Train Epoch 257: averaged Loss: 48.822035\n",
      "2025-03-08 00:13: **********Train Epoch 257: averaged Loss: 48.822035\n",
      "2025-03-08 00:13: **********Train Epoch 257: averaged Loss: 48.822035\n",
      "2025-03-08 00:13: **********Val Epoch 257: average Loss: 2347.340210\n",
      "2025-03-08 00:13: **********Val Epoch 257: average Loss: 2347.340210\n",
      "2025-03-08 00:13: **********Val Epoch 257: average Loss: 2347.340210\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 258: 0/23 Loss: 0.867754\n",
      "2025-03-08 00:13: Train Epoch 258: 0/23 Loss: 0.867754\n",
      "2025-03-08 00:13: Train Epoch 258: 0/23 Loss: 0.867754\n",
      "2025-03-08 00:13: Train Epoch 258: 20/23 Loss: 108.167747\n",
      "2025-03-08 00:13: Train Epoch 258: 20/23 Loss: 108.167747\n",
      "2025-03-08 00:13: Train Epoch 258: 20/23 Loss: 108.167747\n",
      "2025-03-08 00:13: **********Train Epoch 258: averaged Loss: 70.147163\n",
      "2025-03-08 00:13: **********Train Epoch 258: averaged Loss: 70.147163\n",
      "2025-03-08 00:13: **********Train Epoch 258: averaged Loss: 70.147163\n",
      "2025-03-08 00:13: **********Val Epoch 258: average Loss: 2347.959534\n",
      "2025-03-08 00:13: **********Val Epoch 258: average Loss: 2347.959534\n",
      "2025-03-08 00:13: **********Val Epoch 258: average Loss: 2347.959534\n",
      "2025-03-08 00:13: Train Epoch 259: 0/23 Loss: 4.246511\n",
      "2025-03-08 00:13: Train Epoch 259: 0/23 Loss: 4.246511\n",
      "2025-03-08 00:13: Train Epoch 259: 0/23 Loss: 4.246511\n",
      "2025-03-08 00:13: Train Epoch 259: 20/23 Loss: 72.741669\n",
      "2025-03-08 00:13: Train Epoch 259: 20/23 Loss: 72.741669\n",
      "2025-03-08 00:13: Train Epoch 259: 20/23 Loss: 72.741669\n",
      "2025-03-08 00:13: **********Train Epoch 259: averaged Loss: 43.377526\n",
      "2025-03-08 00:13: **********Train Epoch 259: averaged Loss: 43.377526\n",
      "2025-03-08 00:13: **********Train Epoch 259: averaged Loss: 43.377526\n",
      "2025-03-08 00:13: **********Val Epoch 259: average Loss: 2352.688538\n",
      "2025-03-08 00:13: **********Val Epoch 259: average Loss: 2352.688538\n",
      "2025-03-08 00:13: **********Val Epoch 259: average Loss: 2352.688538\n",
      "2025-03-08 00:13: Train Epoch 260: 0/23 Loss: 9.253853\n",
      "2025-03-08 00:13: Train Epoch 260: 0/23 Loss: 9.253853\n",
      "2025-03-08 00:13: Train Epoch 260: 0/23 Loss: 9.253853\n",
      "2025-03-08 00:13: Train Epoch 260: 20/23 Loss: 100.164055\n",
      "2025-03-08 00:13: Train Epoch 260: 20/23 Loss: 100.164055\n",
      "2025-03-08 00:13: Train Epoch 260: 20/23 Loss: 100.164055\n",
      "2025-03-08 00:13: **********Train Epoch 260: averaged Loss: 41.308650\n",
      "2025-03-08 00:13: **********Train Epoch 260: averaged Loss: 41.308650\n",
      "2025-03-08 00:13: **********Train Epoch 260: averaged Loss: 41.308650\n",
      "2025-03-08 00:13: **********Val Epoch 260: average Loss: 2326.663208\n",
      "2025-03-08 00:13: **********Val Epoch 260: average Loss: 2326.663208\n",
      "2025-03-08 00:13: **********Val Epoch 260: average Loss: 2326.663208\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 261: 0/23 Loss: 1.033394\n",
      "2025-03-08 00:13: Train Epoch 261: 0/23 Loss: 1.033394\n",
      "2025-03-08 00:13: Train Epoch 261: 0/23 Loss: 1.033394\n",
      "2025-03-08 00:13: Train Epoch 261: 20/23 Loss: 81.712906\n",
      "2025-03-08 00:13: Train Epoch 261: 20/23 Loss: 81.712906\n",
      "2025-03-08 00:13: Train Epoch 261: 20/23 Loss: 81.712906\n",
      "2025-03-08 00:13: **********Train Epoch 261: averaged Loss: 42.432037\n",
      "2025-03-08 00:13: **********Train Epoch 261: averaged Loss: 42.432037\n",
      "2025-03-08 00:13: **********Train Epoch 261: averaged Loss: 42.432037\n",
      "2025-03-08 00:13: **********Val Epoch 261: average Loss: 2338.013672\n",
      "2025-03-08 00:13: **********Val Epoch 261: average Loss: 2338.013672\n",
      "2025-03-08 00:13: **********Val Epoch 261: average Loss: 2338.013672\n",
      "2025-03-08 00:13: Train Epoch 262: 0/23 Loss: 6.805558\n",
      "2025-03-08 00:13: Train Epoch 262: 0/23 Loss: 6.805558\n",
      "2025-03-08 00:13: Train Epoch 262: 0/23 Loss: 6.805558\n",
      "2025-03-08 00:13: Train Epoch 262: 20/23 Loss: 149.495499\n",
      "2025-03-08 00:13: Train Epoch 262: 20/23 Loss: 149.495499\n",
      "2025-03-08 00:13: Train Epoch 262: 20/23 Loss: 149.495499\n",
      "2025-03-08 00:13: **********Train Epoch 262: averaged Loss: 49.464656\n",
      "2025-03-08 00:13: **********Train Epoch 262: averaged Loss: 49.464656\n",
      "2025-03-08 00:13: **********Train Epoch 262: averaged Loss: 49.464656\n",
      "2025-03-08 00:13: **********Val Epoch 262: average Loss: 2338.409546\n",
      "2025-03-08 00:13: **********Val Epoch 262: average Loss: 2338.409546\n",
      "2025-03-08 00:13: **********Val Epoch 262: average Loss: 2338.409546\n",
      "2025-03-08 00:13: Train Epoch 263: 0/23 Loss: 7.617701\n",
      "2025-03-08 00:13: Train Epoch 263: 0/23 Loss: 7.617701\n",
      "2025-03-08 00:13: Train Epoch 263: 0/23 Loss: 7.617701\n",
      "2025-03-08 00:13: Train Epoch 263: 20/23 Loss: 110.838051\n",
      "2025-03-08 00:13: Train Epoch 263: 20/23 Loss: 110.838051\n",
      "2025-03-08 00:13: Train Epoch 263: 20/23 Loss: 110.838051\n",
      "2025-03-08 00:13: **********Train Epoch 263: averaged Loss: 66.051230\n",
      "2025-03-08 00:13: **********Train Epoch 263: averaged Loss: 66.051230\n",
      "2025-03-08 00:13: **********Train Epoch 263: averaged Loss: 66.051230\n",
      "2025-03-08 00:13: **********Val Epoch 263: average Loss: 2317.117737\n",
      "2025-03-08 00:13: **********Val Epoch 263: average Loss: 2317.117737\n",
      "2025-03-08 00:13: **********Val Epoch 263: average Loss: 2317.117737\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 264: 0/23 Loss: 2.857890\n",
      "2025-03-08 00:13: Train Epoch 264: 0/23 Loss: 2.857890\n",
      "2025-03-08 00:13: Train Epoch 264: 0/23 Loss: 2.857890\n",
      "2025-03-08 00:13: Train Epoch 264: 20/23 Loss: 76.075790\n",
      "2025-03-08 00:13: Train Epoch 264: 20/23 Loss: 76.075790\n",
      "2025-03-08 00:13: Train Epoch 264: 20/23 Loss: 76.075790\n",
      "2025-03-08 00:13: **********Train Epoch 264: averaged Loss: 43.927201\n",
      "2025-03-08 00:13: **********Train Epoch 264: averaged Loss: 43.927201\n",
      "2025-03-08 00:13: **********Train Epoch 264: averaged Loss: 43.927201\n",
      "2025-03-08 00:13: **********Val Epoch 264: average Loss: 2323.195557\n",
      "2025-03-08 00:13: **********Val Epoch 264: average Loss: 2323.195557\n",
      "2025-03-08 00:13: **********Val Epoch 264: average Loss: 2323.195557\n",
      "2025-03-08 00:13: Train Epoch 265: 0/23 Loss: 9.223488\n",
      "2025-03-08 00:13: Train Epoch 265: 0/23 Loss: 9.223488\n",
      "2025-03-08 00:13: Train Epoch 265: 0/23 Loss: 9.223488\n",
      "2025-03-08 00:13: Train Epoch 265: 20/23 Loss: 88.522354\n",
      "2025-03-08 00:13: Train Epoch 265: 20/23 Loss: 88.522354\n",
      "2025-03-08 00:13: Train Epoch 265: 20/23 Loss: 88.522354\n",
      "2025-03-08 00:13: **********Train Epoch 265: averaged Loss: 42.807148\n",
      "2025-03-08 00:13: **********Train Epoch 265: averaged Loss: 42.807148\n",
      "2025-03-08 00:13: **********Train Epoch 265: averaged Loss: 42.807148\n",
      "2025-03-08 00:13: **********Val Epoch 265: average Loss: 2324.123535\n",
      "2025-03-08 00:13: **********Val Epoch 265: average Loss: 2324.123535\n",
      "2025-03-08 00:13: **********Val Epoch 265: average Loss: 2324.123535\n",
      "2025-03-08 00:13: Train Epoch 266: 0/23 Loss: 1.850986\n",
      "2025-03-08 00:13: Train Epoch 266: 0/23 Loss: 1.850986\n",
      "2025-03-08 00:13: Train Epoch 266: 0/23 Loss: 1.850986\n",
      "2025-03-08 00:13: Train Epoch 266: 20/23 Loss: 85.324219\n",
      "2025-03-08 00:13: Train Epoch 266: 20/23 Loss: 85.324219\n",
      "2025-03-08 00:13: Train Epoch 266: 20/23 Loss: 85.324219\n",
      "2025-03-08 00:13: **********Train Epoch 266: averaged Loss: 38.029551\n",
      "2025-03-08 00:13: **********Train Epoch 266: averaged Loss: 38.029551\n",
      "2025-03-08 00:13: **********Train Epoch 266: averaged Loss: 38.029551\n",
      "2025-03-08 00:13: **********Val Epoch 266: average Loss: 2333.256836\n",
      "2025-03-08 00:13: **********Val Epoch 266: average Loss: 2333.256836\n",
      "2025-03-08 00:13: **********Val Epoch 266: average Loss: 2333.256836\n",
      "2025-03-08 00:13: Train Epoch 267: 0/23 Loss: 7.277335\n",
      "2025-03-08 00:13: Train Epoch 267: 0/23 Loss: 7.277335\n",
      "2025-03-08 00:13: Train Epoch 267: 0/23 Loss: 7.277335\n",
      "2025-03-08 00:13: Train Epoch 267: 20/23 Loss: 81.536018\n",
      "2025-03-08 00:13: Train Epoch 267: 20/23 Loss: 81.536018\n",
      "2025-03-08 00:13: Train Epoch 267: 20/23 Loss: 81.536018\n",
      "2025-03-08 00:13: **********Train Epoch 267: averaged Loss: 38.716663\n",
      "2025-03-08 00:13: **********Train Epoch 267: averaged Loss: 38.716663\n",
      "2025-03-08 00:13: **********Train Epoch 267: averaged Loss: 38.716663\n",
      "2025-03-08 00:13: **********Val Epoch 267: average Loss: 2329.962769\n",
      "2025-03-08 00:13: **********Val Epoch 267: average Loss: 2329.962769\n",
      "2025-03-08 00:13: **********Val Epoch 267: average Loss: 2329.962769\n",
      "2025-03-08 00:13: Train Epoch 268: 0/23 Loss: 5.494079\n",
      "2025-03-08 00:13: Train Epoch 268: 0/23 Loss: 5.494079\n",
      "2025-03-08 00:13: Train Epoch 268: 0/23 Loss: 5.494079\n",
      "2025-03-08 00:13: Train Epoch 268: 20/23 Loss: 99.251984\n",
      "2025-03-08 00:13: Train Epoch 268: 20/23 Loss: 99.251984\n",
      "2025-03-08 00:13: Train Epoch 268: 20/23 Loss: 99.251984\n",
      "2025-03-08 00:13: **********Train Epoch 268: averaged Loss: 40.500906\n",
      "2025-03-08 00:13: **********Train Epoch 268: averaged Loss: 40.500906\n",
      "2025-03-08 00:13: **********Train Epoch 268: averaged Loss: 40.500906\n",
      "2025-03-08 00:13: **********Val Epoch 268: average Loss: 2314.771179\n",
      "2025-03-08 00:13: **********Val Epoch 268: average Loss: 2314.771179\n",
      "2025-03-08 00:13: **********Val Epoch 268: average Loss: 2314.771179\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 269: 0/23 Loss: 5.644703\n",
      "2025-03-08 00:13: Train Epoch 269: 0/23 Loss: 5.644703\n",
      "2025-03-08 00:13: Train Epoch 269: 0/23 Loss: 5.644703\n",
      "2025-03-08 00:13: Train Epoch 269: 20/23 Loss: 93.976768\n",
      "2025-03-08 00:13: Train Epoch 269: 20/23 Loss: 93.976768\n",
      "2025-03-08 00:13: Train Epoch 269: 20/23 Loss: 93.976768\n",
      "2025-03-08 00:13: **********Train Epoch 269: averaged Loss: 53.160982\n",
      "2025-03-08 00:13: **********Train Epoch 269: averaged Loss: 53.160982\n",
      "2025-03-08 00:13: **********Train Epoch 269: averaged Loss: 53.160982\n",
      "2025-03-08 00:13: **********Val Epoch 269: average Loss: 2331.693542\n",
      "2025-03-08 00:13: **********Val Epoch 269: average Loss: 2331.693542\n",
      "2025-03-08 00:13: **********Val Epoch 269: average Loss: 2331.693542\n",
      "2025-03-08 00:13: Train Epoch 270: 0/23 Loss: 4.240228\n",
      "2025-03-08 00:13: Train Epoch 270: 0/23 Loss: 4.240228\n",
      "2025-03-08 00:13: Train Epoch 270: 0/23 Loss: 4.240228\n",
      "2025-03-08 00:13: Train Epoch 270: 20/23 Loss: 84.249771\n",
      "2025-03-08 00:13: Train Epoch 270: 20/23 Loss: 84.249771\n",
      "2025-03-08 00:13: Train Epoch 270: 20/23 Loss: 84.249771\n",
      "2025-03-08 00:13: **********Train Epoch 270: averaged Loss: 42.057457\n",
      "2025-03-08 00:13: **********Train Epoch 270: averaged Loss: 42.057457\n",
      "2025-03-08 00:13: **********Train Epoch 270: averaged Loss: 42.057457\n",
      "2025-03-08 00:13: **********Val Epoch 270: average Loss: 2300.399658\n",
      "2025-03-08 00:13: **********Val Epoch 270: average Loss: 2300.399658\n",
      "2025-03-08 00:13: **********Val Epoch 270: average Loss: 2300.399658\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 271: 0/23 Loss: 9.636125\n",
      "2025-03-08 00:13: Train Epoch 271: 0/23 Loss: 9.636125\n",
      "2025-03-08 00:13: Train Epoch 271: 0/23 Loss: 9.636125\n",
      "2025-03-08 00:13: Train Epoch 271: 20/23 Loss: 85.337837\n",
      "2025-03-08 00:13: Train Epoch 271: 20/23 Loss: 85.337837\n",
      "2025-03-08 00:13: Train Epoch 271: 20/23 Loss: 85.337837\n",
      "2025-03-08 00:13: **********Train Epoch 271: averaged Loss: 42.653519\n",
      "2025-03-08 00:13: **********Train Epoch 271: averaged Loss: 42.653519\n",
      "2025-03-08 00:13: **********Train Epoch 271: averaged Loss: 42.653519\n",
      "2025-03-08 00:13: **********Val Epoch 271: average Loss: 2291.702515\n",
      "2025-03-08 00:13: **********Val Epoch 271: average Loss: 2291.702515\n",
      "2025-03-08 00:13: **********Val Epoch 271: average Loss: 2291.702515\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: *********************************Current best model saved!\n",
      "2025-03-08 00:13: Train Epoch 272: 0/23 Loss: 6.287146\n",
      "2025-03-08 00:13: Train Epoch 272: 0/23 Loss: 6.287146\n",
      "2025-03-08 00:13: Train Epoch 272: 0/23 Loss: 6.287146\n",
      "2025-03-08 00:13: Train Epoch 272: 20/23 Loss: 157.678818\n",
      "2025-03-08 00:13: Train Epoch 272: 20/23 Loss: 157.678818\n",
      "2025-03-08 00:13: Train Epoch 272: 20/23 Loss: 157.678818\n",
      "2025-03-08 00:13: **********Train Epoch 272: averaged Loss: 49.339500\n",
      "2025-03-08 00:13: **********Train Epoch 272: averaged Loss: 49.339500\n",
      "2025-03-08 00:13: **********Train Epoch 272: averaged Loss: 49.339500\n",
      "2025-03-08 00:13: **********Val Epoch 272: average Loss: 2306.997192\n",
      "2025-03-08 00:13: **********Val Epoch 272: average Loss: 2306.997192\n",
      "2025-03-08 00:13: **********Val Epoch 272: average Loss: 2306.997192\n",
      "2025-03-08 00:13: Train Epoch 273: 0/23 Loss: 13.882929\n",
      "2025-03-08 00:13: Train Epoch 273: 0/23 Loss: 13.882929\n",
      "2025-03-08 00:13: Train Epoch 273: 0/23 Loss: 13.882929\n",
      "2025-03-08 00:13: Train Epoch 273: 20/23 Loss: 117.721420\n",
      "2025-03-08 00:13: Train Epoch 273: 20/23 Loss: 117.721420\n",
      "2025-03-08 00:13: Train Epoch 273: 20/23 Loss: 117.721420\n",
      "2025-03-08 00:14: **********Train Epoch 273: averaged Loss: 72.387337\n",
      "2025-03-08 00:14: **********Train Epoch 273: averaged Loss: 72.387337\n",
      "2025-03-08 00:14: **********Train Epoch 273: averaged Loss: 72.387337\n",
      "2025-03-08 00:14: **********Val Epoch 273: average Loss: 2303.495911\n",
      "2025-03-08 00:14: **********Val Epoch 273: average Loss: 2303.495911\n",
      "2025-03-08 00:14: **********Val Epoch 273: average Loss: 2303.495911\n",
      "2025-03-08 00:14: Train Epoch 274: 0/23 Loss: 4.058992\n",
      "2025-03-08 00:14: Train Epoch 274: 0/23 Loss: 4.058992\n",
      "2025-03-08 00:14: Train Epoch 274: 0/23 Loss: 4.058992\n",
      "2025-03-08 00:14: Train Epoch 274: 20/23 Loss: 81.891144\n",
      "2025-03-08 00:14: Train Epoch 274: 20/23 Loss: 81.891144\n",
      "2025-03-08 00:14: Train Epoch 274: 20/23 Loss: 81.891144\n",
      "2025-03-08 00:14: **********Train Epoch 274: averaged Loss: 44.596574\n",
      "2025-03-08 00:14: **********Train Epoch 274: averaged Loss: 44.596574\n",
      "2025-03-08 00:14: **********Train Epoch 274: averaged Loss: 44.596574\n",
      "2025-03-08 00:14: **********Val Epoch 274: average Loss: 2297.899719\n",
      "2025-03-08 00:14: **********Val Epoch 274: average Loss: 2297.899719\n",
      "2025-03-08 00:14: **********Val Epoch 274: average Loss: 2297.899719\n",
      "2025-03-08 00:14: Train Epoch 275: 0/23 Loss: 12.218653\n",
      "2025-03-08 00:14: Train Epoch 275: 0/23 Loss: 12.218653\n",
      "2025-03-08 00:14: Train Epoch 275: 0/23 Loss: 12.218653\n",
      "2025-03-08 00:14: Train Epoch 275: 20/23 Loss: 92.107643\n",
      "2025-03-08 00:14: Train Epoch 275: 20/23 Loss: 92.107643\n",
      "2025-03-08 00:14: Train Epoch 275: 20/23 Loss: 92.107643\n",
      "2025-03-08 00:14: **********Train Epoch 275: averaged Loss: 40.511108\n",
      "2025-03-08 00:14: **********Train Epoch 275: averaged Loss: 40.511108\n",
      "2025-03-08 00:14: **********Train Epoch 275: averaged Loss: 40.511108\n",
      "2025-03-08 00:14: **********Val Epoch 275: average Loss: 2307.143921\n",
      "2025-03-08 00:14: **********Val Epoch 275: average Loss: 2307.143921\n",
      "2025-03-08 00:14: **********Val Epoch 275: average Loss: 2307.143921\n",
      "2025-03-08 00:14: Train Epoch 276: 0/23 Loss: 1.279932\n",
      "2025-03-08 00:14: Train Epoch 276: 0/23 Loss: 1.279932\n",
      "2025-03-08 00:14: Train Epoch 276: 0/23 Loss: 1.279932\n",
      "2025-03-08 00:14: Train Epoch 276: 20/23 Loss: 86.782669\n",
      "2025-03-08 00:14: Train Epoch 276: 20/23 Loss: 86.782669\n",
      "2025-03-08 00:14: Train Epoch 276: 20/23 Loss: 86.782669\n",
      "2025-03-08 00:14: **********Train Epoch 276: averaged Loss: 38.164355\n",
      "2025-03-08 00:14: **********Train Epoch 276: averaged Loss: 38.164355\n",
      "2025-03-08 00:14: **********Train Epoch 276: averaged Loss: 38.164355\n",
      "2025-03-08 00:14: **********Val Epoch 276: average Loss: 2300.123901\n",
      "2025-03-08 00:14: **********Val Epoch 276: average Loss: 2300.123901\n",
      "2025-03-08 00:14: **********Val Epoch 276: average Loss: 2300.123901\n",
      "2025-03-08 00:14: Train Epoch 277: 0/23 Loss: 6.883517\n",
      "2025-03-08 00:14: Train Epoch 277: 0/23 Loss: 6.883517\n",
      "2025-03-08 00:14: Train Epoch 277: 0/23 Loss: 6.883517\n",
      "2025-03-08 00:14: Train Epoch 277: 20/23 Loss: 149.924377\n",
      "2025-03-08 00:14: Train Epoch 277: 20/23 Loss: 149.924377\n",
      "2025-03-08 00:14: Train Epoch 277: 20/23 Loss: 149.924377\n",
      "2025-03-08 00:14: **********Train Epoch 277: averaged Loss: 44.328259\n",
      "2025-03-08 00:14: **********Train Epoch 277: averaged Loss: 44.328259\n",
      "2025-03-08 00:14: **********Train Epoch 277: averaged Loss: 44.328259\n",
      "2025-03-08 00:14: **********Val Epoch 277: average Loss: 2290.563110\n",
      "2025-03-08 00:14: **********Val Epoch 277: average Loss: 2290.563110\n",
      "2025-03-08 00:14: **********Val Epoch 277: average Loss: 2290.563110\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 278: 0/23 Loss: 5.964154\n",
      "2025-03-08 00:14: Train Epoch 278: 0/23 Loss: 5.964154\n",
      "2025-03-08 00:14: Train Epoch 278: 0/23 Loss: 5.964154\n",
      "2025-03-08 00:14: Train Epoch 278: 20/23 Loss: 98.322838\n",
      "2025-03-08 00:14: Train Epoch 278: 20/23 Loss: 98.322838\n",
      "2025-03-08 00:14: Train Epoch 278: 20/23 Loss: 98.322838\n",
      "2025-03-08 00:14: **********Train Epoch 278: averaged Loss: 65.772863\n",
      "2025-03-08 00:14: **********Train Epoch 278: averaged Loss: 65.772863\n",
      "2025-03-08 00:14: **********Train Epoch 278: averaged Loss: 65.772863\n",
      "2025-03-08 00:14: **********Val Epoch 278: average Loss: 2298.905090\n",
      "2025-03-08 00:14: **********Val Epoch 278: average Loss: 2298.905090\n",
      "2025-03-08 00:14: **********Val Epoch 278: average Loss: 2298.905090\n",
      "2025-03-08 00:14: Train Epoch 279: 0/23 Loss: 1.675771\n",
      "2025-03-08 00:14: Train Epoch 279: 0/23 Loss: 1.675771\n",
      "2025-03-08 00:14: Train Epoch 279: 0/23 Loss: 1.675771\n",
      "2025-03-08 00:14: Train Epoch 279: 20/23 Loss: 74.929199\n",
      "2025-03-08 00:14: Train Epoch 279: 20/23 Loss: 74.929199\n",
      "2025-03-08 00:14: Train Epoch 279: 20/23 Loss: 74.929199\n",
      "2025-03-08 00:14: **********Train Epoch 279: averaged Loss: 43.078190\n",
      "2025-03-08 00:14: **********Train Epoch 279: averaged Loss: 43.078190\n",
      "2025-03-08 00:14: **********Train Epoch 279: averaged Loss: 43.078190\n",
      "2025-03-08 00:14: **********Val Epoch 279: average Loss: 2289.180298\n",
      "2025-03-08 00:14: **********Val Epoch 279: average Loss: 2289.180298\n",
      "2025-03-08 00:14: **********Val Epoch 279: average Loss: 2289.180298\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 280: 0/23 Loss: 11.328438\n",
      "2025-03-08 00:14: Train Epoch 280: 0/23 Loss: 11.328438\n",
      "2025-03-08 00:14: Train Epoch 280: 0/23 Loss: 11.328438\n",
      "2025-03-08 00:14: Train Epoch 280: 20/23 Loss: 86.285858\n",
      "2025-03-08 00:14: Train Epoch 280: 20/23 Loss: 86.285858\n",
      "2025-03-08 00:14: Train Epoch 280: 20/23 Loss: 86.285858\n",
      "2025-03-08 00:14: **********Train Epoch 280: averaged Loss: 41.053720\n",
      "2025-03-08 00:14: **********Train Epoch 280: averaged Loss: 41.053720\n",
      "2025-03-08 00:14: **********Train Epoch 280: averaged Loss: 41.053720\n",
      "2025-03-08 00:14: **********Val Epoch 280: average Loss: 2283.581848\n",
      "2025-03-08 00:14: **********Val Epoch 280: average Loss: 2283.581848\n",
      "2025-03-08 00:14: **********Val Epoch 280: average Loss: 2283.581848\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 281: 0/23 Loss: 1.299492\n",
      "2025-03-08 00:14: Train Epoch 281: 0/23 Loss: 1.299492\n",
      "2025-03-08 00:14: Train Epoch 281: 0/23 Loss: 1.299492\n",
      "2025-03-08 00:14: Train Epoch 281: 20/23 Loss: 86.123848\n",
      "2025-03-08 00:14: Train Epoch 281: 20/23 Loss: 86.123848\n",
      "2025-03-08 00:14: Train Epoch 281: 20/23 Loss: 86.123848\n",
      "2025-03-08 00:14: **********Train Epoch 281: averaged Loss: 38.431792\n",
      "2025-03-08 00:14: **********Train Epoch 281: averaged Loss: 38.431792\n",
      "2025-03-08 00:14: **********Train Epoch 281: averaged Loss: 38.431792\n",
      "2025-03-08 00:14: **********Val Epoch 281: average Loss: 2288.405151\n",
      "2025-03-08 00:14: **********Val Epoch 281: average Loss: 2288.405151\n",
      "2025-03-08 00:14: **********Val Epoch 281: average Loss: 2288.405151\n",
      "2025-03-08 00:14: Train Epoch 282: 0/23 Loss: 1.024330\n",
      "2025-03-08 00:14: Train Epoch 282: 0/23 Loss: 1.024330\n",
      "2025-03-08 00:14: Train Epoch 282: 0/23 Loss: 1.024330\n",
      "2025-03-08 00:14: Train Epoch 282: 20/23 Loss: 103.647675\n",
      "2025-03-08 00:14: Train Epoch 282: 20/23 Loss: 103.647675\n",
      "2025-03-08 00:14: Train Epoch 282: 20/23 Loss: 103.647675\n",
      "2025-03-08 00:14: **********Train Epoch 282: averaged Loss: 38.614469\n",
      "2025-03-08 00:14: **********Train Epoch 282: averaged Loss: 38.614469\n",
      "2025-03-08 00:14: **********Train Epoch 282: averaged Loss: 38.614469\n",
      "2025-03-08 00:14: **********Val Epoch 282: average Loss: 2266.428589\n",
      "2025-03-08 00:14: **********Val Epoch 282: average Loss: 2266.428589\n",
      "2025-03-08 00:14: **********Val Epoch 282: average Loss: 2266.428589\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 283: 0/23 Loss: 11.932787\n",
      "2025-03-08 00:14: Train Epoch 283: 0/23 Loss: 11.932787\n",
      "2025-03-08 00:14: Train Epoch 283: 0/23 Loss: 11.932787\n",
      "2025-03-08 00:14: Train Epoch 283: 20/23 Loss: 122.577560\n",
      "2025-03-08 00:14: Train Epoch 283: 20/23 Loss: 122.577560\n",
      "2025-03-08 00:14: Train Epoch 283: 20/23 Loss: 122.577560\n",
      "2025-03-08 00:14: **********Train Epoch 283: averaged Loss: 49.850861\n",
      "2025-03-08 00:14: **********Train Epoch 283: averaged Loss: 49.850861\n",
      "2025-03-08 00:14: **********Train Epoch 283: averaged Loss: 49.850861\n",
      "2025-03-08 00:14: **********Val Epoch 283: average Loss: 2275.764526\n",
      "2025-03-08 00:14: **********Val Epoch 283: average Loss: 2275.764526\n",
      "2025-03-08 00:14: **********Val Epoch 283: average Loss: 2275.764526\n",
      "2025-03-08 00:14: Train Epoch 284: 0/23 Loss: 2.532642\n",
      "2025-03-08 00:14: Train Epoch 284: 0/23 Loss: 2.532642\n",
      "2025-03-08 00:14: Train Epoch 284: 0/23 Loss: 2.532642\n",
      "2025-03-08 00:14: Train Epoch 284: 20/23 Loss: 94.955093\n",
      "2025-03-08 00:14: Train Epoch 284: 20/23 Loss: 94.955093\n",
      "2025-03-08 00:14: Train Epoch 284: 20/23 Loss: 94.955093\n",
      "2025-03-08 00:14: **********Train Epoch 284: averaged Loss: 41.757506\n",
      "2025-03-08 00:14: **********Train Epoch 284: averaged Loss: 41.757506\n",
      "2025-03-08 00:14: **********Train Epoch 284: averaged Loss: 41.757506\n",
      "2025-03-08 00:14: **********Val Epoch 284: average Loss: 2307.898438\n",
      "2025-03-08 00:14: **********Val Epoch 284: average Loss: 2307.898438\n",
      "2025-03-08 00:14: **********Val Epoch 284: average Loss: 2307.898438\n",
      "2025-03-08 00:14: Train Epoch 285: 0/23 Loss: 12.133901\n",
      "2025-03-08 00:14: Train Epoch 285: 0/23 Loss: 12.133901\n",
      "2025-03-08 00:14: Train Epoch 285: 0/23 Loss: 12.133901\n",
      "2025-03-08 00:14: Train Epoch 285: 20/23 Loss: 84.873306\n",
      "2025-03-08 00:14: Train Epoch 285: 20/23 Loss: 84.873306\n",
      "2025-03-08 00:14: Train Epoch 285: 20/23 Loss: 84.873306\n",
      "2025-03-08 00:14: **********Train Epoch 285: averaged Loss: 45.979501\n",
      "2025-03-08 00:14: **********Train Epoch 285: averaged Loss: 45.979501\n",
      "2025-03-08 00:14: **********Train Epoch 285: averaged Loss: 45.979501\n",
      "2025-03-08 00:14: **********Val Epoch 285: average Loss: 2293.585449\n",
      "2025-03-08 00:14: **********Val Epoch 285: average Loss: 2293.585449\n",
      "2025-03-08 00:14: **********Val Epoch 285: average Loss: 2293.585449\n",
      "2025-03-08 00:14: Train Epoch 286: 0/23 Loss: 5.154758\n",
      "2025-03-08 00:14: Train Epoch 286: 0/23 Loss: 5.154758\n",
      "2025-03-08 00:14: Train Epoch 286: 0/23 Loss: 5.154758\n",
      "2025-03-08 00:14: Train Epoch 286: 20/23 Loss: 95.689758\n",
      "2025-03-08 00:14: Train Epoch 286: 20/23 Loss: 95.689758\n",
      "2025-03-08 00:14: Train Epoch 286: 20/23 Loss: 95.689758\n",
      "2025-03-08 00:14: **********Train Epoch 286: averaged Loss: 44.429818\n",
      "2025-03-08 00:14: **********Train Epoch 286: averaged Loss: 44.429818\n",
      "2025-03-08 00:14: **********Train Epoch 286: averaged Loss: 44.429818\n",
      "2025-03-08 00:14: **********Val Epoch 286: average Loss: 2299.569946\n",
      "2025-03-08 00:14: **********Val Epoch 286: average Loss: 2299.569946\n",
      "2025-03-08 00:14: **********Val Epoch 286: average Loss: 2299.569946\n",
      "2025-03-08 00:14: Train Epoch 287: 0/23 Loss: 11.031457\n",
      "2025-03-08 00:14: Train Epoch 287: 0/23 Loss: 11.031457\n",
      "2025-03-08 00:14: Train Epoch 287: 0/23 Loss: 11.031457\n",
      "2025-03-08 00:14: Train Epoch 287: 20/23 Loss: 128.308228\n",
      "2025-03-08 00:14: Train Epoch 287: 20/23 Loss: 128.308228\n",
      "2025-03-08 00:14: Train Epoch 287: 20/23 Loss: 128.308228\n",
      "2025-03-08 00:14: **********Train Epoch 287: averaged Loss: 56.885229\n",
      "2025-03-08 00:14: **********Train Epoch 287: averaged Loss: 56.885229\n",
      "2025-03-08 00:14: **********Train Epoch 287: averaged Loss: 56.885229\n",
      "2025-03-08 00:14: **********Val Epoch 287: average Loss: 2265.509277\n",
      "2025-03-08 00:14: **********Val Epoch 287: average Loss: 2265.509277\n",
      "2025-03-08 00:14: **********Val Epoch 287: average Loss: 2265.509277\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 288: 0/23 Loss: 2.422061\n",
      "2025-03-08 00:14: Train Epoch 288: 0/23 Loss: 2.422061\n",
      "2025-03-08 00:14: Train Epoch 288: 0/23 Loss: 2.422061\n",
      "2025-03-08 00:14: Train Epoch 288: 20/23 Loss: 82.111588\n",
      "2025-03-08 00:14: Train Epoch 288: 20/23 Loss: 82.111588\n",
      "2025-03-08 00:14: Train Epoch 288: 20/23 Loss: 82.111588\n",
      "2025-03-08 00:14: **********Train Epoch 288: averaged Loss: 40.971948\n",
      "2025-03-08 00:14: **********Train Epoch 288: averaged Loss: 40.971948\n",
      "2025-03-08 00:14: **********Train Epoch 288: averaged Loss: 40.971948\n",
      "2025-03-08 00:14: **********Val Epoch 288: average Loss: 2275.539246\n",
      "2025-03-08 00:14: **********Val Epoch 288: average Loss: 2275.539246\n",
      "2025-03-08 00:14: **********Val Epoch 288: average Loss: 2275.539246\n",
      "2025-03-08 00:14: Train Epoch 289: 0/23 Loss: 11.935658\n",
      "2025-03-08 00:14: Train Epoch 289: 0/23 Loss: 11.935658\n",
      "2025-03-08 00:14: Train Epoch 289: 0/23 Loss: 11.935658\n",
      "2025-03-08 00:14: Train Epoch 289: 20/23 Loss: 84.267609\n",
      "2025-03-08 00:14: Train Epoch 289: 20/23 Loss: 84.267609\n",
      "2025-03-08 00:14: Train Epoch 289: 20/23 Loss: 84.267609\n",
      "2025-03-08 00:14: **********Train Epoch 289: averaged Loss: 39.573644\n",
      "2025-03-08 00:14: **********Train Epoch 289: averaged Loss: 39.573644\n",
      "2025-03-08 00:14: **********Train Epoch 289: averaged Loss: 39.573644\n",
      "2025-03-08 00:14: **********Val Epoch 289: average Loss: 2299.780762\n",
      "2025-03-08 00:14: **********Val Epoch 289: average Loss: 2299.780762\n",
      "2025-03-08 00:14: **********Val Epoch 289: average Loss: 2299.780762\n",
      "2025-03-08 00:14: Train Epoch 290: 0/23 Loss: 1.480793\n",
      "2025-03-08 00:14: Train Epoch 290: 0/23 Loss: 1.480793\n",
      "2025-03-08 00:14: Train Epoch 290: 0/23 Loss: 1.480793\n",
      "2025-03-08 00:14: Train Epoch 290: 20/23 Loss: 92.167412\n",
      "2025-03-08 00:14: Train Epoch 290: 20/23 Loss: 92.167412\n",
      "2025-03-08 00:14: Train Epoch 290: 20/23 Loss: 92.167412\n",
      "2025-03-08 00:14: **********Train Epoch 290: averaged Loss: 37.517880\n",
      "2025-03-08 00:14: **********Train Epoch 290: averaged Loss: 37.517880\n",
      "2025-03-08 00:14: **********Train Epoch 290: averaged Loss: 37.517880\n",
      "2025-03-08 00:14: **********Val Epoch 290: average Loss: 2243.169067\n",
      "2025-03-08 00:14: **********Val Epoch 290: average Loss: 2243.169067\n",
      "2025-03-08 00:14: **********Val Epoch 290: average Loss: 2243.169067\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 291: 0/23 Loss: 7.745784\n",
      "2025-03-08 00:14: Train Epoch 291: 0/23 Loss: 7.745784\n",
      "2025-03-08 00:14: Train Epoch 291: 0/23 Loss: 7.745784\n",
      "2025-03-08 00:14: Train Epoch 291: 20/23 Loss: 165.206818\n",
      "2025-03-08 00:14: Train Epoch 291: 20/23 Loss: 165.206818\n",
      "2025-03-08 00:14: Train Epoch 291: 20/23 Loss: 165.206818\n",
      "2025-03-08 00:14: **********Train Epoch 291: averaged Loss: 47.294986\n",
      "2025-03-08 00:14: **********Train Epoch 291: averaged Loss: 47.294986\n",
      "2025-03-08 00:14: **********Train Epoch 291: averaged Loss: 47.294986\n",
      "2025-03-08 00:14: **********Val Epoch 291: average Loss: 2265.188049\n",
      "2025-03-08 00:14: **********Val Epoch 291: average Loss: 2265.188049\n",
      "2025-03-08 00:14: **********Val Epoch 291: average Loss: 2265.188049\n",
      "2025-03-08 00:14: Train Epoch 292: 0/23 Loss: 2.947348\n",
      "2025-03-08 00:14: Train Epoch 292: 0/23 Loss: 2.947348\n",
      "2025-03-08 00:14: Train Epoch 292: 0/23 Loss: 2.947348\n",
      "2025-03-08 00:14: Train Epoch 292: 20/23 Loss: 105.695343\n",
      "2025-03-08 00:14: Train Epoch 292: 20/23 Loss: 105.695343\n",
      "2025-03-08 00:14: Train Epoch 292: 20/23 Loss: 105.695343\n",
      "2025-03-08 00:14: **********Train Epoch 292: averaged Loss: 64.635259\n",
      "2025-03-08 00:14: **********Train Epoch 292: averaged Loss: 64.635259\n",
      "2025-03-08 00:14: **********Train Epoch 292: averaged Loss: 64.635259\n",
      "2025-03-08 00:14: **********Val Epoch 292: average Loss: 2251.358032\n",
      "2025-03-08 00:14: **********Val Epoch 292: average Loss: 2251.358032\n",
      "2025-03-08 00:14: **********Val Epoch 292: average Loss: 2251.358032\n",
      "2025-03-08 00:14: Train Epoch 293: 0/23 Loss: 2.805228\n",
      "2025-03-08 00:14: Train Epoch 293: 0/23 Loss: 2.805228\n",
      "2025-03-08 00:14: Train Epoch 293: 0/23 Loss: 2.805228\n",
      "2025-03-08 00:14: Train Epoch 293: 20/23 Loss: 80.315201\n",
      "2025-03-08 00:14: Train Epoch 293: 20/23 Loss: 80.315201\n",
      "2025-03-08 00:14: Train Epoch 293: 20/23 Loss: 80.315201\n",
      "2025-03-08 00:14: **********Train Epoch 293: averaged Loss: 42.282849\n",
      "2025-03-08 00:14: **********Train Epoch 293: averaged Loss: 42.282849\n",
      "2025-03-08 00:14: **********Train Epoch 293: averaged Loss: 42.282849\n",
      "2025-03-08 00:14: **********Val Epoch 293: average Loss: 2262.508484\n",
      "2025-03-08 00:14: **********Val Epoch 293: average Loss: 2262.508484\n",
      "2025-03-08 00:14: **********Val Epoch 293: average Loss: 2262.508484\n",
      "2025-03-08 00:14: Train Epoch 294: 0/23 Loss: 12.210675\n",
      "2025-03-08 00:14: Train Epoch 294: 0/23 Loss: 12.210675\n",
      "2025-03-08 00:14: Train Epoch 294: 0/23 Loss: 12.210675\n",
      "2025-03-08 00:14: Train Epoch 294: 20/23 Loss: 85.855911\n",
      "2025-03-08 00:14: Train Epoch 294: 20/23 Loss: 85.855911\n",
      "2025-03-08 00:14: Train Epoch 294: 20/23 Loss: 85.855911\n",
      "2025-03-08 00:14: **********Train Epoch 294: averaged Loss: 39.920995\n",
      "2025-03-08 00:14: **********Train Epoch 294: averaged Loss: 39.920995\n",
      "2025-03-08 00:14: **********Train Epoch 294: averaged Loss: 39.920995\n",
      "2025-03-08 00:14: **********Val Epoch 294: average Loss: 2248.118469\n",
      "2025-03-08 00:14: **********Val Epoch 294: average Loss: 2248.118469\n",
      "2025-03-08 00:14: **********Val Epoch 294: average Loss: 2248.118469\n",
      "2025-03-08 00:14: Train Epoch 295: 0/23 Loss: 1.302023\n",
      "2025-03-08 00:14: Train Epoch 295: 0/23 Loss: 1.302023\n",
      "2025-03-08 00:14: Train Epoch 295: 0/23 Loss: 1.302023\n",
      "2025-03-08 00:14: Train Epoch 295: 20/23 Loss: 92.365898\n",
      "2025-03-08 00:14: Train Epoch 295: 20/23 Loss: 92.365898\n",
      "2025-03-08 00:14: Train Epoch 295: 20/23 Loss: 92.365898\n",
      "2025-03-08 00:14: **********Train Epoch 295: averaged Loss: 37.242493\n",
      "2025-03-08 00:14: **********Train Epoch 295: averaged Loss: 37.242493\n",
      "2025-03-08 00:14: **********Train Epoch 295: averaged Loss: 37.242493\n",
      "2025-03-08 00:14: **********Val Epoch 295: average Loss: 2264.015259\n",
      "2025-03-08 00:14: **********Val Epoch 295: average Loss: 2264.015259\n",
      "2025-03-08 00:14: **********Val Epoch 295: average Loss: 2264.015259\n",
      "2025-03-08 00:14: Train Epoch 296: 0/23 Loss: 2.773741\n",
      "2025-03-08 00:14: Train Epoch 296: 0/23 Loss: 2.773741\n",
      "2025-03-08 00:14: Train Epoch 296: 0/23 Loss: 2.773741\n",
      "2025-03-08 00:14: Train Epoch 296: 20/23 Loss: 92.341774\n",
      "2025-03-08 00:14: Train Epoch 296: 20/23 Loss: 92.341774\n",
      "2025-03-08 00:14: Train Epoch 296: 20/23 Loss: 92.341774\n",
      "2025-03-08 00:14: **********Train Epoch 296: averaged Loss: 36.605700\n",
      "2025-03-08 00:14: **********Train Epoch 296: averaged Loss: 36.605700\n",
      "2025-03-08 00:14: **********Train Epoch 296: averaged Loss: 36.605700\n",
      "2025-03-08 00:14: **********Val Epoch 296: average Loss: 2264.501160\n",
      "2025-03-08 00:14: **********Val Epoch 296: average Loss: 2264.501160\n",
      "2025-03-08 00:14: **********Val Epoch 296: average Loss: 2264.501160\n",
      "2025-03-08 00:14: Train Epoch 297: 0/23 Loss: 3.408321\n",
      "2025-03-08 00:14: Train Epoch 297: 0/23 Loss: 3.408321\n",
      "2025-03-08 00:14: Train Epoch 297: 0/23 Loss: 3.408321\n",
      "2025-03-08 00:14: Train Epoch 297: 20/23 Loss: 168.672089\n",
      "2025-03-08 00:14: Train Epoch 297: 20/23 Loss: 168.672089\n",
      "2025-03-08 00:14: Train Epoch 297: 20/23 Loss: 168.672089\n",
      "2025-03-08 00:14: **********Train Epoch 297: averaged Loss: 47.992997\n",
      "2025-03-08 00:14: **********Train Epoch 297: averaged Loss: 47.992997\n",
      "2025-03-08 00:14: **********Train Epoch 297: averaged Loss: 47.992997\n",
      "2025-03-08 00:14: **********Val Epoch 297: average Loss: 2259.855713\n",
      "2025-03-08 00:14: **********Val Epoch 297: average Loss: 2259.855713\n",
      "2025-03-08 00:14: **********Val Epoch 297: average Loss: 2259.855713\n",
      "2025-03-08 00:14: Train Epoch 298: 0/23 Loss: 5.072103\n",
      "2025-03-08 00:14: Train Epoch 298: 0/23 Loss: 5.072103\n",
      "2025-03-08 00:14: Train Epoch 298: 0/23 Loss: 5.072103\n",
      "2025-03-08 00:14: Train Epoch 298: 20/23 Loss: 115.987938\n",
      "2025-03-08 00:14: Train Epoch 298: 20/23 Loss: 115.987938\n",
      "2025-03-08 00:14: Train Epoch 298: 20/23 Loss: 115.987938\n",
      "2025-03-08 00:14: **********Train Epoch 298: averaged Loss: 66.970134\n",
      "2025-03-08 00:14: **********Train Epoch 298: averaged Loss: 66.970134\n",
      "2025-03-08 00:14: **********Train Epoch 298: averaged Loss: 66.970134\n",
      "2025-03-08 00:14: **********Val Epoch 298: average Loss: 2276.586304\n",
      "2025-03-08 00:14: **********Val Epoch 298: average Loss: 2276.586304\n",
      "2025-03-08 00:14: **********Val Epoch 298: average Loss: 2276.586304\n",
      "2025-03-08 00:14: Train Epoch 299: 0/23 Loss: 5.898474\n",
      "2025-03-08 00:14: Train Epoch 299: 0/23 Loss: 5.898474\n",
      "2025-03-08 00:14: Train Epoch 299: 0/23 Loss: 5.898474\n",
      "2025-03-08 00:14: Train Epoch 299: 20/23 Loss: 78.834213\n",
      "2025-03-08 00:14: Train Epoch 299: 20/23 Loss: 78.834213\n",
      "2025-03-08 00:14: Train Epoch 299: 20/23 Loss: 78.834213\n",
      "2025-03-08 00:14: **********Train Epoch 299: averaged Loss: 42.060142\n",
      "2025-03-08 00:14: **********Train Epoch 299: averaged Loss: 42.060142\n",
      "2025-03-08 00:14: **********Train Epoch 299: averaged Loss: 42.060142\n",
      "2025-03-08 00:14: **********Val Epoch 299: average Loss: 2254.408081\n",
      "2025-03-08 00:14: **********Val Epoch 299: average Loss: 2254.408081\n",
      "2025-03-08 00:14: **********Val Epoch 299: average Loss: 2254.408081\n",
      "2025-03-08 00:14: Train Epoch 300: 0/23 Loss: 2.641832\n",
      "2025-03-08 00:14: Train Epoch 300: 0/23 Loss: 2.641832\n",
      "2025-03-08 00:14: Train Epoch 300: 0/23 Loss: 2.641832\n",
      "2025-03-08 00:14: Train Epoch 300: 20/23 Loss: 85.577515\n",
      "2025-03-08 00:14: Train Epoch 300: 20/23 Loss: 85.577515\n",
      "2025-03-08 00:14: Train Epoch 300: 20/23 Loss: 85.577515\n",
      "2025-03-08 00:14: **********Train Epoch 300: averaged Loss: 38.234741\n",
      "2025-03-08 00:14: **********Train Epoch 300: averaged Loss: 38.234741\n",
      "2025-03-08 00:14: **********Train Epoch 300: averaged Loss: 38.234741\n",
      "2025-03-08 00:14: **********Val Epoch 300: average Loss: 2268.727905\n",
      "2025-03-08 00:14: **********Val Epoch 300: average Loss: 2268.727905\n",
      "2025-03-08 00:14: **********Val Epoch 300: average Loss: 2268.727905\n",
      "2025-03-08 00:14: Train Epoch 301: 0/23 Loss: 1.135090\n",
      "2025-03-08 00:14: Train Epoch 301: 0/23 Loss: 1.135090\n",
      "2025-03-08 00:14: Train Epoch 301: 0/23 Loss: 1.135090\n",
      "2025-03-08 00:14: Train Epoch 301: 20/23 Loss: 97.797447\n",
      "2025-03-08 00:14: Train Epoch 301: 20/23 Loss: 97.797447\n",
      "2025-03-08 00:14: Train Epoch 301: 20/23 Loss: 97.797447\n",
      "2025-03-08 00:14: **********Train Epoch 301: averaged Loss: 37.436841\n",
      "2025-03-08 00:14: **********Train Epoch 301: averaged Loss: 37.436841\n",
      "2025-03-08 00:14: **********Train Epoch 301: averaged Loss: 37.436841\n",
      "2025-03-08 00:14: **********Val Epoch 301: average Loss: 2233.114014\n",
      "2025-03-08 00:14: **********Val Epoch 301: average Loss: 2233.114014\n",
      "2025-03-08 00:14: **********Val Epoch 301: average Loss: 2233.114014\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 302: 0/23 Loss: 5.969101\n",
      "2025-03-08 00:14: Train Epoch 302: 0/23 Loss: 5.969101\n",
      "2025-03-08 00:14: Train Epoch 302: 0/23 Loss: 5.969101\n",
      "2025-03-08 00:14: Train Epoch 302: 20/23 Loss: 108.934036\n",
      "2025-03-08 00:14: Train Epoch 302: 20/23 Loss: 108.934036\n",
      "2025-03-08 00:14: Train Epoch 302: 20/23 Loss: 108.934036\n",
      "2025-03-08 00:14: **********Train Epoch 302: averaged Loss: 40.573575\n",
      "2025-03-08 00:14: **********Train Epoch 302: averaged Loss: 40.573575\n",
      "2025-03-08 00:14: **********Train Epoch 302: averaged Loss: 40.573575\n",
      "2025-03-08 00:14: **********Val Epoch 302: average Loss: 2220.636475\n",
      "2025-03-08 00:14: **********Val Epoch 302: average Loss: 2220.636475\n",
      "2025-03-08 00:14: **********Val Epoch 302: average Loss: 2220.636475\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: Train Epoch 303: 0/23 Loss: 6.257516\n",
      "2025-03-08 00:14: Train Epoch 303: 0/23 Loss: 6.257516\n",
      "2025-03-08 00:14: Train Epoch 303: 0/23 Loss: 6.257516\n",
      "2025-03-08 00:14: Train Epoch 303: 20/23 Loss: 130.000229\n",
      "2025-03-08 00:14: Train Epoch 303: 20/23 Loss: 130.000229\n",
      "2025-03-08 00:14: Train Epoch 303: 20/23 Loss: 130.000229\n",
      "2025-03-08 00:14: **********Train Epoch 303: averaged Loss: 45.721706\n",
      "2025-03-08 00:14: **********Train Epoch 303: averaged Loss: 45.721706\n",
      "2025-03-08 00:14: **********Train Epoch 303: averaged Loss: 45.721706\n",
      "2025-03-08 00:14: **********Val Epoch 303: average Loss: 2247.022949\n",
      "2025-03-08 00:14: **********Val Epoch 303: average Loss: 2247.022949\n",
      "2025-03-08 00:14: **********Val Epoch 303: average Loss: 2247.022949\n",
      "2025-03-08 00:14: Train Epoch 304: 0/23 Loss: 1.233685\n",
      "2025-03-08 00:14: Train Epoch 304: 0/23 Loss: 1.233685\n",
      "2025-03-08 00:14: Train Epoch 304: 0/23 Loss: 1.233685\n",
      "2025-03-08 00:14: Train Epoch 304: 20/23 Loss: 111.130348\n",
      "2025-03-08 00:14: Train Epoch 304: 20/23 Loss: 111.130348\n",
      "2025-03-08 00:14: Train Epoch 304: 20/23 Loss: 111.130348\n",
      "2025-03-08 00:14: **********Train Epoch 304: averaged Loss: 65.870475\n",
      "2025-03-08 00:14: **********Train Epoch 304: averaged Loss: 65.870475\n",
      "2025-03-08 00:14: **********Train Epoch 304: averaged Loss: 65.870475\n",
      "2025-03-08 00:14: **********Val Epoch 304: average Loss: 2268.835327\n",
      "2025-03-08 00:14: **********Val Epoch 304: average Loss: 2268.835327\n",
      "2025-03-08 00:14: **********Val Epoch 304: average Loss: 2268.835327\n",
      "2025-03-08 00:14: Train Epoch 305: 0/23 Loss: 2.608081\n",
      "2025-03-08 00:14: Train Epoch 305: 0/23 Loss: 2.608081\n",
      "2025-03-08 00:14: Train Epoch 305: 0/23 Loss: 2.608081\n",
      "2025-03-08 00:14: Train Epoch 305: 20/23 Loss: 81.908012\n",
      "2025-03-08 00:14: Train Epoch 305: 20/23 Loss: 81.908012\n",
      "2025-03-08 00:14: Train Epoch 305: 20/23 Loss: 81.908012\n",
      "2025-03-08 00:14: **********Train Epoch 305: averaged Loss: 43.799420\n",
      "2025-03-08 00:14: **********Train Epoch 305: averaged Loss: 43.799420\n",
      "2025-03-08 00:14: **********Train Epoch 305: averaged Loss: 43.799420\n",
      "2025-03-08 00:14: **********Val Epoch 305: average Loss: 2257.359253\n",
      "2025-03-08 00:14: **********Val Epoch 305: average Loss: 2257.359253\n",
      "2025-03-08 00:14: **********Val Epoch 305: average Loss: 2257.359253\n",
      "2025-03-08 00:14: Train Epoch 306: 0/23 Loss: 6.471097\n",
      "2025-03-08 00:14: Train Epoch 306: 0/23 Loss: 6.471097\n",
      "2025-03-08 00:14: Train Epoch 306: 0/23 Loss: 6.471097\n",
      "2025-03-08 00:14: Train Epoch 306: 20/23 Loss: 82.681656\n",
      "2025-03-08 00:14: Train Epoch 306: 20/23 Loss: 82.681656\n",
      "2025-03-08 00:14: Train Epoch 306: 20/23 Loss: 82.681656\n",
      "2025-03-08 00:14: **********Train Epoch 306: averaged Loss: 38.980259\n",
      "2025-03-08 00:14: **********Train Epoch 306: averaged Loss: 38.980259\n",
      "2025-03-08 00:14: **********Train Epoch 306: averaged Loss: 38.980259\n",
      "2025-03-08 00:14: **********Val Epoch 306: average Loss: 2243.034119\n",
      "2025-03-08 00:14: **********Val Epoch 306: average Loss: 2243.034119\n",
      "2025-03-08 00:14: **********Val Epoch 306: average Loss: 2243.034119\n",
      "2025-03-08 00:14: Train Epoch 307: 0/23 Loss: 5.027462\n",
      "2025-03-08 00:14: Train Epoch 307: 0/23 Loss: 5.027462\n",
      "2025-03-08 00:14: Train Epoch 307: 0/23 Loss: 5.027462\n",
      "2025-03-08 00:14: Train Epoch 307: 20/23 Loss: 96.711395\n",
      "2025-03-08 00:14: Train Epoch 307: 20/23 Loss: 96.711395\n",
      "2025-03-08 00:14: Train Epoch 307: 20/23 Loss: 96.711395\n",
      "2025-03-08 00:14: **********Train Epoch 307: averaged Loss: 35.699221\n",
      "2025-03-08 00:14: **********Train Epoch 307: averaged Loss: 35.699221\n",
      "2025-03-08 00:14: **********Train Epoch 307: averaged Loss: 35.699221\n",
      "2025-03-08 00:14: **********Val Epoch 307: average Loss: 2225.930725\n",
      "2025-03-08 00:14: **********Val Epoch 307: average Loss: 2225.930725\n",
      "2025-03-08 00:14: **********Val Epoch 307: average Loss: 2225.930725\n",
      "2025-03-08 00:14: Train Epoch 308: 0/23 Loss: 2.848694\n",
      "2025-03-08 00:14: Train Epoch 308: 0/23 Loss: 2.848694\n",
      "2025-03-08 00:14: Train Epoch 308: 0/23 Loss: 2.848694\n",
      "2025-03-08 00:14: Train Epoch 308: 20/23 Loss: 93.918808\n",
      "2025-03-08 00:14: Train Epoch 308: 20/23 Loss: 93.918808\n",
      "2025-03-08 00:14: Train Epoch 308: 20/23 Loss: 93.918808\n",
      "2025-03-08 00:14: **********Train Epoch 308: averaged Loss: 35.699765\n",
      "2025-03-08 00:14: **********Train Epoch 308: averaged Loss: 35.699765\n",
      "2025-03-08 00:14: **********Train Epoch 308: averaged Loss: 35.699765\n",
      "2025-03-08 00:14: **********Val Epoch 308: average Loss: 2218.022583\n",
      "2025-03-08 00:14: **********Val Epoch 308: average Loss: 2218.022583\n",
      "2025-03-08 00:14: **********Val Epoch 308: average Loss: 2218.022583\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:14: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 309: 0/23 Loss: 1.047971\n",
      "2025-03-08 00:15: Train Epoch 309: 0/23 Loss: 1.047971\n",
      "2025-03-08 00:15: Train Epoch 309: 0/23 Loss: 1.047971\n",
      "2025-03-08 00:15: Train Epoch 309: 20/23 Loss: 102.710815\n",
      "2025-03-08 00:15: Train Epoch 309: 20/23 Loss: 102.710815\n",
      "2025-03-08 00:15: Train Epoch 309: 20/23 Loss: 102.710815\n",
      "2025-03-08 00:15: **********Train Epoch 309: averaged Loss: 38.153719\n",
      "2025-03-08 00:15: **********Train Epoch 309: averaged Loss: 38.153719\n",
      "2025-03-08 00:15: **********Train Epoch 309: averaged Loss: 38.153719\n",
      "2025-03-08 00:15: **********Val Epoch 309: average Loss: 2236.752747\n",
      "2025-03-08 00:15: **********Val Epoch 309: average Loss: 2236.752747\n",
      "2025-03-08 00:15: **********Val Epoch 309: average Loss: 2236.752747\n",
      "2025-03-08 00:15: Train Epoch 310: 0/23 Loss: 9.143314\n",
      "2025-03-08 00:15: Train Epoch 310: 0/23 Loss: 9.143314\n",
      "2025-03-08 00:15: Train Epoch 310: 0/23 Loss: 9.143314\n",
      "2025-03-08 00:15: Train Epoch 310: 20/23 Loss: 136.011597\n",
      "2025-03-08 00:15: Train Epoch 310: 20/23 Loss: 136.011597\n",
      "2025-03-08 00:15: Train Epoch 310: 20/23 Loss: 136.011597\n",
      "2025-03-08 00:15: **********Train Epoch 310: averaged Loss: 46.438400\n",
      "2025-03-08 00:15: **********Train Epoch 310: averaged Loss: 46.438400\n",
      "2025-03-08 00:15: **********Train Epoch 310: averaged Loss: 46.438400\n",
      "2025-03-08 00:15: **********Val Epoch 310: average Loss: 2240.747437\n",
      "2025-03-08 00:15: **********Val Epoch 310: average Loss: 2240.747437\n",
      "2025-03-08 00:15: **********Val Epoch 310: average Loss: 2240.747437\n",
      "2025-03-08 00:15: Train Epoch 311: 0/23 Loss: 1.597124\n",
      "2025-03-08 00:15: Train Epoch 311: 0/23 Loss: 1.597124\n",
      "2025-03-08 00:15: Train Epoch 311: 0/23 Loss: 1.597124\n",
      "2025-03-08 00:15: Train Epoch 311: 20/23 Loss: 95.533447\n",
      "2025-03-08 00:15: Train Epoch 311: 20/23 Loss: 95.533447\n",
      "2025-03-08 00:15: Train Epoch 311: 20/23 Loss: 95.533447\n",
      "2025-03-08 00:15: **********Train Epoch 311: averaged Loss: 62.937955\n",
      "2025-03-08 00:15: **********Train Epoch 311: averaged Loss: 62.937955\n",
      "2025-03-08 00:15: **********Train Epoch 311: averaged Loss: 62.937955\n",
      "2025-03-08 00:15: **********Val Epoch 311: average Loss: 2228.916443\n",
      "2025-03-08 00:15: **********Val Epoch 311: average Loss: 2228.916443\n",
      "2025-03-08 00:15: **********Val Epoch 311: average Loss: 2228.916443\n",
      "2025-03-08 00:15: Train Epoch 312: 0/23 Loss: 6.471051\n",
      "2025-03-08 00:15: Train Epoch 312: 0/23 Loss: 6.471051\n",
      "2025-03-08 00:15: Train Epoch 312: 0/23 Loss: 6.471051\n",
      "2025-03-08 00:15: Train Epoch 312: 20/23 Loss: 85.839310\n",
      "2025-03-08 00:15: Train Epoch 312: 20/23 Loss: 85.839310\n",
      "2025-03-08 00:15: Train Epoch 312: 20/23 Loss: 85.839310\n",
      "2025-03-08 00:15: **********Train Epoch 312: averaged Loss: 42.424587\n",
      "2025-03-08 00:15: **********Train Epoch 312: averaged Loss: 42.424587\n",
      "2025-03-08 00:15: **********Train Epoch 312: averaged Loss: 42.424587\n",
      "2025-03-08 00:15: **********Val Epoch 312: average Loss: 2214.529541\n",
      "2025-03-08 00:15: **********Val Epoch 312: average Loss: 2214.529541\n",
      "2025-03-08 00:15: **********Val Epoch 312: average Loss: 2214.529541\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 313: 0/23 Loss: 10.965971\n",
      "2025-03-08 00:15: Train Epoch 313: 0/23 Loss: 10.965971\n",
      "2025-03-08 00:15: Train Epoch 313: 0/23 Loss: 10.965971\n",
      "2025-03-08 00:15: Train Epoch 313: 20/23 Loss: 86.030258\n",
      "2025-03-08 00:15: Train Epoch 313: 20/23 Loss: 86.030258\n",
      "2025-03-08 00:15: Train Epoch 313: 20/23 Loss: 86.030258\n",
      "2025-03-08 00:15: **********Train Epoch 313: averaged Loss: 40.174628\n",
      "2025-03-08 00:15: **********Train Epoch 313: averaged Loss: 40.174628\n",
      "2025-03-08 00:15: **********Train Epoch 313: averaged Loss: 40.174628\n",
      "2025-03-08 00:15: **********Val Epoch 313: average Loss: 2232.942383\n",
      "2025-03-08 00:15: **********Val Epoch 313: average Loss: 2232.942383\n",
      "2025-03-08 00:15: **********Val Epoch 313: average Loss: 2232.942383\n",
      "2025-03-08 00:15: Train Epoch 314: 0/23 Loss: 1.964247\n",
      "2025-03-08 00:15: Train Epoch 314: 0/23 Loss: 1.964247\n",
      "2025-03-08 00:15: Train Epoch 314: 0/23 Loss: 1.964247\n",
      "2025-03-08 00:15: Train Epoch 314: 20/23 Loss: 93.292740\n",
      "2025-03-08 00:15: Train Epoch 314: 20/23 Loss: 93.292740\n",
      "2025-03-08 00:15: Train Epoch 314: 20/23 Loss: 93.292740\n",
      "2025-03-08 00:15: **********Train Epoch 314: averaged Loss: 35.768726\n",
      "2025-03-08 00:15: **********Train Epoch 314: averaged Loss: 35.768726\n",
      "2025-03-08 00:15: **********Train Epoch 314: averaged Loss: 35.768726\n",
      "2025-03-08 00:15: **********Val Epoch 314: average Loss: 2228.891602\n",
      "2025-03-08 00:15: **********Val Epoch 314: average Loss: 2228.891602\n",
      "2025-03-08 00:15: **********Val Epoch 314: average Loss: 2228.891602\n",
      "2025-03-08 00:15: Train Epoch 315: 0/23 Loss: 1.385717\n",
      "2025-03-08 00:15: Train Epoch 315: 0/23 Loss: 1.385717\n",
      "2025-03-08 00:15: Train Epoch 315: 0/23 Loss: 1.385717\n",
      "2025-03-08 00:15: Train Epoch 315: 20/23 Loss: 87.044502\n",
      "2025-03-08 00:15: Train Epoch 315: 20/23 Loss: 87.044502\n",
      "2025-03-08 00:15: Train Epoch 315: 20/23 Loss: 87.044502\n",
      "2025-03-08 00:15: **********Train Epoch 315: averaged Loss: 36.301532\n",
      "2025-03-08 00:15: **********Train Epoch 315: averaged Loss: 36.301532\n",
      "2025-03-08 00:15: **********Train Epoch 315: averaged Loss: 36.301532\n",
      "2025-03-08 00:15: **********Val Epoch 315: average Loss: 2221.206421\n",
      "2025-03-08 00:15: **********Val Epoch 315: average Loss: 2221.206421\n",
      "2025-03-08 00:15: **********Val Epoch 315: average Loss: 2221.206421\n",
      "2025-03-08 00:15: Train Epoch 316: 0/23 Loss: 3.104163\n",
      "2025-03-08 00:15: Train Epoch 316: 0/23 Loss: 3.104163\n",
      "2025-03-08 00:15: Train Epoch 316: 0/23 Loss: 3.104163\n",
      "2025-03-08 00:15: Train Epoch 316: 20/23 Loss: 113.464706\n",
      "2025-03-08 00:15: Train Epoch 316: 20/23 Loss: 113.464706\n",
      "2025-03-08 00:15: Train Epoch 316: 20/23 Loss: 113.464706\n",
      "2025-03-08 00:15: **********Train Epoch 316: averaged Loss: 40.231437\n",
      "2025-03-08 00:15: **********Train Epoch 316: averaged Loss: 40.231437\n",
      "2025-03-08 00:15: **********Train Epoch 316: averaged Loss: 40.231437\n",
      "2025-03-08 00:15: **********Val Epoch 316: average Loss: 2239.645264\n",
      "2025-03-08 00:15: **********Val Epoch 316: average Loss: 2239.645264\n",
      "2025-03-08 00:15: **********Val Epoch 316: average Loss: 2239.645264\n",
      "2025-03-08 00:15: Train Epoch 317: 0/23 Loss: 7.659845\n",
      "2025-03-08 00:15: Train Epoch 317: 0/23 Loss: 7.659845\n",
      "2025-03-08 00:15: Train Epoch 317: 0/23 Loss: 7.659845\n",
      "2025-03-08 00:15: Train Epoch 317: 20/23 Loss: 91.610802\n",
      "2025-03-08 00:15: Train Epoch 317: 20/23 Loss: 91.610802\n",
      "2025-03-08 00:15: Train Epoch 317: 20/23 Loss: 91.610802\n",
      "2025-03-08 00:15: **********Train Epoch 317: averaged Loss: 58.694437\n",
      "2025-03-08 00:15: **********Train Epoch 317: averaged Loss: 58.694437\n",
      "2025-03-08 00:15: **********Train Epoch 317: averaged Loss: 58.694437\n",
      "2025-03-08 00:15: **********Val Epoch 317: average Loss: 2209.797791\n",
      "2025-03-08 00:15: **********Val Epoch 317: average Loss: 2209.797791\n",
      "2025-03-08 00:15: **********Val Epoch 317: average Loss: 2209.797791\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 318: 0/23 Loss: 1.397952\n",
      "2025-03-08 00:15: Train Epoch 318: 0/23 Loss: 1.397952\n",
      "2025-03-08 00:15: Train Epoch 318: 0/23 Loss: 1.397952\n",
      "2025-03-08 00:15: Train Epoch 318: 20/23 Loss: 83.450363\n",
      "2025-03-08 00:15: Train Epoch 318: 20/23 Loss: 83.450363\n",
      "2025-03-08 00:15: Train Epoch 318: 20/23 Loss: 83.450363\n",
      "2025-03-08 00:15: **********Train Epoch 318: averaged Loss: 41.425929\n",
      "2025-03-08 00:15: **********Train Epoch 318: averaged Loss: 41.425929\n",
      "2025-03-08 00:15: **********Train Epoch 318: averaged Loss: 41.425929\n",
      "2025-03-08 00:15: **********Val Epoch 318: average Loss: 2227.809021\n",
      "2025-03-08 00:15: **********Val Epoch 318: average Loss: 2227.809021\n",
      "2025-03-08 00:15: **********Val Epoch 318: average Loss: 2227.809021\n",
      "2025-03-08 00:15: Train Epoch 319: 0/23 Loss: 11.947886\n",
      "2025-03-08 00:15: Train Epoch 319: 0/23 Loss: 11.947886\n",
      "2025-03-08 00:15: Train Epoch 319: 0/23 Loss: 11.947886\n",
      "2025-03-08 00:15: Train Epoch 319: 20/23 Loss: 83.782715\n",
      "2025-03-08 00:15: Train Epoch 319: 20/23 Loss: 83.782715\n",
      "2025-03-08 00:15: Train Epoch 319: 20/23 Loss: 83.782715\n",
      "2025-03-08 00:15: **********Train Epoch 319: averaged Loss: 39.254041\n",
      "2025-03-08 00:15: **********Train Epoch 319: averaged Loss: 39.254041\n",
      "2025-03-08 00:15: **********Train Epoch 319: averaged Loss: 39.254041\n",
      "2025-03-08 00:15: **********Val Epoch 319: average Loss: 2206.851196\n",
      "2025-03-08 00:15: **********Val Epoch 319: average Loss: 2206.851196\n",
      "2025-03-08 00:15: **********Val Epoch 319: average Loss: 2206.851196\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 320: 0/23 Loss: 5.755800\n",
      "2025-03-08 00:15: Train Epoch 320: 0/23 Loss: 5.755800\n",
      "2025-03-08 00:15: Train Epoch 320: 0/23 Loss: 5.755800\n",
      "2025-03-08 00:15: Train Epoch 320: 20/23 Loss: 95.867203\n",
      "2025-03-08 00:15: Train Epoch 320: 20/23 Loss: 95.867203\n",
      "2025-03-08 00:15: Train Epoch 320: 20/23 Loss: 95.867203\n",
      "2025-03-08 00:15: **********Train Epoch 320: averaged Loss: 35.847226\n",
      "2025-03-08 00:15: **********Train Epoch 320: averaged Loss: 35.847226\n",
      "2025-03-08 00:15: **********Train Epoch 320: averaged Loss: 35.847226\n",
      "2025-03-08 00:15: **********Val Epoch 320: average Loss: 2228.857544\n",
      "2025-03-08 00:15: **********Val Epoch 320: average Loss: 2228.857544\n",
      "2025-03-08 00:15: **********Val Epoch 320: average Loss: 2228.857544\n",
      "2025-03-08 00:15: Train Epoch 321: 0/23 Loss: 7.916004\n",
      "2025-03-08 00:15: Train Epoch 321: 0/23 Loss: 7.916004\n",
      "2025-03-08 00:15: Train Epoch 321: 0/23 Loss: 7.916004\n",
      "2025-03-08 00:15: Train Epoch 321: 20/23 Loss: 80.622734\n",
      "2025-03-08 00:15: Train Epoch 321: 20/23 Loss: 80.622734\n",
      "2025-03-08 00:15: Train Epoch 321: 20/23 Loss: 80.622734\n",
      "2025-03-08 00:15: **********Train Epoch 321: averaged Loss: 41.625480\n",
      "2025-03-08 00:15: **********Train Epoch 321: averaged Loss: 41.625480\n",
      "2025-03-08 00:15: **********Train Epoch 321: averaged Loss: 41.625480\n",
      "2025-03-08 00:15: **********Val Epoch 321: average Loss: 2219.906738\n",
      "2025-03-08 00:15: **********Val Epoch 321: average Loss: 2219.906738\n",
      "2025-03-08 00:15: **********Val Epoch 321: average Loss: 2219.906738\n",
      "2025-03-08 00:15: Train Epoch 322: 0/23 Loss: 7.011966\n",
      "2025-03-08 00:15: Train Epoch 322: 0/23 Loss: 7.011966\n",
      "2025-03-08 00:15: Train Epoch 322: 0/23 Loss: 7.011966\n",
      "2025-03-08 00:15: Train Epoch 322: 20/23 Loss: 93.241898\n",
      "2025-03-08 00:15: Train Epoch 322: 20/23 Loss: 93.241898\n",
      "2025-03-08 00:15: Train Epoch 322: 20/23 Loss: 93.241898\n",
      "2025-03-08 00:15: **********Train Epoch 322: averaged Loss: 39.156015\n",
      "2025-03-08 00:15: **********Train Epoch 322: averaged Loss: 39.156015\n",
      "2025-03-08 00:15: **********Train Epoch 322: averaged Loss: 39.156015\n",
      "2025-03-08 00:15: **********Val Epoch 322: average Loss: 2208.994934\n",
      "2025-03-08 00:15: **********Val Epoch 322: average Loss: 2208.994934\n",
      "2025-03-08 00:15: **********Val Epoch 322: average Loss: 2208.994934\n",
      "2025-03-08 00:15: Train Epoch 323: 0/23 Loss: 11.096922\n",
      "2025-03-08 00:15: Train Epoch 323: 0/23 Loss: 11.096922\n",
      "2025-03-08 00:15: Train Epoch 323: 0/23 Loss: 11.096922\n",
      "2025-03-08 00:15: Train Epoch 323: 20/23 Loss: 100.801651\n",
      "2025-03-08 00:15: Train Epoch 323: 20/23 Loss: 100.801651\n",
      "2025-03-08 00:15: Train Epoch 323: 20/23 Loss: 100.801651\n",
      "2025-03-08 00:15: **********Train Epoch 323: averaged Loss: 48.101533\n",
      "2025-03-08 00:15: **********Train Epoch 323: averaged Loss: 48.101533\n",
      "2025-03-08 00:15: **********Train Epoch 323: averaged Loss: 48.101533\n",
      "2025-03-08 00:15: **********Val Epoch 323: average Loss: 2224.101562\n",
      "2025-03-08 00:15: **********Val Epoch 323: average Loss: 2224.101562\n",
      "2025-03-08 00:15: **********Val Epoch 323: average Loss: 2224.101562\n",
      "2025-03-08 00:15: Train Epoch 324: 0/23 Loss: 1.946549\n",
      "2025-03-08 00:15: Train Epoch 324: 0/23 Loss: 1.946549\n",
      "2025-03-08 00:15: Train Epoch 324: 0/23 Loss: 1.946549\n",
      "2025-03-08 00:15: Train Epoch 324: 20/23 Loss: 85.085159\n",
      "2025-03-08 00:15: Train Epoch 324: 20/23 Loss: 85.085159\n",
      "2025-03-08 00:15: Train Epoch 324: 20/23 Loss: 85.085159\n",
      "2025-03-08 00:15: **********Train Epoch 324: averaged Loss: 40.910967\n",
      "2025-03-08 00:15: **********Train Epoch 324: averaged Loss: 40.910967\n",
      "2025-03-08 00:15: **********Train Epoch 324: averaged Loss: 40.910967\n",
      "2025-03-08 00:15: **********Val Epoch 324: average Loss: 2230.880615\n",
      "2025-03-08 00:15: **********Val Epoch 324: average Loss: 2230.880615\n",
      "2025-03-08 00:15: **********Val Epoch 324: average Loss: 2230.880615\n",
      "2025-03-08 00:15: Train Epoch 325: 0/23 Loss: 15.776406\n",
      "2025-03-08 00:15: Train Epoch 325: 0/23 Loss: 15.776406\n",
      "2025-03-08 00:15: Train Epoch 325: 0/23 Loss: 15.776406\n",
      "2025-03-08 00:15: Train Epoch 325: 20/23 Loss: 84.086998\n",
      "2025-03-08 00:15: Train Epoch 325: 20/23 Loss: 84.086998\n",
      "2025-03-08 00:15: Train Epoch 325: 20/23 Loss: 84.086998\n",
      "2025-03-08 00:15: **********Train Epoch 325: averaged Loss: 40.297327\n",
      "2025-03-08 00:15: **********Train Epoch 325: averaged Loss: 40.297327\n",
      "2025-03-08 00:15: **********Train Epoch 325: averaged Loss: 40.297327\n",
      "2025-03-08 00:15: **********Val Epoch 325: average Loss: 2225.939514\n",
      "2025-03-08 00:15: **********Val Epoch 325: average Loss: 2225.939514\n",
      "2025-03-08 00:15: **********Val Epoch 325: average Loss: 2225.939514\n",
      "2025-03-08 00:15: Train Epoch 326: 0/23 Loss: 5.728491\n",
      "2025-03-08 00:15: Train Epoch 326: 0/23 Loss: 5.728491\n",
      "2025-03-08 00:15: Train Epoch 326: 0/23 Loss: 5.728491\n",
      "2025-03-08 00:15: Train Epoch 326: 20/23 Loss: 129.555084\n",
      "2025-03-08 00:15: Train Epoch 326: 20/23 Loss: 129.555084\n",
      "2025-03-08 00:15: Train Epoch 326: 20/23 Loss: 129.555084\n",
      "2025-03-08 00:15: **********Train Epoch 326: averaged Loss: 45.424079\n",
      "2025-03-08 00:15: **********Train Epoch 326: averaged Loss: 45.424079\n",
      "2025-03-08 00:15: **********Train Epoch 326: averaged Loss: 45.424079\n",
      "2025-03-08 00:15: **********Val Epoch 326: average Loss: 2210.239807\n",
      "2025-03-08 00:15: **********Val Epoch 326: average Loss: 2210.239807\n",
      "2025-03-08 00:15: **********Val Epoch 326: average Loss: 2210.239807\n",
      "2025-03-08 00:15: Train Epoch 327: 0/23 Loss: 14.849806\n",
      "2025-03-08 00:15: Train Epoch 327: 0/23 Loss: 14.849806\n",
      "2025-03-08 00:15: Train Epoch 327: 0/23 Loss: 14.849806\n",
      "2025-03-08 00:15: Train Epoch 327: 20/23 Loss: 94.095016\n",
      "2025-03-08 00:15: Train Epoch 327: 20/23 Loss: 94.095016\n",
      "2025-03-08 00:15: Train Epoch 327: 20/23 Loss: 94.095016\n",
      "2025-03-08 00:15: **********Train Epoch 327: averaged Loss: 62.362204\n",
      "2025-03-08 00:15: **********Train Epoch 327: averaged Loss: 62.362204\n",
      "2025-03-08 00:15: **********Train Epoch 327: averaged Loss: 62.362204\n",
      "2025-03-08 00:15: **********Val Epoch 327: average Loss: 2215.462524\n",
      "2025-03-08 00:15: **********Val Epoch 327: average Loss: 2215.462524\n",
      "2025-03-08 00:15: **********Val Epoch 327: average Loss: 2215.462524\n",
      "2025-03-08 00:15: Train Epoch 328: 0/23 Loss: 2.669924\n",
      "2025-03-08 00:15: Train Epoch 328: 0/23 Loss: 2.669924\n",
      "2025-03-08 00:15: Train Epoch 328: 0/23 Loss: 2.669924\n",
      "2025-03-08 00:15: Train Epoch 328: 20/23 Loss: 85.563156\n",
      "2025-03-08 00:15: Train Epoch 328: 20/23 Loss: 85.563156\n",
      "2025-03-08 00:15: Train Epoch 328: 20/23 Loss: 85.563156\n",
      "2025-03-08 00:15: **********Train Epoch 328: averaged Loss: 42.800149\n",
      "2025-03-08 00:15: **********Train Epoch 328: averaged Loss: 42.800149\n",
      "2025-03-08 00:15: **********Train Epoch 328: averaged Loss: 42.800149\n",
      "2025-03-08 00:15: **********Val Epoch 328: average Loss: 2198.461426\n",
      "2025-03-08 00:15: **********Val Epoch 328: average Loss: 2198.461426\n",
      "2025-03-08 00:15: **********Val Epoch 328: average Loss: 2198.461426\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 329: 0/23 Loss: 8.511562\n",
      "2025-03-08 00:15: Train Epoch 329: 0/23 Loss: 8.511562\n",
      "2025-03-08 00:15: Train Epoch 329: 0/23 Loss: 8.511562\n",
      "2025-03-08 00:15: Train Epoch 329: 20/23 Loss: 83.976974\n",
      "2025-03-08 00:15: Train Epoch 329: 20/23 Loss: 83.976974\n",
      "2025-03-08 00:15: Train Epoch 329: 20/23 Loss: 83.976974\n",
      "2025-03-08 00:15: **********Train Epoch 329: averaged Loss: 38.781284\n",
      "2025-03-08 00:15: **********Train Epoch 329: averaged Loss: 38.781284\n",
      "2025-03-08 00:15: **********Train Epoch 329: averaged Loss: 38.781284\n",
      "2025-03-08 00:15: **********Val Epoch 329: average Loss: 2197.544434\n",
      "2025-03-08 00:15: **********Val Epoch 329: average Loss: 2197.544434\n",
      "2025-03-08 00:15: **********Val Epoch 329: average Loss: 2197.544434\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 330: 0/23 Loss: 1.661388\n",
      "2025-03-08 00:15: Train Epoch 330: 0/23 Loss: 1.661388\n",
      "2025-03-08 00:15: Train Epoch 330: 0/23 Loss: 1.661388\n",
      "2025-03-08 00:15: Train Epoch 330: 20/23 Loss: 92.799088\n",
      "2025-03-08 00:15: Train Epoch 330: 20/23 Loss: 92.799088\n",
      "2025-03-08 00:15: Train Epoch 330: 20/23 Loss: 92.799088\n",
      "2025-03-08 00:15: **********Train Epoch 330: averaged Loss: 35.530794\n",
      "2025-03-08 00:15: **********Train Epoch 330: averaged Loss: 35.530794\n",
      "2025-03-08 00:15: **********Train Epoch 330: averaged Loss: 35.530794\n",
      "2025-03-08 00:15: **********Val Epoch 330: average Loss: 2192.071777\n",
      "2025-03-08 00:15: **********Val Epoch 330: average Loss: 2192.071777\n",
      "2025-03-08 00:15: **********Val Epoch 330: average Loss: 2192.071777\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 331: 0/23 Loss: 3.659210\n",
      "2025-03-08 00:15: Train Epoch 331: 0/23 Loss: 3.659210\n",
      "2025-03-08 00:15: Train Epoch 331: 0/23 Loss: 3.659210\n",
      "2025-03-08 00:15: Train Epoch 331: 20/23 Loss: 80.367561\n",
      "2025-03-08 00:15: Train Epoch 331: 20/23 Loss: 80.367561\n",
      "2025-03-08 00:15: Train Epoch 331: 20/23 Loss: 80.367561\n",
      "2025-03-08 00:15: **********Train Epoch 331: averaged Loss: 35.316526\n",
      "2025-03-08 00:15: **********Train Epoch 331: averaged Loss: 35.316526\n",
      "2025-03-08 00:15: **********Train Epoch 331: averaged Loss: 35.316526\n",
      "2025-03-08 00:15: **********Val Epoch 331: average Loss: 2209.147583\n",
      "2025-03-08 00:15: **********Val Epoch 331: average Loss: 2209.147583\n",
      "2025-03-08 00:15: **********Val Epoch 331: average Loss: 2209.147583\n",
      "2025-03-08 00:15: Train Epoch 332: 0/23 Loss: 1.865873\n",
      "2025-03-08 00:15: Train Epoch 332: 0/23 Loss: 1.865873\n",
      "2025-03-08 00:15: Train Epoch 332: 0/23 Loss: 1.865873\n",
      "2025-03-08 00:15: Train Epoch 332: 20/23 Loss: 96.516785\n",
      "2025-03-08 00:15: Train Epoch 332: 20/23 Loss: 96.516785\n",
      "2025-03-08 00:15: Train Epoch 332: 20/23 Loss: 96.516785\n",
      "2025-03-08 00:15: **********Train Epoch 332: averaged Loss: 37.795407\n",
      "2025-03-08 00:15: **********Train Epoch 332: averaged Loss: 37.795407\n",
      "2025-03-08 00:15: **********Train Epoch 332: averaged Loss: 37.795407\n",
      "2025-03-08 00:15: **********Val Epoch 332: average Loss: 2198.794556\n",
      "2025-03-08 00:15: **********Val Epoch 332: average Loss: 2198.794556\n",
      "2025-03-08 00:15: **********Val Epoch 332: average Loss: 2198.794556\n",
      "2025-03-08 00:15: Train Epoch 333: 0/23 Loss: 8.070009\n",
      "2025-03-08 00:15: Train Epoch 333: 0/23 Loss: 8.070009\n",
      "2025-03-08 00:15: Train Epoch 333: 0/23 Loss: 8.070009\n",
      "2025-03-08 00:15: Train Epoch 333: 20/23 Loss: 94.587257\n",
      "2025-03-08 00:15: Train Epoch 333: 20/23 Loss: 94.587257\n",
      "2025-03-08 00:15: Train Epoch 333: 20/23 Loss: 94.587257\n",
      "2025-03-08 00:15: **********Train Epoch 333: averaged Loss: 50.600677\n",
      "2025-03-08 00:15: **********Train Epoch 333: averaged Loss: 50.600677\n",
      "2025-03-08 00:15: **********Train Epoch 333: averaged Loss: 50.600677\n",
      "2025-03-08 00:15: **********Val Epoch 333: average Loss: 2212.556274\n",
      "2025-03-08 00:15: **********Val Epoch 333: average Loss: 2212.556274\n",
      "2025-03-08 00:15: **********Val Epoch 333: average Loss: 2212.556274\n",
      "2025-03-08 00:15: Train Epoch 334: 0/23 Loss: 7.470919\n",
      "2025-03-08 00:15: Train Epoch 334: 0/23 Loss: 7.470919\n",
      "2025-03-08 00:15: Train Epoch 334: 0/23 Loss: 7.470919\n",
      "2025-03-08 00:15: Train Epoch 334: 20/23 Loss: 78.178177\n",
      "2025-03-08 00:15: Train Epoch 334: 20/23 Loss: 78.178177\n",
      "2025-03-08 00:15: Train Epoch 334: 20/23 Loss: 78.178177\n",
      "2025-03-08 00:15: **********Train Epoch 334: averaged Loss: 39.846507\n",
      "2025-03-08 00:15: **********Train Epoch 334: averaged Loss: 39.846507\n",
      "2025-03-08 00:15: **********Train Epoch 334: averaged Loss: 39.846507\n",
      "2025-03-08 00:15: **********Val Epoch 334: average Loss: 2215.155151\n",
      "2025-03-08 00:15: **********Val Epoch 334: average Loss: 2215.155151\n",
      "2025-03-08 00:15: **********Val Epoch 334: average Loss: 2215.155151\n",
      "2025-03-08 00:15: Train Epoch 335: 0/23 Loss: 13.792403\n",
      "2025-03-08 00:15: Train Epoch 335: 0/23 Loss: 13.792403\n",
      "2025-03-08 00:15: Train Epoch 335: 0/23 Loss: 13.792403\n",
      "2025-03-08 00:15: Train Epoch 335: 20/23 Loss: 76.034981\n",
      "2025-03-08 00:15: Train Epoch 335: 20/23 Loss: 76.034981\n",
      "2025-03-08 00:15: Train Epoch 335: 20/23 Loss: 76.034981\n",
      "2025-03-08 00:15: **********Train Epoch 335: averaged Loss: 38.927258\n",
      "2025-03-08 00:15: **********Train Epoch 335: averaged Loss: 38.927258\n",
      "2025-03-08 00:15: **********Train Epoch 335: averaged Loss: 38.927258\n",
      "2025-03-08 00:15: **********Val Epoch 335: average Loss: 2210.210327\n",
      "2025-03-08 00:15: **********Val Epoch 335: average Loss: 2210.210327\n",
      "2025-03-08 00:15: **********Val Epoch 335: average Loss: 2210.210327\n",
      "2025-03-08 00:15: Train Epoch 336: 0/23 Loss: 1.838713\n",
      "2025-03-08 00:15: Train Epoch 336: 0/23 Loss: 1.838713\n",
      "2025-03-08 00:15: Train Epoch 336: 0/23 Loss: 1.838713\n",
      "2025-03-08 00:15: Train Epoch 336: 20/23 Loss: 88.973427\n",
      "2025-03-08 00:15: Train Epoch 336: 20/23 Loss: 88.973427\n",
      "2025-03-08 00:15: Train Epoch 336: 20/23 Loss: 88.973427\n",
      "2025-03-08 00:15: **********Train Epoch 336: averaged Loss: 35.854340\n",
      "2025-03-08 00:15: **********Train Epoch 336: averaged Loss: 35.854340\n",
      "2025-03-08 00:15: **********Train Epoch 336: averaged Loss: 35.854340\n",
      "2025-03-08 00:15: **********Val Epoch 336: average Loss: 2200.003601\n",
      "2025-03-08 00:15: **********Val Epoch 336: average Loss: 2200.003601\n",
      "2025-03-08 00:15: **********Val Epoch 336: average Loss: 2200.003601\n",
      "2025-03-08 00:15: Train Epoch 337: 0/23 Loss: 9.898870\n",
      "2025-03-08 00:15: Train Epoch 337: 0/23 Loss: 9.898870\n",
      "2025-03-08 00:15: Train Epoch 337: 0/23 Loss: 9.898870\n",
      "2025-03-08 00:15: Train Epoch 337: 20/23 Loss: 80.304909\n",
      "2025-03-08 00:15: Train Epoch 337: 20/23 Loss: 80.304909\n",
      "2025-03-08 00:15: Train Epoch 337: 20/23 Loss: 80.304909\n",
      "2025-03-08 00:15: **********Train Epoch 337: averaged Loss: 40.293684\n",
      "2025-03-08 00:15: **********Train Epoch 337: averaged Loss: 40.293684\n",
      "2025-03-08 00:15: **********Train Epoch 337: averaged Loss: 40.293684\n",
      "2025-03-08 00:15: **********Val Epoch 337: average Loss: 2191.236694\n",
      "2025-03-08 00:15: **********Val Epoch 337: average Loss: 2191.236694\n",
      "2025-03-08 00:15: **********Val Epoch 337: average Loss: 2191.236694\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 338: 0/23 Loss: 5.467546\n",
      "2025-03-08 00:15: Train Epoch 338: 0/23 Loss: 5.467546\n",
      "2025-03-08 00:15: Train Epoch 338: 0/23 Loss: 5.467546\n",
      "2025-03-08 00:15: Train Epoch 338: 20/23 Loss: 95.286018\n",
      "2025-03-08 00:15: Train Epoch 338: 20/23 Loss: 95.286018\n",
      "2025-03-08 00:15: Train Epoch 338: 20/23 Loss: 95.286018\n",
      "2025-03-08 00:15: **********Train Epoch 338: averaged Loss: 41.292203\n",
      "2025-03-08 00:15: **********Train Epoch 338: averaged Loss: 41.292203\n",
      "2025-03-08 00:15: **********Train Epoch 338: averaged Loss: 41.292203\n",
      "2025-03-08 00:15: **********Val Epoch 338: average Loss: 2211.115479\n",
      "2025-03-08 00:15: **********Val Epoch 338: average Loss: 2211.115479\n",
      "2025-03-08 00:15: **********Val Epoch 338: average Loss: 2211.115479\n",
      "2025-03-08 00:15: Train Epoch 339: 0/23 Loss: 10.613484\n",
      "2025-03-08 00:15: Train Epoch 339: 0/23 Loss: 10.613484\n",
      "2025-03-08 00:15: Train Epoch 339: 0/23 Loss: 10.613484\n",
      "2025-03-08 00:15: Train Epoch 339: 20/23 Loss: 116.413376\n",
      "2025-03-08 00:15: Train Epoch 339: 20/23 Loss: 116.413376\n",
      "2025-03-08 00:15: Train Epoch 339: 20/23 Loss: 116.413376\n",
      "2025-03-08 00:15: **********Train Epoch 339: averaged Loss: 50.651861\n",
      "2025-03-08 00:15: **********Train Epoch 339: averaged Loss: 50.651861\n",
      "2025-03-08 00:15: **********Train Epoch 339: averaged Loss: 50.651861\n",
      "2025-03-08 00:15: **********Val Epoch 339: average Loss: 2179.587708\n",
      "2025-03-08 00:15: **********Val Epoch 339: average Loss: 2179.587708\n",
      "2025-03-08 00:15: **********Val Epoch 339: average Loss: 2179.587708\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: *********************************Current best model saved!\n",
      "2025-03-08 00:15: Train Epoch 340: 0/23 Loss: 6.202562\n",
      "2025-03-08 00:15: Train Epoch 340: 0/23 Loss: 6.202562\n",
      "2025-03-08 00:15: Train Epoch 340: 0/23 Loss: 6.202562\n",
      "2025-03-08 00:15: Train Epoch 340: 20/23 Loss: 72.882797\n",
      "2025-03-08 00:15: Train Epoch 340: 20/23 Loss: 72.882797\n",
      "2025-03-08 00:15: Train Epoch 340: 20/23 Loss: 72.882797\n",
      "2025-03-08 00:15: **********Train Epoch 340: averaged Loss: 40.310726\n",
      "2025-03-08 00:15: **********Train Epoch 340: averaged Loss: 40.310726\n",
      "2025-03-08 00:15: **********Train Epoch 340: averaged Loss: 40.310726\n",
      "2025-03-08 00:15: **********Val Epoch 340: average Loss: 2184.691345\n",
      "2025-03-08 00:15: **********Val Epoch 340: average Loss: 2184.691345\n",
      "2025-03-08 00:15: **********Val Epoch 340: average Loss: 2184.691345\n",
      "2025-03-08 00:15: Train Epoch 341: 0/23 Loss: 8.719340\n",
      "2025-03-08 00:15: Train Epoch 341: 0/23 Loss: 8.719340\n",
      "2025-03-08 00:15: Train Epoch 341: 0/23 Loss: 8.719340\n",
      "2025-03-08 00:15: Train Epoch 341: 20/23 Loss: 83.037010\n",
      "2025-03-08 00:15: Train Epoch 341: 20/23 Loss: 83.037010\n",
      "2025-03-08 00:15: Train Epoch 341: 20/23 Loss: 83.037010\n",
      "2025-03-08 00:15: **********Train Epoch 341: averaged Loss: 35.456278\n",
      "2025-03-08 00:15: **********Train Epoch 341: averaged Loss: 35.456278\n",
      "2025-03-08 00:15: **********Train Epoch 341: averaged Loss: 35.456278\n",
      "2025-03-08 00:15: **********Val Epoch 341: average Loss: 2197.864136\n",
      "2025-03-08 00:15: **********Val Epoch 341: average Loss: 2197.864136\n",
      "2025-03-08 00:15: **********Val Epoch 341: average Loss: 2197.864136\n",
      "2025-03-08 00:15: Train Epoch 342: 0/23 Loss: 3.685319\n",
      "2025-03-08 00:15: Train Epoch 342: 0/23 Loss: 3.685319\n",
      "2025-03-08 00:15: Train Epoch 342: 0/23 Loss: 3.685319\n",
      "2025-03-08 00:15: Train Epoch 342: 20/23 Loss: 83.471581\n",
      "2025-03-08 00:15: Train Epoch 342: 20/23 Loss: 83.471581\n",
      "2025-03-08 00:15: Train Epoch 342: 20/23 Loss: 83.471581\n",
      "2025-03-08 00:15: **********Train Epoch 342: averaged Loss: 36.543983\n",
      "2025-03-08 00:15: **********Train Epoch 342: averaged Loss: 36.543983\n",
      "2025-03-08 00:15: **********Train Epoch 342: averaged Loss: 36.543983\n",
      "2025-03-08 00:15: **********Val Epoch 342: average Loss: 2200.531616\n",
      "2025-03-08 00:15: **********Val Epoch 342: average Loss: 2200.531616\n",
      "2025-03-08 00:15: **********Val Epoch 342: average Loss: 2200.531616\n",
      "2025-03-08 00:15: Train Epoch 343: 0/23 Loss: 8.721888\n",
      "2025-03-08 00:15: Train Epoch 343: 0/23 Loss: 8.721888\n",
      "2025-03-08 00:15: Train Epoch 343: 0/23 Loss: 8.721888\n",
      "2025-03-08 00:15: Train Epoch 343: 20/23 Loss: 117.981064\n",
      "2025-03-08 00:15: Train Epoch 343: 20/23 Loss: 117.981064\n",
      "2025-03-08 00:15: Train Epoch 343: 20/23 Loss: 117.981064\n",
      "2025-03-08 00:15: **********Train Epoch 343: averaged Loss: 44.562773\n",
      "2025-03-08 00:15: **********Train Epoch 343: averaged Loss: 44.562773\n",
      "2025-03-08 00:15: **********Train Epoch 343: averaged Loss: 44.562773\n",
      "2025-03-08 00:15: **********Val Epoch 343: average Loss: 2200.570679\n",
      "2025-03-08 00:15: **********Val Epoch 343: average Loss: 2200.570679\n",
      "2025-03-08 00:15: **********Val Epoch 343: average Loss: 2200.570679\n",
      "2025-03-08 00:15: Train Epoch 344: 0/23 Loss: 14.122343\n",
      "2025-03-08 00:15: Train Epoch 344: 0/23 Loss: 14.122343\n",
      "2025-03-08 00:15: Train Epoch 344: 0/23 Loss: 14.122343\n",
      "2025-03-08 00:16: Train Epoch 344: 20/23 Loss: 93.706047\n",
      "2025-03-08 00:16: Train Epoch 344: 20/23 Loss: 93.706047\n",
      "2025-03-08 00:16: Train Epoch 344: 20/23 Loss: 93.706047\n",
      "2025-03-08 00:16: **********Train Epoch 344: averaged Loss: 65.745215\n",
      "2025-03-08 00:16: **********Train Epoch 344: averaged Loss: 65.745215\n",
      "2025-03-08 00:16: **********Train Epoch 344: averaged Loss: 65.745215\n",
      "2025-03-08 00:16: **********Val Epoch 344: average Loss: 2171.002869\n",
      "2025-03-08 00:16: **********Val Epoch 344: average Loss: 2171.002869\n",
      "2025-03-08 00:16: **********Val Epoch 344: average Loss: 2171.002869\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: Train Epoch 345: 0/23 Loss: 6.917168\n",
      "2025-03-08 00:16: Train Epoch 345: 0/23 Loss: 6.917168\n",
      "2025-03-08 00:16: Train Epoch 345: 0/23 Loss: 6.917168\n",
      "2025-03-08 00:16: Train Epoch 345: 20/23 Loss: 83.474701\n",
      "2025-03-08 00:16: Train Epoch 345: 20/23 Loss: 83.474701\n",
      "2025-03-08 00:16: Train Epoch 345: 20/23 Loss: 83.474701\n",
      "2025-03-08 00:16: **********Train Epoch 345: averaged Loss: 42.197773\n",
      "2025-03-08 00:16: **********Train Epoch 345: averaged Loss: 42.197773\n",
      "2025-03-08 00:16: **********Train Epoch 345: averaged Loss: 42.197773\n",
      "2025-03-08 00:16: **********Val Epoch 345: average Loss: 2199.298157\n",
      "2025-03-08 00:16: **********Val Epoch 345: average Loss: 2199.298157\n",
      "2025-03-08 00:16: **********Val Epoch 345: average Loss: 2199.298157\n",
      "2025-03-08 00:16: Train Epoch 346: 0/23 Loss: 11.105440\n",
      "2025-03-08 00:16: Train Epoch 346: 0/23 Loss: 11.105440\n",
      "2025-03-08 00:16: Train Epoch 346: 0/23 Loss: 11.105440\n",
      "2025-03-08 00:16: Train Epoch 346: 20/23 Loss: 82.990784\n",
      "2025-03-08 00:16: Train Epoch 346: 20/23 Loss: 82.990784\n",
      "2025-03-08 00:16: Train Epoch 346: 20/23 Loss: 82.990784\n",
      "2025-03-08 00:16: **********Train Epoch 346: averaged Loss: 35.820499\n",
      "2025-03-08 00:16: **********Train Epoch 346: averaged Loss: 35.820499\n",
      "2025-03-08 00:16: **********Train Epoch 346: averaged Loss: 35.820499\n",
      "2025-03-08 00:16: **********Val Epoch 346: average Loss: 2209.036438\n",
      "2025-03-08 00:16: **********Val Epoch 346: average Loss: 2209.036438\n",
      "2025-03-08 00:16: **********Val Epoch 346: average Loss: 2209.036438\n",
      "2025-03-08 00:16: Train Epoch 347: 0/23 Loss: 2.197146\n",
      "2025-03-08 00:16: Train Epoch 347: 0/23 Loss: 2.197146\n",
      "2025-03-08 00:16: Train Epoch 347: 0/23 Loss: 2.197146\n",
      "2025-03-08 00:16: Train Epoch 347: 20/23 Loss: 87.284515\n",
      "2025-03-08 00:16: Train Epoch 347: 20/23 Loss: 87.284515\n",
      "2025-03-08 00:16: Train Epoch 347: 20/23 Loss: 87.284515\n",
      "2025-03-08 00:16: **********Train Epoch 347: averaged Loss: 35.278396\n",
      "2025-03-08 00:16: **********Train Epoch 347: averaged Loss: 35.278396\n",
      "2025-03-08 00:16: **********Train Epoch 347: averaged Loss: 35.278396\n",
      "2025-03-08 00:16: **********Val Epoch 347: average Loss: 2170.041260\n",
      "2025-03-08 00:16: **********Val Epoch 347: average Loss: 2170.041260\n",
      "2025-03-08 00:16: **********Val Epoch 347: average Loss: 2170.041260\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: Train Epoch 348: 0/23 Loss: 8.768358\n",
      "2025-03-08 00:16: Train Epoch 348: 0/23 Loss: 8.768358\n",
      "2025-03-08 00:16: Train Epoch 348: 0/23 Loss: 8.768358\n",
      "2025-03-08 00:16: Train Epoch 348: 20/23 Loss: 77.243637\n",
      "2025-03-08 00:16: Train Epoch 348: 20/23 Loss: 77.243637\n",
      "2025-03-08 00:16: Train Epoch 348: 20/23 Loss: 77.243637\n",
      "2025-03-08 00:16: **********Train Epoch 348: averaged Loss: 41.179861\n",
      "2025-03-08 00:16: **********Train Epoch 348: averaged Loss: 41.179861\n",
      "2025-03-08 00:16: **********Train Epoch 348: averaged Loss: 41.179861\n",
      "2025-03-08 00:16: **********Val Epoch 348: average Loss: 2177.895264\n",
      "2025-03-08 00:16: **********Val Epoch 348: average Loss: 2177.895264\n",
      "2025-03-08 00:16: **********Val Epoch 348: average Loss: 2177.895264\n",
      "2025-03-08 00:16: Train Epoch 349: 0/23 Loss: 2.719927\n",
      "2025-03-08 00:16: Train Epoch 349: 0/23 Loss: 2.719927\n",
      "2025-03-08 00:16: Train Epoch 349: 0/23 Loss: 2.719927\n",
      "2025-03-08 00:16: Train Epoch 349: 20/23 Loss: 77.194901\n",
      "2025-03-08 00:16: Train Epoch 349: 20/23 Loss: 77.194901\n",
      "2025-03-08 00:16: Train Epoch 349: 20/23 Loss: 77.194901\n",
      "2025-03-08 00:16: **********Train Epoch 349: averaged Loss: 36.177071\n",
      "2025-03-08 00:16: **********Train Epoch 349: averaged Loss: 36.177071\n",
      "2025-03-08 00:16: **********Train Epoch 349: averaged Loss: 36.177071\n",
      "2025-03-08 00:16: **********Val Epoch 349: average Loss: 2195.995667\n",
      "2025-03-08 00:16: **********Val Epoch 349: average Loss: 2195.995667\n",
      "2025-03-08 00:16: **********Val Epoch 349: average Loss: 2195.995667\n",
      "2025-03-08 00:16: Train Epoch 350: 0/23 Loss: 14.170042\n",
      "2025-03-08 00:16: Train Epoch 350: 0/23 Loss: 14.170042\n",
      "2025-03-08 00:16: Train Epoch 350: 0/23 Loss: 14.170042\n",
      "2025-03-08 00:16: Train Epoch 350: 20/23 Loss: 79.189484\n",
      "2025-03-08 00:16: Train Epoch 350: 20/23 Loss: 79.189484\n",
      "2025-03-08 00:16: Train Epoch 350: 20/23 Loss: 79.189484\n",
      "2025-03-08 00:16: **********Train Epoch 350: averaged Loss: 40.713900\n",
      "2025-03-08 00:16: **********Train Epoch 350: averaged Loss: 40.713900\n",
      "2025-03-08 00:16: **********Train Epoch 350: averaged Loss: 40.713900\n",
      "2025-03-08 00:16: **********Val Epoch 350: average Loss: 2205.932556\n",
      "2025-03-08 00:16: **********Val Epoch 350: average Loss: 2205.932556\n",
      "2025-03-08 00:16: **********Val Epoch 350: average Loss: 2205.932556\n",
      "2025-03-08 00:16: Train Epoch 351: 0/23 Loss: 6.153242\n",
      "2025-03-08 00:16: Train Epoch 351: 0/23 Loss: 6.153242\n",
      "2025-03-08 00:16: Train Epoch 351: 0/23 Loss: 6.153242\n",
      "2025-03-08 00:16: Train Epoch 351: 20/23 Loss: 79.896896\n",
      "2025-03-08 00:16: Train Epoch 351: 20/23 Loss: 79.896896\n",
      "2025-03-08 00:16: Train Epoch 351: 20/23 Loss: 79.896896\n",
      "2025-03-08 00:16: **********Train Epoch 351: averaged Loss: 37.476171\n",
      "2025-03-08 00:16: **********Train Epoch 351: averaged Loss: 37.476171\n",
      "2025-03-08 00:16: **********Train Epoch 351: averaged Loss: 37.476171\n",
      "2025-03-08 00:16: **********Val Epoch 351: average Loss: 2150.162354\n",
      "2025-03-08 00:16: **********Val Epoch 351: average Loss: 2150.162354\n",
      "2025-03-08 00:16: **********Val Epoch 351: average Loss: 2150.162354\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: Train Epoch 352: 0/23 Loss: 10.651201\n",
      "2025-03-08 00:16: Train Epoch 352: 0/23 Loss: 10.651201\n",
      "2025-03-08 00:16: Train Epoch 352: 0/23 Loss: 10.651201\n",
      "2025-03-08 00:16: Train Epoch 352: 20/23 Loss: 89.175064\n",
      "2025-03-08 00:16: Train Epoch 352: 20/23 Loss: 89.175064\n",
      "2025-03-08 00:16: Train Epoch 352: 20/23 Loss: 89.175064\n",
      "2025-03-08 00:16: **********Train Epoch 352: averaged Loss: 48.348757\n",
      "2025-03-08 00:16: **********Train Epoch 352: averaged Loss: 48.348757\n",
      "2025-03-08 00:16: **********Train Epoch 352: averaged Loss: 48.348757\n",
      "2025-03-08 00:16: **********Val Epoch 352: average Loss: 2186.989746\n",
      "2025-03-08 00:16: **********Val Epoch 352: average Loss: 2186.989746\n",
      "2025-03-08 00:16: **********Val Epoch 352: average Loss: 2186.989746\n",
      "2025-03-08 00:16: Train Epoch 353: 0/23 Loss: 4.803986\n",
      "2025-03-08 00:16: Train Epoch 353: 0/23 Loss: 4.803986\n",
      "2025-03-08 00:16: Train Epoch 353: 0/23 Loss: 4.803986\n",
      "2025-03-08 00:16: Train Epoch 353: 20/23 Loss: 70.877518\n",
      "2025-03-08 00:16: Train Epoch 353: 20/23 Loss: 70.877518\n",
      "2025-03-08 00:16: Train Epoch 353: 20/23 Loss: 70.877518\n",
      "2025-03-08 00:16: **********Train Epoch 353: averaged Loss: 36.684303\n",
      "2025-03-08 00:16: **********Train Epoch 353: averaged Loss: 36.684303\n",
      "2025-03-08 00:16: **********Train Epoch 353: averaged Loss: 36.684303\n",
      "2025-03-08 00:16: **********Val Epoch 353: average Loss: 2179.263428\n",
      "2025-03-08 00:16: **********Val Epoch 353: average Loss: 2179.263428\n",
      "2025-03-08 00:16: **********Val Epoch 353: average Loss: 2179.263428\n",
      "2025-03-08 00:16: Train Epoch 354: 0/23 Loss: 14.995167\n",
      "2025-03-08 00:16: Train Epoch 354: 0/23 Loss: 14.995167\n",
      "2025-03-08 00:16: Train Epoch 354: 0/23 Loss: 14.995167\n",
      "2025-03-08 00:16: Train Epoch 354: 20/23 Loss: 82.812912\n",
      "2025-03-08 00:16: Train Epoch 354: 20/23 Loss: 82.812912\n",
      "2025-03-08 00:16: Train Epoch 354: 20/23 Loss: 82.812912\n",
      "2025-03-08 00:16: **********Train Epoch 354: averaged Loss: 36.487807\n",
      "2025-03-08 00:16: **********Train Epoch 354: averaged Loss: 36.487807\n",
      "2025-03-08 00:16: **********Train Epoch 354: averaged Loss: 36.487807\n",
      "2025-03-08 00:16: **********Val Epoch 354: average Loss: 2174.901550\n",
      "2025-03-08 00:16: **********Val Epoch 354: average Loss: 2174.901550\n",
      "2025-03-08 00:16: **********Val Epoch 354: average Loss: 2174.901550\n",
      "2025-03-08 00:16: Train Epoch 355: 0/23 Loss: 1.628704\n",
      "2025-03-08 00:16: Train Epoch 355: 0/23 Loss: 1.628704\n",
      "2025-03-08 00:16: Train Epoch 355: 0/23 Loss: 1.628704\n",
      "2025-03-08 00:16: Train Epoch 355: 20/23 Loss: 83.102631\n",
      "2025-03-08 00:16: Train Epoch 355: 20/23 Loss: 83.102631\n",
      "2025-03-08 00:16: Train Epoch 355: 20/23 Loss: 83.102631\n",
      "2025-03-08 00:16: **********Train Epoch 355: averaged Loss: 34.566229\n",
      "2025-03-08 00:16: **********Train Epoch 355: averaged Loss: 34.566229\n",
      "2025-03-08 00:16: **********Train Epoch 355: averaged Loss: 34.566229\n",
      "2025-03-08 00:16: **********Val Epoch 355: average Loss: 2188.930908\n",
      "2025-03-08 00:16: **********Val Epoch 355: average Loss: 2188.930908\n",
      "2025-03-08 00:16: **********Val Epoch 355: average Loss: 2188.930908\n",
      "2025-03-08 00:16: Train Epoch 356: 0/23 Loss: 3.075601\n",
      "2025-03-08 00:16: Train Epoch 356: 0/23 Loss: 3.075601\n",
      "2025-03-08 00:16: Train Epoch 356: 0/23 Loss: 3.075601\n",
      "2025-03-08 00:16: Train Epoch 356: 20/23 Loss: 112.511688\n",
      "2025-03-08 00:16: Train Epoch 356: 20/23 Loss: 112.511688\n",
      "2025-03-08 00:16: Train Epoch 356: 20/23 Loss: 112.511688\n",
      "2025-03-08 00:16: **********Train Epoch 356: averaged Loss: 42.076462\n",
      "2025-03-08 00:16: **********Train Epoch 356: averaged Loss: 42.076462\n",
      "2025-03-08 00:16: **********Train Epoch 356: averaged Loss: 42.076462\n",
      "2025-03-08 00:16: **********Val Epoch 356: average Loss: 2172.197266\n",
      "2025-03-08 00:16: **********Val Epoch 356: average Loss: 2172.197266\n",
      "2025-03-08 00:16: **********Val Epoch 356: average Loss: 2172.197266\n",
      "2025-03-08 00:16: Train Epoch 357: 0/23 Loss: 7.200321\n",
      "2025-03-08 00:16: Train Epoch 357: 0/23 Loss: 7.200321\n",
      "2025-03-08 00:16: Train Epoch 357: 0/23 Loss: 7.200321\n",
      "2025-03-08 00:16: Train Epoch 357: 20/23 Loss: 91.504646\n",
      "2025-03-08 00:16: Train Epoch 357: 20/23 Loss: 91.504646\n",
      "2025-03-08 00:16: Train Epoch 357: 20/23 Loss: 91.504646\n",
      "2025-03-08 00:16: **********Train Epoch 357: averaged Loss: 62.980438\n",
      "2025-03-08 00:16: **********Train Epoch 357: averaged Loss: 62.980438\n",
      "2025-03-08 00:16: **********Train Epoch 357: averaged Loss: 62.980438\n",
      "2025-03-08 00:16: **********Val Epoch 357: average Loss: 2164.645386\n",
      "2025-03-08 00:16: **********Val Epoch 357: average Loss: 2164.645386\n",
      "2025-03-08 00:16: **********Val Epoch 357: average Loss: 2164.645386\n",
      "2025-03-08 00:16: Train Epoch 358: 0/23 Loss: 6.784894\n",
      "2025-03-08 00:16: Train Epoch 358: 0/23 Loss: 6.784894\n",
      "2025-03-08 00:16: Train Epoch 358: 0/23 Loss: 6.784894\n",
      "2025-03-08 00:16: Train Epoch 358: 20/23 Loss: 89.542854\n",
      "2025-03-08 00:16: Train Epoch 358: 20/23 Loss: 89.542854\n",
      "2025-03-08 00:16: Train Epoch 358: 20/23 Loss: 89.542854\n",
      "2025-03-08 00:16: **********Train Epoch 358: averaged Loss: 42.232233\n",
      "2025-03-08 00:16: **********Train Epoch 358: averaged Loss: 42.232233\n",
      "2025-03-08 00:16: **********Train Epoch 358: averaged Loss: 42.232233\n",
      "2025-03-08 00:16: **********Val Epoch 358: average Loss: 2165.235596\n",
      "2025-03-08 00:16: **********Val Epoch 358: average Loss: 2165.235596\n",
      "2025-03-08 00:16: **********Val Epoch 358: average Loss: 2165.235596\n",
      "2025-03-08 00:16: Train Epoch 359: 0/23 Loss: 9.164941\n",
      "2025-03-08 00:16: Train Epoch 359: 0/23 Loss: 9.164941\n",
      "2025-03-08 00:16: Train Epoch 359: 0/23 Loss: 9.164941\n",
      "2025-03-08 00:16: Train Epoch 359: 20/23 Loss: 81.588226\n",
      "2025-03-08 00:16: Train Epoch 359: 20/23 Loss: 81.588226\n",
      "2025-03-08 00:16: Train Epoch 359: 20/23 Loss: 81.588226\n",
      "2025-03-08 00:16: **********Train Epoch 359: averaged Loss: 38.437088\n",
      "2025-03-08 00:16: **********Train Epoch 359: averaged Loss: 38.437088\n",
      "2025-03-08 00:16: **********Train Epoch 359: averaged Loss: 38.437088\n",
      "2025-03-08 00:16: **********Val Epoch 359: average Loss: 2177.966248\n",
      "2025-03-08 00:16: **********Val Epoch 359: average Loss: 2177.966248\n",
      "2025-03-08 00:16: **********Val Epoch 359: average Loss: 2177.966248\n",
      "2025-03-08 00:16: Train Epoch 360: 0/23 Loss: 3.966902\n",
      "2025-03-08 00:16: Train Epoch 360: 0/23 Loss: 3.966902\n",
      "2025-03-08 00:16: Train Epoch 360: 0/23 Loss: 3.966902\n",
      "2025-03-08 00:16: Train Epoch 360: 20/23 Loss: 95.960602\n",
      "2025-03-08 00:16: Train Epoch 360: 20/23 Loss: 95.960602\n",
      "2025-03-08 00:16: Train Epoch 360: 20/23 Loss: 95.960602\n",
      "2025-03-08 00:16: **********Train Epoch 360: averaged Loss: 35.923353\n",
      "2025-03-08 00:16: **********Train Epoch 360: averaged Loss: 35.923353\n",
      "2025-03-08 00:16: **********Train Epoch 360: averaged Loss: 35.923353\n",
      "2025-03-08 00:16: **********Val Epoch 360: average Loss: 2180.690674\n",
      "2025-03-08 00:16: **********Val Epoch 360: average Loss: 2180.690674\n",
      "2025-03-08 00:16: **********Val Epoch 360: average Loss: 2180.690674\n",
      "2025-03-08 00:16: Train Epoch 361: 0/23 Loss: 7.934609\n",
      "2025-03-08 00:16: Train Epoch 361: 0/23 Loss: 7.934609\n",
      "2025-03-08 00:16: Train Epoch 361: 0/23 Loss: 7.934609\n",
      "2025-03-08 00:16: Train Epoch 361: 20/23 Loss: 87.825546\n",
      "2025-03-08 00:16: Train Epoch 361: 20/23 Loss: 87.825546\n",
      "2025-03-08 00:16: Train Epoch 361: 20/23 Loss: 87.825546\n",
      "2025-03-08 00:16: **********Train Epoch 361: averaged Loss: 47.091924\n",
      "2025-03-08 00:16: **********Train Epoch 361: averaged Loss: 47.091924\n",
      "2025-03-08 00:16: **********Train Epoch 361: averaged Loss: 47.091924\n",
      "2025-03-08 00:16: **********Val Epoch 361: average Loss: 2161.173340\n",
      "2025-03-08 00:16: **********Val Epoch 361: average Loss: 2161.173340\n",
      "2025-03-08 00:16: **********Val Epoch 361: average Loss: 2161.173340\n",
      "2025-03-08 00:16: Train Epoch 362: 0/23 Loss: 4.321524\n",
      "2025-03-08 00:16: Train Epoch 362: 0/23 Loss: 4.321524\n",
      "2025-03-08 00:16: Train Epoch 362: 0/23 Loss: 4.321524\n",
      "2025-03-08 00:16: Train Epoch 362: 20/23 Loss: 71.943909\n",
      "2025-03-08 00:16: Train Epoch 362: 20/23 Loss: 71.943909\n",
      "2025-03-08 00:16: Train Epoch 362: 20/23 Loss: 71.943909\n",
      "2025-03-08 00:16: **********Train Epoch 362: averaged Loss: 38.190456\n",
      "2025-03-08 00:16: **********Train Epoch 362: averaged Loss: 38.190456\n",
      "2025-03-08 00:16: **********Train Epoch 362: averaged Loss: 38.190456\n",
      "2025-03-08 00:16: **********Val Epoch 362: average Loss: 2182.088135\n",
      "2025-03-08 00:16: **********Val Epoch 362: average Loss: 2182.088135\n",
      "2025-03-08 00:16: **********Val Epoch 362: average Loss: 2182.088135\n",
      "2025-03-08 00:16: Train Epoch 363: 0/23 Loss: 12.228424\n",
      "2025-03-08 00:16: Train Epoch 363: 0/23 Loss: 12.228424\n",
      "2025-03-08 00:16: Train Epoch 363: 0/23 Loss: 12.228424\n",
      "2025-03-08 00:16: Train Epoch 363: 20/23 Loss: 69.528191\n",
      "2025-03-08 00:16: Train Epoch 363: 20/23 Loss: 69.528191\n",
      "2025-03-08 00:16: Train Epoch 363: 20/23 Loss: 69.528191\n",
      "2025-03-08 00:16: **********Train Epoch 363: averaged Loss: 34.818019\n",
      "2025-03-08 00:16: **********Train Epoch 363: averaged Loss: 34.818019\n",
      "2025-03-08 00:16: **********Train Epoch 363: averaged Loss: 34.818019\n",
      "2025-03-08 00:16: **********Val Epoch 363: average Loss: 2177.867188\n",
      "2025-03-08 00:16: **********Val Epoch 363: average Loss: 2177.867188\n",
      "2025-03-08 00:16: **********Val Epoch 363: average Loss: 2177.867188\n",
      "2025-03-08 00:16: Train Epoch 364: 0/23 Loss: 3.660360\n",
      "2025-03-08 00:16: Train Epoch 364: 0/23 Loss: 3.660360\n",
      "2025-03-08 00:16: Train Epoch 364: 0/23 Loss: 3.660360\n",
      "2025-03-08 00:16: Train Epoch 364: 20/23 Loss: 97.512657\n",
      "2025-03-08 00:16: Train Epoch 364: 20/23 Loss: 97.512657\n",
      "2025-03-08 00:16: Train Epoch 364: 20/23 Loss: 97.512657\n",
      "2025-03-08 00:16: **********Train Epoch 364: averaged Loss: 36.897742\n",
      "2025-03-08 00:16: **********Train Epoch 364: averaged Loss: 36.897742\n",
      "2025-03-08 00:16: **********Train Epoch 364: averaged Loss: 36.897742\n",
      "2025-03-08 00:16: **********Val Epoch 364: average Loss: 2187.624084\n",
      "2025-03-08 00:16: **********Val Epoch 364: average Loss: 2187.624084\n",
      "2025-03-08 00:16: **********Val Epoch 364: average Loss: 2187.624084\n",
      "2025-03-08 00:16: Train Epoch 365: 0/23 Loss: 10.032808\n",
      "2025-03-08 00:16: Train Epoch 365: 0/23 Loss: 10.032808\n",
      "2025-03-08 00:16: Train Epoch 365: 0/23 Loss: 10.032808\n",
      "2025-03-08 00:16: Train Epoch 365: 20/23 Loss: 81.291054\n",
      "2025-03-08 00:16: Train Epoch 365: 20/23 Loss: 81.291054\n",
      "2025-03-08 00:16: Train Epoch 365: 20/23 Loss: 81.291054\n",
      "2025-03-08 00:16: **********Train Epoch 365: averaged Loss: 53.840333\n",
      "2025-03-08 00:16: **********Train Epoch 365: averaged Loss: 53.840333\n",
      "2025-03-08 00:16: **********Train Epoch 365: averaged Loss: 53.840333\n",
      "2025-03-08 00:16: **********Val Epoch 365: average Loss: 2154.607605\n",
      "2025-03-08 00:16: **********Val Epoch 365: average Loss: 2154.607605\n",
      "2025-03-08 00:16: **********Val Epoch 365: average Loss: 2154.607605\n",
      "2025-03-08 00:16: Train Epoch 366: 0/23 Loss: 3.213625\n",
      "2025-03-08 00:16: Train Epoch 366: 0/23 Loss: 3.213625\n",
      "2025-03-08 00:16: Train Epoch 366: 0/23 Loss: 3.213625\n",
      "2025-03-08 00:16: Train Epoch 366: 20/23 Loss: 84.534187\n",
      "2025-03-08 00:16: Train Epoch 366: 20/23 Loss: 84.534187\n",
      "2025-03-08 00:16: Train Epoch 366: 20/23 Loss: 84.534187\n",
      "2025-03-08 00:16: **********Train Epoch 366: averaged Loss: 39.169267\n",
      "2025-03-08 00:16: **********Train Epoch 366: averaged Loss: 39.169267\n",
      "2025-03-08 00:16: **********Train Epoch 366: averaged Loss: 39.169267\n",
      "2025-03-08 00:16: **********Val Epoch 366: average Loss: 2172.478149\n",
      "2025-03-08 00:16: **********Val Epoch 366: average Loss: 2172.478149\n",
      "2025-03-08 00:16: **********Val Epoch 366: average Loss: 2172.478149\n",
      "2025-03-08 00:16: Train Epoch 367: 0/23 Loss: 19.399055\n",
      "2025-03-08 00:16: Train Epoch 367: 0/23 Loss: 19.399055\n",
      "2025-03-08 00:16: Train Epoch 367: 0/23 Loss: 19.399055\n",
      "2025-03-08 00:16: Train Epoch 367: 20/23 Loss: 88.612793\n",
      "2025-03-08 00:16: Train Epoch 367: 20/23 Loss: 88.612793\n",
      "2025-03-08 00:16: Train Epoch 367: 20/23 Loss: 88.612793\n",
      "2025-03-08 00:16: **********Train Epoch 367: averaged Loss: 38.881346\n",
      "2025-03-08 00:16: **********Train Epoch 367: averaged Loss: 38.881346\n",
      "2025-03-08 00:16: **********Train Epoch 367: averaged Loss: 38.881346\n",
      "2025-03-08 00:16: **********Val Epoch 367: average Loss: 2189.994080\n",
      "2025-03-08 00:16: **********Val Epoch 367: average Loss: 2189.994080\n",
      "2025-03-08 00:16: **********Val Epoch 367: average Loss: 2189.994080\n",
      "2025-03-08 00:16: Train Epoch 368: 0/23 Loss: 3.767404\n",
      "2025-03-08 00:16: Train Epoch 368: 0/23 Loss: 3.767404\n",
      "2025-03-08 00:16: Train Epoch 368: 0/23 Loss: 3.767404\n",
      "2025-03-08 00:16: Train Epoch 368: 20/23 Loss: 120.626213\n",
      "2025-03-08 00:16: Train Epoch 368: 20/23 Loss: 120.626213\n",
      "2025-03-08 00:16: Train Epoch 368: 20/23 Loss: 120.626213\n",
      "2025-03-08 00:16: **********Train Epoch 368: averaged Loss: 38.526732\n",
      "2025-03-08 00:16: **********Train Epoch 368: averaged Loss: 38.526732\n",
      "2025-03-08 00:16: **********Train Epoch 368: averaged Loss: 38.526732\n",
      "2025-03-08 00:16: **********Val Epoch 368: average Loss: 2155.741943\n",
      "2025-03-08 00:16: **********Val Epoch 368: average Loss: 2155.741943\n",
      "2025-03-08 00:16: **********Val Epoch 368: average Loss: 2155.741943\n",
      "2025-03-08 00:16: Train Epoch 369: 0/23 Loss: 13.514937\n",
      "2025-03-08 00:16: Train Epoch 369: 0/23 Loss: 13.514937\n",
      "2025-03-08 00:16: Train Epoch 369: 0/23 Loss: 13.514937\n",
      "2025-03-08 00:16: Train Epoch 369: 20/23 Loss: 111.487198\n",
      "2025-03-08 00:16: Train Epoch 369: 20/23 Loss: 111.487198\n",
      "2025-03-08 00:16: Train Epoch 369: 20/23 Loss: 111.487198\n",
      "2025-03-08 00:16: **********Train Epoch 369: averaged Loss: 61.678535\n",
      "2025-03-08 00:16: **********Train Epoch 369: averaged Loss: 61.678535\n",
      "2025-03-08 00:16: **********Train Epoch 369: averaged Loss: 61.678535\n",
      "2025-03-08 00:16: **********Val Epoch 369: average Loss: 2133.472168\n",
      "2025-03-08 00:16: **********Val Epoch 369: average Loss: 2133.472168\n",
      "2025-03-08 00:16: **********Val Epoch 369: average Loss: 2133.472168\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: *********************************Current best model saved!\n",
      "2025-03-08 00:16: Train Epoch 370: 0/23 Loss: 4.794460\n",
      "2025-03-08 00:16: Train Epoch 370: 0/23 Loss: 4.794460\n",
      "2025-03-08 00:16: Train Epoch 370: 0/23 Loss: 4.794460\n",
      "2025-03-08 00:16: Train Epoch 370: 20/23 Loss: 88.591408\n",
      "2025-03-08 00:16: Train Epoch 370: 20/23 Loss: 88.591408\n",
      "2025-03-08 00:16: Train Epoch 370: 20/23 Loss: 88.591408\n",
      "2025-03-08 00:16: **********Train Epoch 370: averaged Loss: 41.767541\n",
      "2025-03-08 00:16: **********Train Epoch 370: averaged Loss: 41.767541\n",
      "2025-03-08 00:16: **********Train Epoch 370: averaged Loss: 41.767541\n",
      "2025-03-08 00:16: **********Val Epoch 370: average Loss: 2167.238037\n",
      "2025-03-08 00:16: **********Val Epoch 370: average Loss: 2167.238037\n",
      "2025-03-08 00:16: **********Val Epoch 370: average Loss: 2167.238037\n",
      "2025-03-08 00:16: Train Epoch 371: 0/23 Loss: 10.865904\n",
      "2025-03-08 00:16: Train Epoch 371: 0/23 Loss: 10.865904\n",
      "2025-03-08 00:16: Train Epoch 371: 0/23 Loss: 10.865904\n",
      "2025-03-08 00:16: Train Epoch 371: 20/23 Loss: 98.213806\n",
      "2025-03-08 00:16: Train Epoch 371: 20/23 Loss: 98.213806\n",
      "2025-03-08 00:16: Train Epoch 371: 20/23 Loss: 98.213806\n",
      "2025-03-08 00:16: **********Train Epoch 371: averaged Loss: 36.559607\n",
      "2025-03-08 00:16: **********Train Epoch 371: averaged Loss: 36.559607\n",
      "2025-03-08 00:16: **********Train Epoch 371: averaged Loss: 36.559607\n",
      "2025-03-08 00:16: **********Val Epoch 371: average Loss: 2174.040894\n",
      "2025-03-08 00:16: **********Val Epoch 371: average Loss: 2174.040894\n",
      "2025-03-08 00:16: **********Val Epoch 371: average Loss: 2174.040894\n",
      "2025-03-08 00:16: Train Epoch 372: 0/23 Loss: 1.471259\n",
      "2025-03-08 00:16: Train Epoch 372: 0/23 Loss: 1.471259\n",
      "2025-03-08 00:16: Train Epoch 372: 0/23 Loss: 1.471259\n",
      "2025-03-08 00:16: Train Epoch 372: 20/23 Loss: 129.937164\n",
      "2025-03-08 00:16: Train Epoch 372: 20/23 Loss: 129.937164\n",
      "2025-03-08 00:16: Train Epoch 372: 20/23 Loss: 129.937164\n",
      "2025-03-08 00:16: **********Train Epoch 372: averaged Loss: 36.897046\n",
      "2025-03-08 00:16: **********Train Epoch 372: averaged Loss: 36.897046\n",
      "2025-03-08 00:16: **********Train Epoch 372: averaged Loss: 36.897046\n",
      "2025-03-08 00:16: **********Val Epoch 372: average Loss: 2160.431396\n",
      "2025-03-08 00:16: **********Val Epoch 372: average Loss: 2160.431396\n",
      "2025-03-08 00:16: **********Val Epoch 372: average Loss: 2160.431396\n",
      "2025-03-08 00:16: Train Epoch 373: 0/23 Loss: 9.272992\n",
      "2025-03-08 00:16: Train Epoch 373: 0/23 Loss: 9.272992\n",
      "2025-03-08 00:16: Train Epoch 373: 0/23 Loss: 9.272992\n",
      "2025-03-08 00:16: Train Epoch 373: 20/23 Loss: 97.520859\n",
      "2025-03-08 00:16: Train Epoch 373: 20/23 Loss: 97.520859\n",
      "2025-03-08 00:16: Train Epoch 373: 20/23 Loss: 97.520859\n",
      "2025-03-08 00:16: **********Train Epoch 373: averaged Loss: 49.034305\n",
      "2025-03-08 00:16: **********Train Epoch 373: averaged Loss: 49.034305\n",
      "2025-03-08 00:16: **********Train Epoch 373: averaged Loss: 49.034305\n",
      "2025-03-08 00:16: **********Val Epoch 373: average Loss: 2146.535522\n",
      "2025-03-08 00:16: **********Val Epoch 373: average Loss: 2146.535522\n",
      "2025-03-08 00:16: **********Val Epoch 373: average Loss: 2146.535522\n",
      "2025-03-08 00:16: Train Epoch 374: 0/23 Loss: 2.643873\n",
      "2025-03-08 00:16: Train Epoch 374: 0/23 Loss: 2.643873\n",
      "2025-03-08 00:16: Train Epoch 374: 0/23 Loss: 2.643873\n",
      "2025-03-08 00:16: Train Epoch 374: 20/23 Loss: 75.979263\n",
      "2025-03-08 00:16: Train Epoch 374: 20/23 Loss: 75.979263\n",
      "2025-03-08 00:16: Train Epoch 374: 20/23 Loss: 75.979263\n",
      "2025-03-08 00:16: **********Train Epoch 374: averaged Loss: 38.376544\n",
      "2025-03-08 00:16: **********Train Epoch 374: averaged Loss: 38.376544\n",
      "2025-03-08 00:16: **********Train Epoch 374: averaged Loss: 38.376544\n",
      "2025-03-08 00:16: **********Val Epoch 374: average Loss: 2157.025757\n",
      "2025-03-08 00:16: **********Val Epoch 374: average Loss: 2157.025757\n",
      "2025-03-08 00:16: **********Val Epoch 374: average Loss: 2157.025757\n",
      "2025-03-08 00:16: Train Epoch 375: 0/23 Loss: 17.138676\n",
      "2025-03-08 00:16: Train Epoch 375: 0/23 Loss: 17.138676\n",
      "2025-03-08 00:16: Train Epoch 375: 0/23 Loss: 17.138676\n",
      "2025-03-08 00:16: Train Epoch 375: 20/23 Loss: 70.084152\n",
      "2025-03-08 00:16: Train Epoch 375: 20/23 Loss: 70.084152\n",
      "2025-03-08 00:16: Train Epoch 375: 20/23 Loss: 70.084152\n",
      "2025-03-08 00:16: **********Train Epoch 375: averaged Loss: 37.400649\n",
      "2025-03-08 00:16: **********Train Epoch 375: averaged Loss: 37.400649\n",
      "2025-03-08 00:16: **********Train Epoch 375: averaged Loss: 37.400649\n",
      "2025-03-08 00:16: **********Val Epoch 375: average Loss: 2149.537964\n",
      "2025-03-08 00:16: **********Val Epoch 375: average Loss: 2149.537964\n",
      "2025-03-08 00:16: **********Val Epoch 375: average Loss: 2149.537964\n",
      "2025-03-08 00:16: Train Epoch 376: 0/23 Loss: 1.584144\n",
      "2025-03-08 00:16: Train Epoch 376: 0/23 Loss: 1.584144\n",
      "2025-03-08 00:16: Train Epoch 376: 0/23 Loss: 1.584144\n",
      "2025-03-08 00:16: Train Epoch 376: 20/23 Loss: 100.367081\n",
      "2025-03-08 00:16: Train Epoch 376: 20/23 Loss: 100.367081\n",
      "2025-03-08 00:16: Train Epoch 376: 20/23 Loss: 100.367081\n",
      "2025-03-08 00:16: **********Train Epoch 376: averaged Loss: 36.208210\n",
      "2025-03-08 00:16: **********Train Epoch 376: averaged Loss: 36.208210\n",
      "2025-03-08 00:16: **********Train Epoch 376: averaged Loss: 36.208210\n",
      "2025-03-08 00:16: **********Val Epoch 376: average Loss: 2174.736084\n",
      "2025-03-08 00:16: **********Val Epoch 376: average Loss: 2174.736084\n",
      "2025-03-08 00:16: **********Val Epoch 376: average Loss: 2174.736084\n",
      "2025-03-08 00:16: Train Epoch 377: 0/23 Loss: 7.197580\n",
      "2025-03-08 00:16: Train Epoch 377: 0/23 Loss: 7.197580\n",
      "2025-03-08 00:16: Train Epoch 377: 0/23 Loss: 7.197580\n",
      "2025-03-08 00:16: Train Epoch 377: 20/23 Loss: 89.867966\n",
      "2025-03-08 00:16: Train Epoch 377: 20/23 Loss: 89.867966\n",
      "2025-03-08 00:16: Train Epoch 377: 20/23 Loss: 89.867966\n",
      "2025-03-08 00:16: **********Train Epoch 377: averaged Loss: 41.846974\n",
      "2025-03-08 00:16: **********Train Epoch 377: averaged Loss: 41.846974\n",
      "2025-03-08 00:16: **********Train Epoch 377: averaged Loss: 41.846974\n",
      "2025-03-08 00:16: **********Val Epoch 377: average Loss: 2133.920532\n",
      "2025-03-08 00:16: **********Val Epoch 377: average Loss: 2133.920532\n",
      "2025-03-08 00:16: **********Val Epoch 377: average Loss: 2133.920532\n",
      "2025-03-08 00:16: Train Epoch 378: 0/23 Loss: 3.308315\n",
      "2025-03-08 00:16: Train Epoch 378: 0/23 Loss: 3.308315\n",
      "2025-03-08 00:16: Train Epoch 378: 0/23 Loss: 3.308315\n",
      "2025-03-08 00:16: Train Epoch 378: 20/23 Loss: 99.102623\n",
      "2025-03-08 00:16: Train Epoch 378: 20/23 Loss: 99.102623\n",
      "2025-03-08 00:16: Train Epoch 378: 20/23 Loss: 99.102623\n",
      "2025-03-08 00:16: **********Train Epoch 378: averaged Loss: 40.720962\n",
      "2025-03-08 00:16: **********Train Epoch 378: averaged Loss: 40.720962\n",
      "2025-03-08 00:16: **********Train Epoch 378: averaged Loss: 40.720962\n",
      "2025-03-08 00:16: **********Val Epoch 378: average Loss: 2152.012512\n",
      "2025-03-08 00:16: **********Val Epoch 378: average Loss: 2152.012512\n",
      "2025-03-08 00:16: **********Val Epoch 378: average Loss: 2152.012512\n",
      "2025-03-08 00:16: Train Epoch 379: 0/23 Loss: 13.981044\n",
      "2025-03-08 00:16: Train Epoch 379: 0/23 Loss: 13.981044\n",
      "2025-03-08 00:16: Train Epoch 379: 0/23 Loss: 13.981044\n",
      "2025-03-08 00:17: Train Epoch 379: 20/23 Loss: 89.778305\n",
      "2025-03-08 00:17: Train Epoch 379: 20/23 Loss: 89.778305\n",
      "2025-03-08 00:17: Train Epoch 379: 20/23 Loss: 89.778305\n",
      "2025-03-08 00:17: **********Train Epoch 379: averaged Loss: 50.931442\n",
      "2025-03-08 00:17: **********Train Epoch 379: averaged Loss: 50.931442\n",
      "2025-03-08 00:17: **********Train Epoch 379: averaged Loss: 50.931442\n",
      "2025-03-08 00:17: **********Val Epoch 379: average Loss: 2152.435486\n",
      "2025-03-08 00:17: **********Val Epoch 379: average Loss: 2152.435486\n",
      "2025-03-08 00:17: **********Val Epoch 379: average Loss: 2152.435486\n",
      "2025-03-08 00:17: Train Epoch 380: 0/23 Loss: 4.025228\n",
      "2025-03-08 00:17: Train Epoch 380: 0/23 Loss: 4.025228\n",
      "2025-03-08 00:17: Train Epoch 380: 0/23 Loss: 4.025228\n",
      "2025-03-08 00:17: Train Epoch 380: 20/23 Loss: 66.449135\n",
      "2025-03-08 00:17: Train Epoch 380: 20/23 Loss: 66.449135\n",
      "2025-03-08 00:17: Train Epoch 380: 20/23 Loss: 66.449135\n",
      "2025-03-08 00:17: **********Train Epoch 380: averaged Loss: 39.851356\n",
      "2025-03-08 00:17: **********Train Epoch 380: averaged Loss: 39.851356\n",
      "2025-03-08 00:17: **********Train Epoch 380: averaged Loss: 39.851356\n",
      "2025-03-08 00:17: **********Val Epoch 380: average Loss: 2155.859375\n",
      "2025-03-08 00:17: **********Val Epoch 380: average Loss: 2155.859375\n",
      "2025-03-08 00:17: **********Val Epoch 380: average Loss: 2155.859375\n",
      "2025-03-08 00:17: Train Epoch 381: 0/23 Loss: 12.797058\n",
      "2025-03-08 00:17: Train Epoch 381: 0/23 Loss: 12.797058\n",
      "2025-03-08 00:17: Train Epoch 381: 0/23 Loss: 12.797058\n",
      "2025-03-08 00:17: Train Epoch 381: 20/23 Loss: 65.034248\n",
      "2025-03-08 00:17: Train Epoch 381: 20/23 Loss: 65.034248\n",
      "2025-03-08 00:17: Train Epoch 381: 20/23 Loss: 65.034248\n",
      "2025-03-08 00:17: **********Train Epoch 381: averaged Loss: 34.785362\n",
      "2025-03-08 00:17: **********Train Epoch 381: averaged Loss: 34.785362\n",
      "2025-03-08 00:17: **********Train Epoch 381: averaged Loss: 34.785362\n",
      "2025-03-08 00:17: **********Val Epoch 381: average Loss: 2138.387268\n",
      "2025-03-08 00:17: **********Val Epoch 381: average Loss: 2138.387268\n",
      "2025-03-08 00:17: **********Val Epoch 381: average Loss: 2138.387268\n",
      "2025-03-08 00:17: Train Epoch 382: 0/23 Loss: 1.829600\n",
      "2025-03-08 00:17: Train Epoch 382: 0/23 Loss: 1.829600\n",
      "2025-03-08 00:17: Train Epoch 382: 0/23 Loss: 1.829600\n",
      "2025-03-08 00:17: Train Epoch 382: 20/23 Loss: 93.918198\n",
      "2025-03-08 00:17: Train Epoch 382: 20/23 Loss: 93.918198\n",
      "2025-03-08 00:17: Train Epoch 382: 20/23 Loss: 93.918198\n",
      "2025-03-08 00:17: **********Train Epoch 382: averaged Loss: 33.962216\n",
      "2025-03-08 00:17: **********Train Epoch 382: averaged Loss: 33.962216\n",
      "2025-03-08 00:17: **********Train Epoch 382: averaged Loss: 33.962216\n",
      "2025-03-08 00:17: **********Val Epoch 382: average Loss: 2139.048340\n",
      "2025-03-08 00:17: **********Val Epoch 382: average Loss: 2139.048340\n",
      "2025-03-08 00:17: **********Val Epoch 382: average Loss: 2139.048340\n",
      "2025-03-08 00:17: Train Epoch 383: 0/23 Loss: 5.809833\n",
      "2025-03-08 00:17: Train Epoch 383: 0/23 Loss: 5.809833\n",
      "2025-03-08 00:17: Train Epoch 383: 0/23 Loss: 5.809833\n",
      "2025-03-08 00:17: Train Epoch 383: 20/23 Loss: 89.783768\n",
      "2025-03-08 00:17: Train Epoch 383: 20/23 Loss: 89.783768\n",
      "2025-03-08 00:17: Train Epoch 383: 20/23 Loss: 89.783768\n",
      "2025-03-08 00:17: **********Train Epoch 383: averaged Loss: 41.343977\n",
      "2025-03-08 00:17: **********Train Epoch 383: averaged Loss: 41.343977\n",
      "2025-03-08 00:17: **********Train Epoch 383: averaged Loss: 41.343977\n",
      "2025-03-08 00:17: **********Val Epoch 383: average Loss: 2156.641846\n",
      "2025-03-08 00:17: **********Val Epoch 383: average Loss: 2156.641846\n",
      "2025-03-08 00:17: **********Val Epoch 383: average Loss: 2156.641846\n",
      "2025-03-08 00:17: Train Epoch 384: 0/23 Loss: 1.740345\n",
      "2025-03-08 00:17: Train Epoch 384: 0/23 Loss: 1.740345\n",
      "2025-03-08 00:17: Train Epoch 384: 0/23 Loss: 1.740345\n",
      "2025-03-08 00:17: Train Epoch 384: 20/23 Loss: 96.795837\n",
      "2025-03-08 00:17: Train Epoch 384: 20/23 Loss: 96.795837\n",
      "2025-03-08 00:17: Train Epoch 384: 20/23 Loss: 96.795837\n",
      "2025-03-08 00:17: **********Train Epoch 384: averaged Loss: 37.415295\n",
      "2025-03-08 00:17: **********Train Epoch 384: averaged Loss: 37.415295\n",
      "2025-03-08 00:17: **********Train Epoch 384: averaged Loss: 37.415295\n",
      "2025-03-08 00:17: **********Val Epoch 384: average Loss: 2152.745117\n",
      "2025-03-08 00:17: **********Val Epoch 384: average Loss: 2152.745117\n",
      "2025-03-08 00:17: **********Val Epoch 384: average Loss: 2152.745117\n",
      "2025-03-08 00:17: Train Epoch 385: 0/23 Loss: 15.499139\n",
      "2025-03-08 00:17: Train Epoch 385: 0/23 Loss: 15.499139\n",
      "2025-03-08 00:17: Train Epoch 385: 0/23 Loss: 15.499139\n",
      "2025-03-08 00:17: Train Epoch 385: 20/23 Loss: 76.801376\n",
      "2025-03-08 00:17: Train Epoch 385: 20/23 Loss: 76.801376\n",
      "2025-03-08 00:17: Train Epoch 385: 20/23 Loss: 76.801376\n",
      "2025-03-08 00:17: **********Train Epoch 385: averaged Loss: 37.999851\n",
      "2025-03-08 00:17: **********Train Epoch 385: averaged Loss: 37.999851\n",
      "2025-03-08 00:17: **********Train Epoch 385: averaged Loss: 37.999851\n",
      "2025-03-08 00:17: **********Val Epoch 385: average Loss: 2133.262573\n",
      "2025-03-08 00:17: **********Val Epoch 385: average Loss: 2133.262573\n",
      "2025-03-08 00:17: **********Val Epoch 385: average Loss: 2133.262573\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 386: 0/23 Loss: 2.300107\n",
      "2025-03-08 00:17: Train Epoch 386: 0/23 Loss: 2.300107\n",
      "2025-03-08 00:17: Train Epoch 386: 0/23 Loss: 2.300107\n",
      "2025-03-08 00:17: Train Epoch 386: 20/23 Loss: 137.732590\n",
      "2025-03-08 00:17: Train Epoch 386: 20/23 Loss: 137.732590\n",
      "2025-03-08 00:17: Train Epoch 386: 20/23 Loss: 137.732590\n",
      "2025-03-08 00:17: **********Train Epoch 386: averaged Loss: 43.464075\n",
      "2025-03-08 00:17: **********Train Epoch 386: averaged Loss: 43.464075\n",
      "2025-03-08 00:17: **********Train Epoch 386: averaged Loss: 43.464075\n",
      "2025-03-08 00:17: **********Val Epoch 386: average Loss: 2128.183716\n",
      "2025-03-08 00:17: **********Val Epoch 386: average Loss: 2128.183716\n",
      "2025-03-08 00:17: **********Val Epoch 386: average Loss: 2128.183716\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 387: 0/23 Loss: 7.930978\n",
      "2025-03-08 00:17: Train Epoch 387: 0/23 Loss: 7.930978\n",
      "2025-03-08 00:17: Train Epoch 387: 0/23 Loss: 7.930978\n",
      "2025-03-08 00:17: Train Epoch 387: 20/23 Loss: 106.295731\n",
      "2025-03-08 00:17: Train Epoch 387: 20/23 Loss: 106.295731\n",
      "2025-03-08 00:17: Train Epoch 387: 20/23 Loss: 106.295731\n",
      "2025-03-08 00:17: **********Train Epoch 387: averaged Loss: 65.881690\n",
      "2025-03-08 00:17: **********Train Epoch 387: averaged Loss: 65.881690\n",
      "2025-03-08 00:17: **********Train Epoch 387: averaged Loss: 65.881690\n",
      "2025-03-08 00:17: **********Val Epoch 387: average Loss: 2124.660156\n",
      "2025-03-08 00:17: **********Val Epoch 387: average Loss: 2124.660156\n",
      "2025-03-08 00:17: **********Val Epoch 387: average Loss: 2124.660156\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 388: 0/23 Loss: 2.850613\n",
      "2025-03-08 00:17: Train Epoch 388: 0/23 Loss: 2.850613\n",
      "2025-03-08 00:17: Train Epoch 388: 0/23 Loss: 2.850613\n",
      "2025-03-08 00:17: Train Epoch 388: 20/23 Loss: 106.966125\n",
      "2025-03-08 00:17: Train Epoch 388: 20/23 Loss: 106.966125\n",
      "2025-03-08 00:17: Train Epoch 388: 20/23 Loss: 106.966125\n",
      "2025-03-08 00:17: **********Train Epoch 388: averaged Loss: 41.101770\n",
      "2025-03-08 00:17: **********Train Epoch 388: averaged Loss: 41.101770\n",
      "2025-03-08 00:17: **********Train Epoch 388: averaged Loss: 41.101770\n",
      "2025-03-08 00:17: **********Val Epoch 388: average Loss: 2147.488403\n",
      "2025-03-08 00:17: **********Val Epoch 388: average Loss: 2147.488403\n",
      "2025-03-08 00:17: **********Val Epoch 388: average Loss: 2147.488403\n",
      "2025-03-08 00:17: Train Epoch 389: 0/23 Loss: 11.845345\n",
      "2025-03-08 00:17: Train Epoch 389: 0/23 Loss: 11.845345\n",
      "2025-03-08 00:17: Train Epoch 389: 0/23 Loss: 11.845345\n",
      "2025-03-08 00:17: Train Epoch 389: 20/23 Loss: 85.148796\n",
      "2025-03-08 00:17: Train Epoch 389: 20/23 Loss: 85.148796\n",
      "2025-03-08 00:17: Train Epoch 389: 20/23 Loss: 85.148796\n",
      "2025-03-08 00:17: **********Train Epoch 389: averaged Loss: 38.199557\n",
      "2025-03-08 00:17: **********Train Epoch 389: averaged Loss: 38.199557\n",
      "2025-03-08 00:17: **********Train Epoch 389: averaged Loss: 38.199557\n",
      "2025-03-08 00:17: **********Val Epoch 389: average Loss: 2147.210022\n",
      "2025-03-08 00:17: **********Val Epoch 389: average Loss: 2147.210022\n",
      "2025-03-08 00:17: **********Val Epoch 389: average Loss: 2147.210022\n",
      "2025-03-08 00:17: Train Epoch 390: 0/23 Loss: 2.771712\n",
      "2025-03-08 00:17: Train Epoch 390: 0/23 Loss: 2.771712\n",
      "2025-03-08 00:17: Train Epoch 390: 0/23 Loss: 2.771712\n",
      "2025-03-08 00:17: Train Epoch 390: 20/23 Loss: 82.909767\n",
      "2025-03-08 00:17: Train Epoch 390: 20/23 Loss: 82.909767\n",
      "2025-03-08 00:17: Train Epoch 390: 20/23 Loss: 82.909767\n",
      "2025-03-08 00:17: **********Train Epoch 390: averaged Loss: 33.406159\n",
      "2025-03-08 00:17: **********Train Epoch 390: averaged Loss: 33.406159\n",
      "2025-03-08 00:17: **********Train Epoch 390: averaged Loss: 33.406159\n",
      "2025-03-08 00:17: **********Val Epoch 390: average Loss: 2148.094482\n",
      "2025-03-08 00:17: **********Val Epoch 390: average Loss: 2148.094482\n",
      "2025-03-08 00:17: **********Val Epoch 390: average Loss: 2148.094482\n",
      "2025-03-08 00:17: Train Epoch 391: 0/23 Loss: 1.977946\n",
      "2025-03-08 00:17: Train Epoch 391: 0/23 Loss: 1.977946\n",
      "2025-03-08 00:17: Train Epoch 391: 0/23 Loss: 1.977946\n",
      "2025-03-08 00:17: Train Epoch 391: 20/23 Loss: 78.601395\n",
      "2025-03-08 00:17: Train Epoch 391: 20/23 Loss: 78.601395\n",
      "2025-03-08 00:17: Train Epoch 391: 20/23 Loss: 78.601395\n",
      "2025-03-08 00:17: **********Train Epoch 391: averaged Loss: 32.789965\n",
      "2025-03-08 00:17: **********Train Epoch 391: averaged Loss: 32.789965\n",
      "2025-03-08 00:17: **********Train Epoch 391: averaged Loss: 32.789965\n",
      "2025-03-08 00:17: **********Val Epoch 391: average Loss: 2133.221741\n",
      "2025-03-08 00:17: **********Val Epoch 391: average Loss: 2133.221741\n",
      "2025-03-08 00:17: **********Val Epoch 391: average Loss: 2133.221741\n",
      "2025-03-08 00:17: Train Epoch 392: 0/23 Loss: 1.420867\n",
      "2025-03-08 00:17: Train Epoch 392: 0/23 Loss: 1.420867\n",
      "2025-03-08 00:17: Train Epoch 392: 0/23 Loss: 1.420867\n",
      "2025-03-08 00:17: Train Epoch 392: 20/23 Loss: 112.235245\n",
      "2025-03-08 00:17: Train Epoch 392: 20/23 Loss: 112.235245\n",
      "2025-03-08 00:17: Train Epoch 392: 20/23 Loss: 112.235245\n",
      "2025-03-08 00:17: **********Train Epoch 392: averaged Loss: 37.852776\n",
      "2025-03-08 00:17: **********Train Epoch 392: averaged Loss: 37.852776\n",
      "2025-03-08 00:17: **********Train Epoch 392: averaged Loss: 37.852776\n",
      "2025-03-08 00:17: **********Val Epoch 392: average Loss: 2137.525452\n",
      "2025-03-08 00:17: **********Val Epoch 392: average Loss: 2137.525452\n",
      "2025-03-08 00:17: **********Val Epoch 392: average Loss: 2137.525452\n",
      "2025-03-08 00:17: Train Epoch 393: 0/23 Loss: 13.632581\n",
      "2025-03-08 00:17: Train Epoch 393: 0/23 Loss: 13.632581\n",
      "2025-03-08 00:17: Train Epoch 393: 0/23 Loss: 13.632581\n",
      "2025-03-08 00:17: Train Epoch 393: 20/23 Loss: 97.905464\n",
      "2025-03-08 00:17: Train Epoch 393: 20/23 Loss: 97.905464\n",
      "2025-03-08 00:17: Train Epoch 393: 20/23 Loss: 97.905464\n",
      "2025-03-08 00:17: **********Train Epoch 393: averaged Loss: 53.948353\n",
      "2025-03-08 00:17: **********Train Epoch 393: averaged Loss: 53.948353\n",
      "2025-03-08 00:17: **********Train Epoch 393: averaged Loss: 53.948353\n",
      "2025-03-08 00:17: **********Val Epoch 393: average Loss: 2128.482910\n",
      "2025-03-08 00:17: **********Val Epoch 393: average Loss: 2128.482910\n",
      "2025-03-08 00:17: **********Val Epoch 393: average Loss: 2128.482910\n",
      "2025-03-08 00:17: Train Epoch 394: 0/23 Loss: 6.345852\n",
      "2025-03-08 00:17: Train Epoch 394: 0/23 Loss: 6.345852\n",
      "2025-03-08 00:17: Train Epoch 394: 0/23 Loss: 6.345852\n",
      "2025-03-08 00:17: Train Epoch 394: 20/23 Loss: 68.266823\n",
      "2025-03-08 00:17: Train Epoch 394: 20/23 Loss: 68.266823\n",
      "2025-03-08 00:17: Train Epoch 394: 20/23 Loss: 68.266823\n",
      "2025-03-08 00:17: **********Train Epoch 394: averaged Loss: 38.730721\n",
      "2025-03-08 00:17: **********Train Epoch 394: averaged Loss: 38.730721\n",
      "2025-03-08 00:17: **********Train Epoch 394: averaged Loss: 38.730721\n",
      "2025-03-08 00:17: **********Val Epoch 394: average Loss: 2123.256897\n",
      "2025-03-08 00:17: **********Val Epoch 394: average Loss: 2123.256897\n",
      "2025-03-08 00:17: **********Val Epoch 394: average Loss: 2123.256897\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 395: 0/23 Loss: 8.904654\n",
      "2025-03-08 00:17: Train Epoch 395: 0/23 Loss: 8.904654\n",
      "2025-03-08 00:17: Train Epoch 395: 0/23 Loss: 8.904654\n",
      "2025-03-08 00:17: Train Epoch 395: 20/23 Loss: 64.952065\n",
      "2025-03-08 00:17: Train Epoch 395: 20/23 Loss: 64.952065\n",
      "2025-03-08 00:17: Train Epoch 395: 20/23 Loss: 64.952065\n",
      "2025-03-08 00:17: **********Train Epoch 395: averaged Loss: 33.541885\n",
      "2025-03-08 00:17: **********Train Epoch 395: averaged Loss: 33.541885\n",
      "2025-03-08 00:17: **********Train Epoch 395: averaged Loss: 33.541885\n",
      "2025-03-08 00:17: **********Val Epoch 395: average Loss: 2133.030640\n",
      "2025-03-08 00:17: **********Val Epoch 395: average Loss: 2133.030640\n",
      "2025-03-08 00:17: **********Val Epoch 395: average Loss: 2133.030640\n",
      "2025-03-08 00:17: Train Epoch 396: 0/23 Loss: 1.416419\n",
      "2025-03-08 00:17: Train Epoch 396: 0/23 Loss: 1.416419\n",
      "2025-03-08 00:17: Train Epoch 396: 0/23 Loss: 1.416419\n",
      "2025-03-08 00:17: Train Epoch 396: 20/23 Loss: 106.195648\n",
      "2025-03-08 00:17: Train Epoch 396: 20/23 Loss: 106.195648\n",
      "2025-03-08 00:17: Train Epoch 396: 20/23 Loss: 106.195648\n",
      "2025-03-08 00:17: **********Train Epoch 396: averaged Loss: 32.502338\n",
      "2025-03-08 00:17: **********Train Epoch 396: averaged Loss: 32.502338\n",
      "2025-03-08 00:17: **********Train Epoch 396: averaged Loss: 32.502338\n",
      "2025-03-08 00:17: **********Val Epoch 396: average Loss: 2118.241882\n",
      "2025-03-08 00:17: **********Val Epoch 396: average Loss: 2118.241882\n",
      "2025-03-08 00:17: **********Val Epoch 396: average Loss: 2118.241882\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 397: 0/23 Loss: 5.511036\n",
      "2025-03-08 00:17: Train Epoch 397: 0/23 Loss: 5.511036\n",
      "2025-03-08 00:17: Train Epoch 397: 0/23 Loss: 5.511036\n",
      "2025-03-08 00:17: Train Epoch 397: 20/23 Loss: 85.500610\n",
      "2025-03-08 00:17: Train Epoch 397: 20/23 Loss: 85.500610\n",
      "2025-03-08 00:17: Train Epoch 397: 20/23 Loss: 85.500610\n",
      "2025-03-08 00:17: **********Train Epoch 397: averaged Loss: 41.016187\n",
      "2025-03-08 00:17: **********Train Epoch 397: averaged Loss: 41.016187\n",
      "2025-03-08 00:17: **********Train Epoch 397: averaged Loss: 41.016187\n",
      "2025-03-08 00:17: **********Val Epoch 397: average Loss: 2110.805725\n",
      "2025-03-08 00:17: **********Val Epoch 397: average Loss: 2110.805725\n",
      "2025-03-08 00:17: **********Val Epoch 397: average Loss: 2110.805725\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 398: 0/23 Loss: 2.171537\n",
      "2025-03-08 00:17: Train Epoch 398: 0/23 Loss: 2.171537\n",
      "2025-03-08 00:17: Train Epoch 398: 0/23 Loss: 2.171537\n",
      "2025-03-08 00:17: Train Epoch 398: 20/23 Loss: 103.587921\n",
      "2025-03-08 00:17: Train Epoch 398: 20/23 Loss: 103.587921\n",
      "2025-03-08 00:17: Train Epoch 398: 20/23 Loss: 103.587921\n",
      "2025-03-08 00:17: **********Train Epoch 398: averaged Loss: 38.318404\n",
      "2025-03-08 00:17: **********Train Epoch 398: averaged Loss: 38.318404\n",
      "2025-03-08 00:17: **********Train Epoch 398: averaged Loss: 38.318404\n",
      "2025-03-08 00:17: **********Val Epoch 398: average Loss: 2134.802124\n",
      "2025-03-08 00:17: **********Val Epoch 398: average Loss: 2134.802124\n",
      "2025-03-08 00:17: **********Val Epoch 398: average Loss: 2134.802124\n",
      "2025-03-08 00:17: Train Epoch 399: 0/23 Loss: 9.043886\n",
      "2025-03-08 00:17: Train Epoch 399: 0/23 Loss: 9.043886\n",
      "2025-03-08 00:17: Train Epoch 399: 0/23 Loss: 9.043886\n",
      "2025-03-08 00:17: Train Epoch 399: 20/23 Loss: 92.142578\n",
      "2025-03-08 00:17: Train Epoch 399: 20/23 Loss: 92.142578\n",
      "2025-03-08 00:17: Train Epoch 399: 20/23 Loss: 92.142578\n",
      "2025-03-08 00:17: **********Train Epoch 399: averaged Loss: 38.746125\n",
      "2025-03-08 00:17: **********Train Epoch 399: averaged Loss: 38.746125\n",
      "2025-03-08 00:17: **********Train Epoch 399: averaged Loss: 38.746125\n",
      "2025-03-08 00:17: **********Val Epoch 399: average Loss: 2126.588867\n",
      "2025-03-08 00:17: **********Val Epoch 399: average Loss: 2126.588867\n",
      "2025-03-08 00:17: **********Val Epoch 399: average Loss: 2126.588867\n",
      "2025-03-08 00:17: Train Epoch 400: 0/23 Loss: 4.735848\n",
      "2025-03-08 00:17: Train Epoch 400: 0/23 Loss: 4.735848\n",
      "2025-03-08 00:17: Train Epoch 400: 0/23 Loss: 4.735848\n",
      "2025-03-08 00:17: Train Epoch 400: 20/23 Loss: 101.784912\n",
      "2025-03-08 00:17: Train Epoch 400: 20/23 Loss: 101.784912\n",
      "2025-03-08 00:17: Train Epoch 400: 20/23 Loss: 101.784912\n",
      "2025-03-08 00:17: **********Train Epoch 400: averaged Loss: 39.035797\n",
      "2025-03-08 00:17: **********Train Epoch 400: averaged Loss: 39.035797\n",
      "2025-03-08 00:17: **********Train Epoch 400: averaged Loss: 39.035797\n",
      "2025-03-08 00:17: **********Val Epoch 400: average Loss: 2100.469421\n",
      "2025-03-08 00:17: **********Val Epoch 400: average Loss: 2100.469421\n",
      "2025-03-08 00:17: **********Val Epoch 400: average Loss: 2100.469421\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 401: 0/23 Loss: 16.103720\n",
      "2025-03-08 00:17: Train Epoch 401: 0/23 Loss: 16.103720\n",
      "2025-03-08 00:17: Train Epoch 401: 0/23 Loss: 16.103720\n",
      "2025-03-08 00:17: Train Epoch 401: 20/23 Loss: 104.702011\n",
      "2025-03-08 00:17: Train Epoch 401: 20/23 Loss: 104.702011\n",
      "2025-03-08 00:17: Train Epoch 401: 20/23 Loss: 104.702011\n",
      "2025-03-08 00:17: **********Train Epoch 401: averaged Loss: 57.110369\n",
      "2025-03-08 00:17: **********Train Epoch 401: averaged Loss: 57.110369\n",
      "2025-03-08 00:17: **********Train Epoch 401: averaged Loss: 57.110369\n",
      "2025-03-08 00:17: **********Val Epoch 401: average Loss: 2115.471008\n",
      "2025-03-08 00:17: **********Val Epoch 401: average Loss: 2115.471008\n",
      "2025-03-08 00:17: **********Val Epoch 401: average Loss: 2115.471008\n",
      "2025-03-08 00:17: Train Epoch 402: 0/23 Loss: 3.231380\n",
      "2025-03-08 00:17: Train Epoch 402: 0/23 Loss: 3.231380\n",
      "2025-03-08 00:17: Train Epoch 402: 0/23 Loss: 3.231380\n",
      "2025-03-08 00:17: Train Epoch 402: 20/23 Loss: 72.737747\n",
      "2025-03-08 00:17: Train Epoch 402: 20/23 Loss: 72.737747\n",
      "2025-03-08 00:17: Train Epoch 402: 20/23 Loss: 72.737747\n",
      "2025-03-08 00:17: **********Train Epoch 402: averaged Loss: 38.196518\n",
      "2025-03-08 00:17: **********Train Epoch 402: averaged Loss: 38.196518\n",
      "2025-03-08 00:17: **********Train Epoch 402: averaged Loss: 38.196518\n",
      "2025-03-08 00:17: **********Val Epoch 402: average Loss: 2116.976257\n",
      "2025-03-08 00:17: **********Val Epoch 402: average Loss: 2116.976257\n",
      "2025-03-08 00:17: **********Val Epoch 402: average Loss: 2116.976257\n",
      "2025-03-08 00:17: Train Epoch 403: 0/23 Loss: 11.727266\n",
      "2025-03-08 00:17: Train Epoch 403: 0/23 Loss: 11.727266\n",
      "2025-03-08 00:17: Train Epoch 403: 0/23 Loss: 11.727266\n",
      "2025-03-08 00:17: Train Epoch 403: 20/23 Loss: 62.158424\n",
      "2025-03-08 00:17: Train Epoch 403: 20/23 Loss: 62.158424\n",
      "2025-03-08 00:17: Train Epoch 403: 20/23 Loss: 62.158424\n",
      "2025-03-08 00:17: **********Train Epoch 403: averaged Loss: 33.845041\n",
      "2025-03-08 00:17: **********Train Epoch 403: averaged Loss: 33.845041\n",
      "2025-03-08 00:17: **********Train Epoch 403: averaged Loss: 33.845041\n",
      "2025-03-08 00:17: **********Val Epoch 403: average Loss: 2120.793457\n",
      "2025-03-08 00:17: **********Val Epoch 403: average Loss: 2120.793457\n",
      "2025-03-08 00:17: **********Val Epoch 403: average Loss: 2120.793457\n",
      "2025-03-08 00:17: Train Epoch 404: 0/23 Loss: 1.413257\n",
      "2025-03-08 00:17: Train Epoch 404: 0/23 Loss: 1.413257\n",
      "2025-03-08 00:17: Train Epoch 404: 0/23 Loss: 1.413257\n",
      "2025-03-08 00:17: Train Epoch 404: 20/23 Loss: 98.117081\n",
      "2025-03-08 00:17: Train Epoch 404: 20/23 Loss: 98.117081\n",
      "2025-03-08 00:17: Train Epoch 404: 20/23 Loss: 98.117081\n",
      "2025-03-08 00:17: **********Train Epoch 404: averaged Loss: 32.086754\n",
      "2025-03-08 00:17: **********Train Epoch 404: averaged Loss: 32.086754\n",
      "2025-03-08 00:17: **********Train Epoch 404: averaged Loss: 32.086754\n",
      "2025-03-08 00:17: **********Val Epoch 404: average Loss: 2127.656006\n",
      "2025-03-08 00:17: **********Val Epoch 404: average Loss: 2127.656006\n",
      "2025-03-08 00:17: **********Val Epoch 404: average Loss: 2127.656006\n",
      "2025-03-08 00:17: Train Epoch 405: 0/23 Loss: 11.550638\n",
      "2025-03-08 00:17: Train Epoch 405: 0/23 Loss: 11.550638\n",
      "2025-03-08 00:17: Train Epoch 405: 0/23 Loss: 11.550638\n",
      "2025-03-08 00:17: Train Epoch 405: 20/23 Loss: 71.857803\n",
      "2025-03-08 00:17: Train Epoch 405: 20/23 Loss: 71.857803\n",
      "2025-03-08 00:17: Train Epoch 405: 20/23 Loss: 71.857803\n",
      "2025-03-08 00:17: **********Train Epoch 405: averaged Loss: 40.070511\n",
      "2025-03-08 00:17: **********Train Epoch 405: averaged Loss: 40.070511\n",
      "2025-03-08 00:17: **********Train Epoch 405: averaged Loss: 40.070511\n",
      "2025-03-08 00:17: **********Val Epoch 405: average Loss: 2122.403259\n",
      "2025-03-08 00:17: **********Val Epoch 405: average Loss: 2122.403259\n",
      "2025-03-08 00:17: **********Val Epoch 405: average Loss: 2122.403259\n",
      "2025-03-08 00:17: Train Epoch 406: 0/23 Loss: 1.994454\n",
      "2025-03-08 00:17: Train Epoch 406: 0/23 Loss: 1.994454\n",
      "2025-03-08 00:17: Train Epoch 406: 0/23 Loss: 1.994454\n",
      "2025-03-08 00:17: Train Epoch 406: 20/23 Loss: 76.748634\n",
      "2025-03-08 00:17: Train Epoch 406: 20/23 Loss: 76.748634\n",
      "2025-03-08 00:17: Train Epoch 406: 20/23 Loss: 76.748634\n",
      "2025-03-08 00:17: **********Train Epoch 406: averaged Loss: 36.156799\n",
      "2025-03-08 00:17: **********Train Epoch 406: averaged Loss: 36.156799\n",
      "2025-03-08 00:17: **********Train Epoch 406: averaged Loss: 36.156799\n",
      "2025-03-08 00:17: **********Val Epoch 406: average Loss: 2133.804077\n",
      "2025-03-08 00:17: **********Val Epoch 406: average Loss: 2133.804077\n",
      "2025-03-08 00:17: **********Val Epoch 406: average Loss: 2133.804077\n",
      "2025-03-08 00:17: Train Epoch 407: 0/23 Loss: 11.942987\n",
      "2025-03-08 00:17: Train Epoch 407: 0/23 Loss: 11.942987\n",
      "2025-03-08 00:17: Train Epoch 407: 0/23 Loss: 11.942987\n",
      "2025-03-08 00:17: Train Epoch 407: 20/23 Loss: 69.737595\n",
      "2025-03-08 00:17: Train Epoch 407: 20/23 Loss: 69.737595\n",
      "2025-03-08 00:17: Train Epoch 407: 20/23 Loss: 69.737595\n",
      "2025-03-08 00:17: **********Train Epoch 407: averaged Loss: 37.591833\n",
      "2025-03-08 00:17: **********Train Epoch 407: averaged Loss: 37.591833\n",
      "2025-03-08 00:17: **********Train Epoch 407: averaged Loss: 37.591833\n",
      "2025-03-08 00:17: **********Val Epoch 407: average Loss: 2104.015869\n",
      "2025-03-08 00:17: **********Val Epoch 407: average Loss: 2104.015869\n",
      "2025-03-08 00:17: **********Val Epoch 407: average Loss: 2104.015869\n",
      "2025-03-08 00:17: Train Epoch 408: 0/23 Loss: 1.721205\n",
      "2025-03-08 00:17: Train Epoch 408: 0/23 Loss: 1.721205\n",
      "2025-03-08 00:17: Train Epoch 408: 0/23 Loss: 1.721205\n",
      "2025-03-08 00:17: Train Epoch 408: 20/23 Loss: 119.282433\n",
      "2025-03-08 00:17: Train Epoch 408: 20/23 Loss: 119.282433\n",
      "2025-03-08 00:17: Train Epoch 408: 20/23 Loss: 119.282433\n",
      "2025-03-08 00:17: **********Train Epoch 408: averaged Loss: 38.767337\n",
      "2025-03-08 00:17: **********Train Epoch 408: averaged Loss: 38.767337\n",
      "2025-03-08 00:17: **********Train Epoch 408: averaged Loss: 38.767337\n",
      "2025-03-08 00:17: **********Val Epoch 408: average Loss: 2094.512878\n",
      "2025-03-08 00:17: **********Val Epoch 408: average Loss: 2094.512878\n",
      "2025-03-08 00:17: **********Val Epoch 408: average Loss: 2094.512878\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 409: 0/23 Loss: 13.660648\n",
      "2025-03-08 00:17: Train Epoch 409: 0/23 Loss: 13.660648\n",
      "2025-03-08 00:17: Train Epoch 409: 0/23 Loss: 13.660648\n",
      "2025-03-08 00:17: Train Epoch 409: 20/23 Loss: 106.517014\n",
      "2025-03-08 00:17: Train Epoch 409: 20/23 Loss: 106.517014\n",
      "2025-03-08 00:17: Train Epoch 409: 20/23 Loss: 106.517014\n",
      "2025-03-08 00:17: **********Train Epoch 409: averaged Loss: 53.007449\n",
      "2025-03-08 00:17: **********Train Epoch 409: averaged Loss: 53.007449\n",
      "2025-03-08 00:17: **********Train Epoch 409: averaged Loss: 53.007449\n",
      "2025-03-08 00:17: **********Val Epoch 409: average Loss: 2090.138733\n",
      "2025-03-08 00:17: **********Val Epoch 409: average Loss: 2090.138733\n",
      "2025-03-08 00:17: **********Val Epoch 409: average Loss: 2090.138733\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: *********************************Current best model saved!\n",
      "2025-03-08 00:17: Train Epoch 410: 0/23 Loss: 7.886215\n",
      "2025-03-08 00:17: Train Epoch 410: 0/23 Loss: 7.886215\n",
      "2025-03-08 00:17: Train Epoch 410: 0/23 Loss: 7.886215\n",
      "2025-03-08 00:17: Train Epoch 410: 20/23 Loss: 75.250465\n",
      "2025-03-08 00:17: Train Epoch 410: 20/23 Loss: 75.250465\n",
      "2025-03-08 00:17: Train Epoch 410: 20/23 Loss: 75.250465\n",
      "2025-03-08 00:17: **********Train Epoch 410: averaged Loss: 39.410295\n",
      "2025-03-08 00:17: **********Train Epoch 410: averaged Loss: 39.410295\n",
      "2025-03-08 00:17: **********Train Epoch 410: averaged Loss: 39.410295\n",
      "2025-03-08 00:17: **********Val Epoch 410: average Loss: 2105.400085\n",
      "2025-03-08 00:17: **********Val Epoch 410: average Loss: 2105.400085\n",
      "2025-03-08 00:17: **********Val Epoch 410: average Loss: 2105.400085\n",
      "2025-03-08 00:17: Train Epoch 411: 0/23 Loss: 13.245147\n",
      "2025-03-08 00:17: Train Epoch 411: 0/23 Loss: 13.245147\n",
      "2025-03-08 00:17: Train Epoch 411: 0/23 Loss: 13.245147\n",
      "2025-03-08 00:17: Train Epoch 411: 20/23 Loss: 62.534416\n",
      "2025-03-08 00:17: Train Epoch 411: 20/23 Loss: 62.534416\n",
      "2025-03-08 00:17: Train Epoch 411: 20/23 Loss: 62.534416\n",
      "2025-03-08 00:17: **********Train Epoch 411: averaged Loss: 33.854989\n",
      "2025-03-08 00:17: **********Train Epoch 411: averaged Loss: 33.854989\n",
      "2025-03-08 00:17: **********Train Epoch 411: averaged Loss: 33.854989\n",
      "2025-03-08 00:17: **********Val Epoch 411: average Loss: 2103.031860\n",
      "2025-03-08 00:17: **********Val Epoch 411: average Loss: 2103.031860\n",
      "2025-03-08 00:17: **********Val Epoch 411: average Loss: 2103.031860\n",
      "2025-03-08 00:17: Train Epoch 412: 0/23 Loss: 1.841538\n",
      "2025-03-08 00:17: Train Epoch 412: 0/23 Loss: 1.841538\n",
      "2025-03-08 00:17: Train Epoch 412: 0/23 Loss: 1.841538\n",
      "2025-03-08 00:17: Train Epoch 412: 20/23 Loss: 92.520355\n",
      "2025-03-08 00:17: Train Epoch 412: 20/23 Loss: 92.520355\n",
      "2025-03-08 00:17: Train Epoch 412: 20/23 Loss: 92.520355\n",
      "2025-03-08 00:17: **********Train Epoch 412: averaged Loss: 31.764294\n",
      "2025-03-08 00:17: **********Train Epoch 412: averaged Loss: 31.764294\n",
      "2025-03-08 00:17: **********Train Epoch 412: averaged Loss: 31.764294\n",
      "2025-03-08 00:17: **********Val Epoch 412: average Loss: 2104.858032\n",
      "2025-03-08 00:17: **********Val Epoch 412: average Loss: 2104.858032\n",
      "2025-03-08 00:17: **********Val Epoch 412: average Loss: 2104.858032\n",
      "2025-03-08 00:17: Train Epoch 413: 0/23 Loss: 5.387548\n",
      "2025-03-08 00:17: Train Epoch 413: 0/23 Loss: 5.387548\n",
      "2025-03-08 00:17: Train Epoch 413: 0/23 Loss: 5.387548\n",
      "2025-03-08 00:17: Train Epoch 413: 20/23 Loss: 93.370453\n",
      "2025-03-08 00:17: Train Epoch 413: 20/23 Loss: 93.370453\n",
      "2025-03-08 00:17: Train Epoch 413: 20/23 Loss: 93.370453\n",
      "2025-03-08 00:17: **********Train Epoch 413: averaged Loss: 40.881846\n",
      "2025-03-08 00:17: **********Train Epoch 413: averaged Loss: 40.881846\n",
      "2025-03-08 00:17: **********Train Epoch 413: averaged Loss: 40.881846\n",
      "2025-03-08 00:17: **********Val Epoch 413: average Loss: 2123.241333\n",
      "2025-03-08 00:17: **********Val Epoch 413: average Loss: 2123.241333\n",
      "2025-03-08 00:17: **********Val Epoch 413: average Loss: 2123.241333\n",
      "2025-03-08 00:17: Train Epoch 414: 0/23 Loss: 3.155770\n",
      "2025-03-08 00:17: Train Epoch 414: 0/23 Loss: 3.155770\n",
      "2025-03-08 00:17: Train Epoch 414: 0/23 Loss: 3.155770\n",
      "2025-03-08 00:17: Train Epoch 414: 20/23 Loss: 66.461487\n",
      "2025-03-08 00:17: Train Epoch 414: 20/23 Loss: 66.461487\n",
      "2025-03-08 00:17: Train Epoch 414: 20/23 Loss: 66.461487\n",
      "2025-03-08 00:18: **********Train Epoch 414: averaged Loss: 32.253378\n",
      "2025-03-08 00:18: **********Train Epoch 414: averaged Loss: 32.253378\n",
      "2025-03-08 00:18: **********Train Epoch 414: averaged Loss: 32.253378\n",
      "2025-03-08 00:18: **********Val Epoch 414: average Loss: 2094.383057\n",
      "2025-03-08 00:18: **********Val Epoch 414: average Loss: 2094.383057\n",
      "2025-03-08 00:18: **********Val Epoch 414: average Loss: 2094.383057\n",
      "2025-03-08 00:18: Train Epoch 415: 0/23 Loss: 5.886483\n",
      "2025-03-08 00:18: Train Epoch 415: 0/23 Loss: 5.886483\n",
      "2025-03-08 00:18: Train Epoch 415: 0/23 Loss: 5.886483\n",
      "2025-03-08 00:18: Train Epoch 415: 20/23 Loss: 63.232895\n",
      "2025-03-08 00:18: Train Epoch 415: 20/23 Loss: 63.232895\n",
      "2025-03-08 00:18: Train Epoch 415: 20/23 Loss: 63.232895\n",
      "2025-03-08 00:18: **********Train Epoch 415: averaged Loss: 31.270639\n",
      "2025-03-08 00:18: **********Train Epoch 415: averaged Loss: 31.270639\n",
      "2025-03-08 00:18: **********Train Epoch 415: averaged Loss: 31.270639\n",
      "2025-03-08 00:18: **********Val Epoch 415: average Loss: 2117.769775\n",
      "2025-03-08 00:18: **********Val Epoch 415: average Loss: 2117.769775\n",
      "2025-03-08 00:18: **********Val Epoch 415: average Loss: 2117.769775\n",
      "2025-03-08 00:18: Train Epoch 416: 0/23 Loss: 5.344577\n",
      "2025-03-08 00:18: Train Epoch 416: 0/23 Loss: 5.344577\n",
      "2025-03-08 00:18: Train Epoch 416: 0/23 Loss: 5.344577\n",
      "2025-03-08 00:18: Train Epoch 416: 20/23 Loss: 77.195930\n",
      "2025-03-08 00:18: Train Epoch 416: 20/23 Loss: 77.195930\n",
      "2025-03-08 00:18: Train Epoch 416: 20/23 Loss: 77.195930\n",
      "2025-03-08 00:18: **********Train Epoch 416: averaged Loss: 31.738861\n",
      "2025-03-08 00:18: **********Train Epoch 416: averaged Loss: 31.738861\n",
      "2025-03-08 00:18: **********Train Epoch 416: averaged Loss: 31.738861\n",
      "2025-03-08 00:18: **********Val Epoch 416: average Loss: 2125.797180\n",
      "2025-03-08 00:18: **********Val Epoch 416: average Loss: 2125.797180\n",
      "2025-03-08 00:18: **********Val Epoch 416: average Loss: 2125.797180\n",
      "2025-03-08 00:18: Train Epoch 417: 0/23 Loss: 7.055760\n",
      "2025-03-08 00:18: Train Epoch 417: 0/23 Loss: 7.055760\n",
      "2025-03-08 00:18: Train Epoch 417: 0/23 Loss: 7.055760\n",
      "2025-03-08 00:18: Train Epoch 417: 20/23 Loss: 99.145859\n",
      "2025-03-08 00:18: Train Epoch 417: 20/23 Loss: 99.145859\n",
      "2025-03-08 00:18: Train Epoch 417: 20/23 Loss: 99.145859\n",
      "2025-03-08 00:18: **********Train Epoch 417: averaged Loss: 33.381700\n",
      "2025-03-08 00:18: **********Train Epoch 417: averaged Loss: 33.381700\n",
      "2025-03-08 00:18: **********Train Epoch 417: averaged Loss: 33.381700\n",
      "2025-03-08 00:18: **********Val Epoch 417: average Loss: 2097.904297\n",
      "2025-03-08 00:18: **********Val Epoch 417: average Loss: 2097.904297\n",
      "2025-03-08 00:18: **********Val Epoch 417: average Loss: 2097.904297\n",
      "2025-03-08 00:18: Train Epoch 418: 0/23 Loss: 10.535759\n",
      "2025-03-08 00:18: Train Epoch 418: 0/23 Loss: 10.535759\n",
      "2025-03-08 00:18: Train Epoch 418: 0/23 Loss: 10.535759\n",
      "2025-03-08 00:18: Train Epoch 418: 20/23 Loss: 78.280777\n",
      "2025-03-08 00:18: Train Epoch 418: 20/23 Loss: 78.280777\n",
      "2025-03-08 00:18: Train Epoch 418: 20/23 Loss: 78.280777\n",
      "2025-03-08 00:18: **********Train Epoch 418: averaged Loss: 36.011963\n",
      "2025-03-08 00:18: **********Train Epoch 418: averaged Loss: 36.011963\n",
      "2025-03-08 00:18: **********Train Epoch 418: averaged Loss: 36.011963\n",
      "2025-03-08 00:18: **********Val Epoch 418: average Loss: 2106.763306\n",
      "2025-03-08 00:18: **********Val Epoch 418: average Loss: 2106.763306\n",
      "2025-03-08 00:18: **********Val Epoch 418: average Loss: 2106.763306\n",
      "2025-03-08 00:18: Train Epoch 419: 0/23 Loss: 2.173452\n",
      "2025-03-08 00:18: Train Epoch 419: 0/23 Loss: 2.173452\n",
      "2025-03-08 00:18: Train Epoch 419: 0/23 Loss: 2.173452\n",
      "2025-03-08 00:18: Train Epoch 419: 20/23 Loss: 117.655579\n",
      "2025-03-08 00:18: Train Epoch 419: 20/23 Loss: 117.655579\n",
      "2025-03-08 00:18: Train Epoch 419: 20/23 Loss: 117.655579\n",
      "2025-03-08 00:18: **********Train Epoch 419: averaged Loss: 41.501852\n",
      "2025-03-08 00:18: **********Train Epoch 419: averaged Loss: 41.501852\n",
      "2025-03-08 00:18: **********Train Epoch 419: averaged Loss: 41.501852\n",
      "2025-03-08 00:18: **********Val Epoch 419: average Loss: 2086.587158\n",
      "2025-03-08 00:18: **********Val Epoch 419: average Loss: 2086.587158\n",
      "2025-03-08 00:18: **********Val Epoch 419: average Loss: 2086.587158\n",
      "2025-03-08 00:18: *********************************Current best model saved!\n",
      "2025-03-08 00:18: *********************************Current best model saved!\n",
      "2025-03-08 00:18: *********************************Current best model saved!\n",
      "2025-03-08 00:18: Train Epoch 420: 0/23 Loss: 15.757011\n",
      "2025-03-08 00:18: Train Epoch 420: 0/23 Loss: 15.757011\n",
      "2025-03-08 00:18: Train Epoch 420: 0/23 Loss: 15.757011\n",
      "2025-03-08 00:18: Train Epoch 420: 20/23 Loss: 106.823868\n",
      "2025-03-08 00:18: Train Epoch 420: 20/23 Loss: 106.823868\n",
      "2025-03-08 00:18: Train Epoch 420: 20/23 Loss: 106.823868\n",
      "2025-03-08 00:18: **********Train Epoch 420: averaged Loss: 59.011979\n",
      "2025-03-08 00:18: **********Train Epoch 420: averaged Loss: 59.011979\n",
      "2025-03-08 00:18: **********Train Epoch 420: averaged Loss: 59.011979\n",
      "2025-03-08 00:18: **********Val Epoch 420: average Loss: 2056.184937\n",
      "2025-03-08 00:18: **********Val Epoch 420: average Loss: 2056.184937\n",
      "2025-03-08 00:18: **********Val Epoch 420: average Loss: 2056.184937\n",
      "2025-03-08 00:18: *********************************Current best model saved!\n",
      "2025-03-08 00:18: *********************************Current best model saved!\n",
      "2025-03-08 00:18: *********************************Current best model saved!\n",
      "2025-03-08 00:18: Train Epoch 421: 0/23 Loss: 6.085371\n",
      "2025-03-08 00:18: Train Epoch 421: 0/23 Loss: 6.085371\n",
      "2025-03-08 00:18: Train Epoch 421: 0/23 Loss: 6.085371\n",
      "2025-03-08 00:18: Train Epoch 421: 20/23 Loss: 79.175552\n",
      "2025-03-08 00:18: Train Epoch 421: 20/23 Loss: 79.175552\n",
      "2025-03-08 00:18: Train Epoch 421: 20/23 Loss: 79.175552\n",
      "2025-03-08 00:18: **********Train Epoch 421: averaged Loss: 38.782823\n",
      "2025-03-08 00:18: **********Train Epoch 421: averaged Loss: 38.782823\n",
      "2025-03-08 00:18: **********Train Epoch 421: averaged Loss: 38.782823\n",
      "2025-03-08 00:18: **********Val Epoch 421: average Loss: 2077.592224\n",
      "2025-03-08 00:18: **********Val Epoch 421: average Loss: 2077.592224\n",
      "2025-03-08 00:18: **********Val Epoch 421: average Loss: 2077.592224\n",
      "2025-03-08 00:18: Train Epoch 422: 0/23 Loss: 12.708082\n",
      "2025-03-08 00:18: Train Epoch 422: 0/23 Loss: 12.708082\n",
      "2025-03-08 00:18: Train Epoch 422: 0/23 Loss: 12.708082\n",
      "2025-03-08 00:18: Train Epoch 422: 20/23 Loss: 73.342552\n",
      "2025-03-08 00:18: Train Epoch 422: 20/23 Loss: 73.342552\n",
      "2025-03-08 00:18: Train Epoch 422: 20/23 Loss: 73.342552\n",
      "2025-03-08 00:18: **********Train Epoch 422: averaged Loss: 34.742989\n",
      "2025-03-08 00:18: **********Train Epoch 422: averaged Loss: 34.742989\n",
      "2025-03-08 00:18: **********Train Epoch 422: averaged Loss: 34.742989\n",
      "2025-03-08 00:18: **********Val Epoch 422: average Loss: 2109.861389\n",
      "2025-03-08 00:18: **********Val Epoch 422: average Loss: 2109.861389\n",
      "2025-03-08 00:18: **********Val Epoch 422: average Loss: 2109.861389\n",
      "2025-03-08 00:18: Train Epoch 423: 0/23 Loss: 1.891811\n",
      "2025-03-08 00:18: Train Epoch 423: 0/23 Loss: 1.891811\n",
      "2025-03-08 00:18: Train Epoch 423: 0/23 Loss: 1.891811\n",
      "2025-03-08 00:18: Train Epoch 423: 20/23 Loss: 138.521545\n",
      "2025-03-08 00:18: Train Epoch 423: 20/23 Loss: 138.521545\n",
      "2025-03-08 00:18: Train Epoch 423: 20/23 Loss: 138.521545\n",
      "2025-03-08 00:18: **********Train Epoch 423: averaged Loss: 38.496763\n",
      "2025-03-08 00:18: **********Train Epoch 423: averaged Loss: 38.496763\n",
      "2025-03-08 00:18: **********Train Epoch 423: averaged Loss: 38.496763\n",
      "2025-03-08 00:18: **********Val Epoch 423: average Loss: 2079.737671\n",
      "2025-03-08 00:18: **********Val Epoch 423: average Loss: 2079.737671\n",
      "2025-03-08 00:18: **********Val Epoch 423: average Loss: 2079.737671\n",
      "2025-03-08 00:18: Train Epoch 424: 0/23 Loss: 12.681757\n",
      "2025-03-08 00:18: Train Epoch 424: 0/23 Loss: 12.681757\n",
      "2025-03-08 00:18: Train Epoch 424: 0/23 Loss: 12.681757\n",
      "2025-03-08 00:18: Train Epoch 424: 20/23 Loss: 106.449059\n",
      "2025-03-08 00:18: Train Epoch 424: 20/23 Loss: 106.449059\n",
      "2025-03-08 00:18: Train Epoch 424: 20/23 Loss: 106.449059\n",
      "2025-03-08 00:18: **********Train Epoch 424: averaged Loss: 56.764915\n",
      "2025-03-08 00:18: **********Train Epoch 424: averaged Loss: 56.764915\n",
      "2025-03-08 00:18: **********Train Epoch 424: averaged Loss: 56.764915\n",
      "2025-03-08 00:18: **********Val Epoch 424: average Loss: 2093.879395\n",
      "2025-03-08 00:18: **********Val Epoch 424: average Loss: 2093.879395\n",
      "2025-03-08 00:18: **********Val Epoch 424: average Loss: 2093.879395\n",
      "2025-03-08 00:18: Train Epoch 425: 0/23 Loss: 1.884866\n",
      "2025-03-08 00:18: Train Epoch 425: 0/23 Loss: 1.884866\n",
      "2025-03-08 00:18: Train Epoch 425: 0/23 Loss: 1.884866\n",
      "2025-03-08 00:18: Train Epoch 425: 20/23 Loss: 69.568527\n",
      "2025-03-08 00:18: Train Epoch 425: 20/23 Loss: 69.568527\n",
      "2025-03-08 00:18: Train Epoch 425: 20/23 Loss: 69.568527\n",
      "2025-03-08 00:18: **********Train Epoch 425: averaged Loss: 37.478944\n",
      "2025-03-08 00:18: **********Train Epoch 425: averaged Loss: 37.478944\n",
      "2025-03-08 00:18: **********Train Epoch 425: averaged Loss: 37.478944\n",
      "2025-03-08 00:18: **********Val Epoch 425: average Loss: 2105.703857\n",
      "2025-03-08 00:18: **********Val Epoch 425: average Loss: 2105.703857\n",
      "2025-03-08 00:18: **********Val Epoch 425: average Loss: 2105.703857\n",
      "2025-03-08 00:18: Train Epoch 426: 0/23 Loss: 14.325039\n",
      "2025-03-08 00:18: Train Epoch 426: 0/23 Loss: 14.325039\n",
      "2025-03-08 00:18: Train Epoch 426: 0/23 Loss: 14.325039\n",
      "2025-03-08 00:18: Train Epoch 426: 20/23 Loss: 63.674465\n",
      "2025-03-08 00:18: Train Epoch 426: 20/23 Loss: 63.674465\n",
      "2025-03-08 00:18: Train Epoch 426: 20/23 Loss: 63.674465\n",
      "2025-03-08 00:18: **********Train Epoch 426: averaged Loss: 33.666599\n",
      "2025-03-08 00:18: **********Train Epoch 426: averaged Loss: 33.666599\n",
      "2025-03-08 00:18: **********Train Epoch 426: averaged Loss: 33.666599\n",
      "2025-03-08 00:18: **********Val Epoch 426: average Loss: 2095.254272\n",
      "2025-03-08 00:18: **********Val Epoch 426: average Loss: 2095.254272\n",
      "2025-03-08 00:18: **********Val Epoch 426: average Loss: 2095.254272\n",
      "2025-03-08 00:18: Train Epoch 427: 0/23 Loss: 3.401686\n",
      "2025-03-08 00:18: Train Epoch 427: 0/23 Loss: 3.401686\n",
      "2025-03-08 00:18: Train Epoch 427: 0/23 Loss: 3.401686\n",
      "2025-03-08 00:18: Train Epoch 427: 20/23 Loss: 103.117828\n",
      "2025-03-08 00:18: Train Epoch 427: 20/23 Loss: 103.117828\n",
      "2025-03-08 00:18: Train Epoch 427: 20/23 Loss: 103.117828\n",
      "2025-03-08 00:18: **********Train Epoch 427: averaged Loss: 31.797960\n",
      "2025-03-08 00:18: **********Train Epoch 427: averaged Loss: 31.797960\n",
      "2025-03-08 00:18: **********Train Epoch 427: averaged Loss: 31.797960\n",
      "2025-03-08 00:18: **********Val Epoch 427: average Loss: 2092.788269\n",
      "2025-03-08 00:18: **********Val Epoch 427: average Loss: 2092.788269\n",
      "2025-03-08 00:18: **********Val Epoch 427: average Loss: 2092.788269\n",
      "2025-03-08 00:18: Train Epoch 428: 0/23 Loss: 6.208241\n",
      "2025-03-08 00:18: Train Epoch 428: 0/23 Loss: 6.208241\n",
      "2025-03-08 00:18: Train Epoch 428: 0/23 Loss: 6.208241\n",
      "2025-03-08 00:18: Train Epoch 428: 20/23 Loss: 100.594017\n",
      "2025-03-08 00:18: Train Epoch 428: 20/23 Loss: 100.594017\n",
      "2025-03-08 00:18: Train Epoch 428: 20/23 Loss: 100.594017\n",
      "2025-03-08 00:18: **********Train Epoch 428: averaged Loss: 46.796957\n",
      "2025-03-08 00:18: **********Train Epoch 428: averaged Loss: 46.796957\n",
      "2025-03-08 00:18: **********Train Epoch 428: averaged Loss: 46.796957\n",
      "2025-03-08 00:18: **********Val Epoch 428: average Loss: 2090.695312\n",
      "2025-03-08 00:18: **********Val Epoch 428: average Loss: 2090.695312\n",
      "2025-03-08 00:18: **********Val Epoch 428: average Loss: 2090.695312\n",
      "2025-03-08 00:18: Train Epoch 429: 0/23 Loss: 1.677509\n",
      "2025-03-08 00:18: Train Epoch 429: 0/23 Loss: 1.677509\n",
      "2025-03-08 00:18: Train Epoch 429: 0/23 Loss: 1.677509\n",
      "2025-03-08 00:18: Train Epoch 429: 20/23 Loss: 79.475739\n",
      "2025-03-08 00:18: Train Epoch 429: 20/23 Loss: 79.475739\n",
      "2025-03-08 00:18: Train Epoch 429: 20/23 Loss: 79.475739\n",
      "2025-03-08 00:18: **********Train Epoch 429: averaged Loss: 35.513636\n",
      "2025-03-08 00:18: **********Train Epoch 429: averaged Loss: 35.513636\n",
      "2025-03-08 00:18: **********Train Epoch 429: averaged Loss: 35.513636\n",
      "2025-03-08 00:18: **********Val Epoch 429: average Loss: 2065.652588\n",
      "2025-03-08 00:18: **********Val Epoch 429: average Loss: 2065.652588\n",
      "2025-03-08 00:18: **********Val Epoch 429: average Loss: 2065.652588\n",
      "2025-03-08 00:18: Train Epoch 430: 0/23 Loss: 8.323850\n",
      "2025-03-08 00:18: Train Epoch 430: 0/23 Loss: 8.323850\n",
      "2025-03-08 00:18: Train Epoch 430: 0/23 Loss: 8.323850\n",
      "2025-03-08 00:18: Train Epoch 430: 20/23 Loss: 70.181229\n",
      "2025-03-08 00:18: Train Epoch 430: 20/23 Loss: 70.181229\n",
      "2025-03-08 00:18: Train Epoch 430: 20/23 Loss: 70.181229\n",
      "2025-03-08 00:18: **********Train Epoch 430: averaged Loss: 32.956445\n",
      "2025-03-08 00:18: **********Train Epoch 430: averaged Loss: 32.956445\n",
      "2025-03-08 00:18: **********Train Epoch 430: averaged Loss: 32.956445\n",
      "2025-03-08 00:18: **********Val Epoch 430: average Loss: 2108.085449\n",
      "2025-03-08 00:18: **********Val Epoch 430: average Loss: 2108.085449\n",
      "2025-03-08 00:18: **********Val Epoch 430: average Loss: 2108.085449\n",
      "2025-03-08 00:18: Train Epoch 431: 0/23 Loss: 1.550500\n",
      "2025-03-08 00:18: Train Epoch 431: 0/23 Loss: 1.550500\n",
      "2025-03-08 00:18: Train Epoch 431: 0/23 Loss: 1.550500\n",
      "2025-03-08 00:18: Train Epoch 431: 20/23 Loss: 99.386620\n",
      "2025-03-08 00:18: Train Epoch 431: 20/23 Loss: 99.386620\n",
      "2025-03-08 00:18: Train Epoch 431: 20/23 Loss: 99.386620\n",
      "2025-03-08 00:18: **********Train Epoch 431: averaged Loss: 33.376163\n",
      "2025-03-08 00:18: **********Train Epoch 431: averaged Loss: 33.376163\n",
      "2025-03-08 00:18: **********Train Epoch 431: averaged Loss: 33.376163\n",
      "2025-03-08 00:18: **********Val Epoch 431: average Loss: 2091.708984\n",
      "2025-03-08 00:18: **********Val Epoch 431: average Loss: 2091.708984\n",
      "2025-03-08 00:18: **********Val Epoch 431: average Loss: 2091.708984\n",
      "2025-03-08 00:18: Train Epoch 432: 0/23 Loss: 2.271314\n",
      "2025-03-08 00:18: Train Epoch 432: 0/23 Loss: 2.271314\n",
      "2025-03-08 00:18: Train Epoch 432: 0/23 Loss: 2.271314\n",
      "2025-03-08 00:18: Train Epoch 432: 20/23 Loss: 86.487518\n",
      "2025-03-08 00:18: Train Epoch 432: 20/23 Loss: 86.487518\n",
      "2025-03-08 00:18: Train Epoch 432: 20/23 Loss: 86.487518\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    seq_in: int\n",
    "    seq_out: int\n",
    "    d_state: int =128\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 3\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Ensure the tensors are of type float32\n",
    "    x = x.float()\n",
    "    y = y.float()\n",
    "\n",
    "    # Compute the mean of each tensor\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "\n",
    "    # Compute the deviations from the mean\n",
    "    dev_x = x - mean_x\n",
    "    dev_y = y - mean_y\n",
    "\n",
    "    # Compute the covariance between x and y\n",
    "    covariance = torch.sum(dev_x * dev_y)\n",
    "\n",
    "    # Compute the standard deviations of x and y\n",
    "    std_x = torch.sqrt(torch.sum(dev_x ** 2))\n",
    "    std_y = torch.sqrt(torch.sum(dev_y ** 2))\n",
    "\n",
    "    # Compute the Pearson correlation coefficient\n",
    "    pearson_corr = covariance / (std_x * std_y)\n",
    "\n",
    "    return pearson_corr\n",
    "\n",
    "def rank_tensor(x):\n",
    "    \"\"\"\n",
    "    Return the ranks of elements in a tensor.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): Input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Ranks of the input tensor elements.\n",
    "    \"\"\"\n",
    "    # Get the sorted indices\n",
    "    sorted_indices = torch.argsort(x)\n",
    "    \n",
    "    # Create an empty tensor to hold the ranks\n",
    "    ranks = torch.zeros_like(sorted_indices, dtype=torch.float)\n",
    "    \n",
    "    # Assign ranks based on sorted indices\n",
    "    ranks[sorted_indices] = torch.arange(1, len(x) + 1).float()\n",
    "    \n",
    "    return ranks\n",
    "\n",
    "def rank_information_coefficient(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Rank Information Coefficient (RIC) or Spearman's Rank Correlation Coefficient.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Rank Information Coefficient (RIC).\n",
    "    \"\"\"\n",
    "    # Get the ranks of the elements in x and y\n",
    "    rank_x = rank_tensor(x)\n",
    "    rank_y = rank_tensor(y)\n",
    "\n",
    "    # Calculate the mean rank for both tensors\n",
    "    mean_rank_x = torch.mean(rank_x)\n",
    "    mean_rank_y = torch.mean(rank_y)\n",
    "\n",
    "    # Calculate the covariance of the rank variables\n",
    "    covariance = torch.sum((rank_x - mean_rank_x) * (rank_y - mean_rank_y))\n",
    "\n",
    "    # Calculate the standard deviations of the ranks\n",
    "    std_rank_x = torch.sqrt(torch.sum((rank_x - mean_rank_x) ** 2))\n",
    "    std_rank_y = torch.sqrt(torch.sum((rank_y - mean_rank_y) ** 2))\n",
    "\n",
    "    # Calculate the Spearman rank correlation (RIC)\n",
    "    ric = covariance / (std_rank_x * std_rank_y)\n",
    "    \n",
    "    return ric\n",
    "\n",
    "def cal_r2(x, y):\n",
    "    true = pd.DataFrame(x)\n",
    "    pred = pd.DataFrame(y)\n",
    "    ss_total = np.sum((true[0] - np.mean(true[0]))**2)\n",
    "    ss_residual = np.sum((true[0] - pred[0])**2)\n",
    "    r2_out_of_sample = 1 - (ss_residual / ss_total)\n",
    "    return r2_out_of_sample\n",
    "    \n",
    "for i in range(5):\n",
    "\n",
    "    Mode = 'train'\n",
    "    DEBUG = 'True'\n",
    "    DATASET = 'PEMSD8'      #PEMSD4 or PEMSD8\n",
    "    DEVICE = 'cuda:0'\n",
    "    MODEL = 'AGCRN'\n",
    "\n",
    "#get configuration\n",
    "    config_file = './{}_{}.conf'.format(DATASET, MODEL)\n",
    "#print('Read configuration file: %s' % (config_file))\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parser\n",
    "\n",
    "    args={\"dataset\":DATASET,\"mode\":Mode,\"device\":DEVICE,\"debug\":DEBUG,\"model\":MODEL,\"cuda\":True,\"val_ratio\":0.15,\"test_ratio\":0.15,\n",
    "      \"lag\":window,\"horizon\":predict,\"num_nodes\":XX.shape[2],\"tod\":False,\"normalizer\":'std',\"column_wise\":False,\"default_graph\":True,\n",
    "     \"input_dim\":1,\"output_dim\":1,\"embed_dim\":10,\"rnn_units\":128,\"num_layers\":5,\"cheb_k\":3,\"loss_func\":'mae',\"seed\":1,\n",
    "     \"batch_size\":256,\"epochs\":1500,\"lr_init\":0.0001,\"lr_decay\":True,\"lr_decay_rate\":0.5,\"lr_decay_step\":[40,70,100],\n",
    "      \"early_stop\":True,\"early_stop_patience\":200,\"grad_norm\":False,\"max_grad_norm\":5,\"real_value\":False,\"mae_thresh\":None,\n",
    "      \"mape_thresh\":0,\"log_dir\":'./',\"log_step\":20,\"plot\":False,\"teacher_forcing\":False,\"d_in\":32,\"hid\":32}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#init model\n",
    "    model = SAMBA(ModelArgs(args.get(\"d_in\"),args.get(\"num_layers\"),args.get(\"num_nodes\"),args.get('lag'),args.get('horizon')),args.get('hid'),args.get('lag'),args.get('horizon'),args.get('embed_dim'),args.get(\"cheb_k\"))\n",
    "    model = model.cuda()\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p)\n",
    "    print_model_parameters(model, only_num=False)\n",
    "\n",
    "    if args.get('loss_func') == 'mask_mae':\n",
    "        loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "    elif args.get('loss_func') == 'mae':\n",
    "        loss = torch.nn.L1Loss().to(args.get('device'))\n",
    "    elif args.get('loss_func') == 'mse':\n",
    "        loss = torch.nn.MSELoss().to(args.get('device'))\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=args.get('lr_init'), eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "#learning rate decay\n",
    "    lr_scheduler = None\n",
    "    if args.get('lr_decay'):\n",
    "        print('Applying learning rate decay.')\n",
    "        lr_decay_steps = [int(i) for i in args.get('lr_decay_step')]\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "        milestones=[0.5 * args.get('epochs'),0.7 * args.get('epochs'), 0.9 * args.get('epochs')],gamma=0.1)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "    trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, args=args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    y1,y2=trainer.test(trainer.model, trainer.args, test_loader, trainer.logger)\n",
    "    \n",
    "    y_p=np.array(y1[:,0,:].cpu())\n",
    "\n",
    "    y_t=np.array(y2[:,0,:].cpu())\n",
    "\n",
    "\n",
    "#y_p=(y_p-mean)/std\n",
    "#y_t=(y_t-mean)/std\n",
    "\n",
    "    y_p=torch.tensor(y_p)\n",
    "    y_t=torch.tensor(y_t)\n",
    "\n",
    "    diff = y_p[1:] - y_p[:-1]\n",
    "    return_p = diff / y_p[:-1]\n",
    "    #return_p = y_p\n",
    "    \n",
    "    diff = y_t[1:] - y_t[:-1]\n",
    "    return_t = diff / y_t[:-1]\n",
    "    #return_t = y_t\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    mae, rmse, _=All_Metrics(return_p,return_t, None,None )\n",
    "\n",
    "    IC=pearson_correlation(return_t,return_p)\n",
    "    \n",
    "    RIC=rank_information_coefficient(return_t[:,0],return_p[:,0])\n",
    "\n",
    "    R2 = cal_r2(return_t,return_p)\n",
    "    \n",
    "    result_train_file = os.path.join(\"SAMBA_Data2023_return_250306\")\n",
    "\n",
    "    \n",
    "\n",
    "    save_model(trainer,result_train_file,i+1)\n",
    "\n",
    "    with open('SAMBA_Data2023return_250306', 'a') as f:\n",
    "        f.write(str(np.array(IC)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(RIC)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(mae)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(rmse)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.array(R2)))\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "    saved_return_p = return_p  # Store return_p in a variable\n",
    "    np.save(f'return_p1_{i+1}.npy', saved_return_p.cpu().numpy())  # Save to .npy file\n",
    "    \n",
    "\n",
    "    saved_return_t = return_t  # Store return_p in a variable\n",
    "    np.save(f'return_t1_{i+1}.npy', saved_return_t.cpu().numpy())  # Save to .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d89f3-7cc3-4c95-8c9c-1a3f22b73001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb1d253-8e53-4e50-8789-bfa64bde5d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Number of GPUs: 0\n",
      "No GPU detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/network/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd9b8bc-ac0f-4c5f-add0-28a01029eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Metrics: Data2023 Monthly\n",
      "----------------------------------------------------------------------\n",
      "Index              RMSE           IC          RIC           R2\n",
      "----------------------------------------------------------------------\n",
      "Epoch_5           0.0399↓     0.4401↑      0.3137↑      0.1749↑\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_metrics(file_path):\n",
    "    metrics = {}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        num_epochs = len(lines) // 5\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            IC = float(lines[i * 5].strip())\n",
    "            RIC = float(lines[i * 5 + 1].strip())\n",
    "            MAE = float(lines[i * 5 + 2].strip())\n",
    "            RMSE = float(lines[i * 5 + 3].strip())\n",
    "            R2 = float(lines[i * 5 + 4].strip())\n",
    "            \n",
    "            metrics[f\"Epoch_{i+1}\"] = {\n",
    "                \"IC\": IC,\n",
    "                \"RIC\": RIC,\n",
    "                \"MAE\": MAE,\n",
    "                \"RMSE\": RMSE,\n",
    "                'R2': R2,\n",
    "            }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "file_path = 'SAMBA_Data2023return_250306'\n",
    "metrics = read_metrics(file_path)\n",
    "\n",
    "\n",
    "print(\"Prediction Metrics: Data2023 Monthly\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Index              RMSE           IC          RIC           R2\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "# 最后一个结果代表训练到最后的情况\n",
    "for epoch, metrics_values in list(metrics.items())[0:1]:\n",
    "    rmse_str = f\"{metrics_values['RMSE']:.4f}\"\n",
    "    ic_str = f\"{metrics_values['IC']:.4f}\"\n",
    "    ric_str = f\"{metrics_values['RIC']:.4f}\"\n",
    "    r2_str = f\"{metrics_values['R2']:.4f}\"\n",
    "    \n",
    "    rmse_change = \"↓\" \n",
    "    ic_change = \"↑\" \n",
    "    ric_change = \"↑\" \n",
    "    R2_change = \"↑\"\n",
    "    \n",
    "    print(f\"{epoch:<18}{rmse_str}{rmse_change}     {ic_str}{ic_change}      {ric_str}{ric_change}      {r2_str}{R2_change}\")\n",
    "print(\"----------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadbf74-171e-41d2-b22c-c38385c8346e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
